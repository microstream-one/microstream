Single-Master Clusteringkonzept
Thomas Münz
2018-10-25

Eine laufende Multi-User-Datenbankanwendung besteht aus sehr viel Arbeit zur Laufzeit. Sowohl auf dem Client (egal ob Browser, Smartphone App, Fat Client oder was auchimmer) und auf dem Server. Der Client, der letztendlich nur ein Anzeigetool für einen einzelnen Benutzer ist, soll hier nicht das Thema sein. Es geht um den Server, der die Daten und Aktionen für alle Benutzer gleichzeitig irgendwie managen muss und dabei keine Fehler machen soll und das auch noch mit höchsten Ansprüchen an Geschwindigkeit und Ausfallsicherheit.
Nur einige Beispiele für Serveraufgaben, man könnte die Liste bestimmt erweitern:
- Clientkommunikation (Netzwerkkommunikation, ggf. auch Erzeugen der GUI-Informationen, wie bei HTML)
- Anwendungslogik (Validierung von Benutzeranfragen, Berechnungen, Abfragen)
- Datenpersistierung ("Datenbank")
- Überwachungs- und Fehlerdiagnose-Unterstützung ("Logging")
- Backend-Administrationsmöglichkeiten
- Aufbereitung von Daten zur Performancesteigerung (z.B. Aggregation von Daten in verschiedenen Gruppierungen, in verschiedenen Formaten, Dateien, usw.)

Je nach Komplexität der Anwendungslogik, Umfang der persistenten Datenmenge und Anzahl der gleichzeitigen Benutzer kann eine einzelne Serverhardware, und sei sie noch so leistungsfähig, damit schnell an ihre Grenzen kommen. Vor allem die Geschwindigkeit von Computerhardware nimmt nicht (mehr) kontinuierlich zu. Mit um die 4-5 GHz sind wir bei CPUs momentan an einer Grenze angekommen. Auch RAM, Festplatten und die systeminterne Datenübertragung zwischen all diesen Komponenten werden nicht beliebig schneller.
Also muss eine Anwendung über mehrere Hardwaresysteme verteilt betrieben werden und jede Hardware kümmert sich um einen Teil der Last ("Cluster"). Das erzeugt aber große Software-architektonische Probleme: Wenn zwei Benutzer gleichzeitig auf unterschiedlichen Teilen des Clusters an demselben Datensatz Änderungen vornehmen wollen, wer hat recht? Wie wird bei solchen parallelen Aktionen in einem komplexen Datenmodell mit komplexer Anwendungslogik die Konsistenz der Daten gewährleistet? Muss die Anwendungslogik selbst permanent Logik zur Nebenläufigkeitskoordinierung ausführen? Macht das nicht alles entweder extrem langsam oder extrem fehleranfällig oder beides? Von der Mühsal für den Anwendungsentwickler ganz zu schweigen.
Die Antwort ist: Ja, das ist so. Es gibt Clusteringkonzepte, da wird bei einer Änderung der gesamte Cluster "gestoppt" und kann erst weiterarbeiten, wenn alle Teile des Clusters ("Nodes") wieder auf dem aktuellen Stand sind. Es gibt Clusteringkonzepte, da werden Inkonsistenzen durch parallel konfliktete Änderungen einfach mit einem "ist halt so" hingenommen.
Geht das wirklich nicht besser? Doch. Natürlich.
Dazu nochmal ein Blick auf die zahlreichen Aufgaben oben. Der einzig kritische Punkt bei parallel arbeitenden Teilen (wie Nodes in einem Cluster) ist, wenn gleichzeitig Änderungen an gemeinsam genutzten Daten ausgeführt werden. Solche Änderungen dauern meist nur wenige Mikrosekunden, manchmal nur Nanosekunden. Der Kern "Änderungsaufwands" macht vielleicht 0,1% der Arbeit in so einer Serveranwendung aus. ALLES andere, sei es Client-Kommunikation, Validierung von Anfragen, Berechnung, gecachte Datenaufbereitung, usw., die ganzen 99,9% riesiger Aufwand, erzeugt zwischen den Teilen nie Konflikte.
Wie wäre also folgende Idee:
Ein Node (genauer gesagt ein Thread auf einem Node) kümmert sich ununterbrochen nur darum, gewünschte Änderungen abzuarbeiten. Und zwar strikt seriell hintereinander. Die Änderungsanfragen wurden vorher schon größtenteils validiert. Es muss nur nochmal kurz geprüft werden, ob die Änderung noch zu den aktuellen Daten im Moment der Verarbeitung passt, dann wird die Änderung vorgenommen und dann kommt schon die nächste Änderungsanfrage dran. Übertragungen im Internet dauern mehrere Millisekunden. Eine Wartezeit von 100 Millisekunden nehmen Menschen kaum wahr. Wie viele Mikrosekunden-Arbeitspakete für wie viele gleichzeitigen Benutzer könnte eine moderne Hardware mit diesem Konzept wohl in so einer Zeitspanne (d.h. gefühlt "gleichzeitig") abarbeiten? Tausende, zehntausende, vielleicht hunderttausende gleichzeitige Benutzer. Immerhin drückt ja nicht jeder "aktive" Benutzer ununterbrochen im Millisekundentakt irgendeinen Knopf, der ein Änderungskommando auslöst. Die allerallermeisten Aktionen erfordern nur lesenden Zugriff. Echte Änderungen (an persistenten Daten) sind eher selten, erfolgen bei einem Benutzer im Abstand von Sekunden oder Minuten oder manchmal sogar gar nicht (z.B. das Lesen einer News-Seite oder schauen eines Videos auf Youtube. Alles nur lesend).
Das heißt: echte "Kernarbeit" von schreibenden Aktionen auf einen einzelnen Node (und Thread) legen, ALLES andere irgendwie geschickt auf andere Nodes verteilen. Daten auf beliebig viele "read-only" Nodes (oder "Slaves") replizieren, mit denen beliebig viele gleichzeitige Benutzer bedienen und von dort nur echte Änderungsanfragen, nach einer vorab-Validieren, an den "Master" zurückschicken. Änderungen an den Daten werden kontinuierlich an die Slaves gesendet (natürlich schon von einem anderen Thread auf dem Master-Node, um den Thread für die Kernarbeit nicht unnötig zu belasten).
Die Slaves sind problemlos dynamisch zu- und wegschaltbar, da sie selbst ja nie wertvolle Änderungen an Daten vornehmen, sondern immer nur replizierte Daten bekommen und anbieten.
Sollte der Master ausfallen, kann in kürzester Zeit und vollautomatisch einer der Slaves zum neuen Master befördert werden. Die Daten hat er ja alle schon. Die Anwendungslogik auch. Master sein ist keine Frage des "Könnens", sondern nur des "Dürfens". Sobald der eine ausfällt, darf einfach der nächste.
Der Rest sind technische Details, die mit viel Nachdenken und Programmierarbeit gelöst werden können. Etwa das Request-Handling zwischen Slaves und Master für Schreibänderungen, die Slaves effizient aktualisieren, usw.
Man kann auch den Master massiv entlasten, wenn für dafür günstige Daten auch das Schreiben ausgelagert wird. Beispiel: Alle Arten von großen Binärdaten, die zwar viel Platz belegen, aber datenmodellmäßig keine Struktur haben. Etwa Bilder, Audio, Video, Dateien allgemein, sogar große Texte. Jeder datenmodellmäßig strukturlose Bytehaufen über, sagen wir 1000 Byte Größe, wird unter einer eindeutigen ID an einen externen generischen Speicherdienst ausgelagert. Bei Bedarf wird die ID wieder zu dem riesigen Byte-Haufen aufgelöst. Idealerweise gleich auf dem Client, damit der Server sich mit dem Hin- und herschaufeln der vielen Bytes gar nicht belasten muss. Beispiel: Eine HTML-Seite generieren mit einem URI auf ein Bild, das irgendwo in einem Media Cache Server liegt. Es gibt keinen Grund, warum der Anwendungsserver selbst sich mit dem Laden des Bilds herumschlagen sollte. Oder gar der Schreib-Master.
Man kann sogar Datensätze des Datenmodells, also echte "Premium Struktur Daten" auf diese Weise auslagern, wenn zwei Bedingungen erfüllt sind: 1.) sie ändern sich nach der Erzeugung nicht mehr und 2.) sie werden vom restlichen Datenmodell nicht als einzelne Datensätze direkt referenziert, sondern nur als Sammlung von Datensätzen. Beispiele dafür gibt es viele: Messdaten, Umsatzdaten, Logeinträge, sich nicht mehr ändernde Vorjahresdaten, usw. Alles, was man irgendwie "Massendaten" nennen kann, die immer nur "paketweise" geladen werden müssen. Einfach das ganze Paket persistieren zu einem Byteblock und den ganzen Byteblock unter einer ID auslagern. Wie Waren auf einer Palette.
Das heißt: Der Master selbst kümmerst sich nur um relativ "wenige" Datensätze (Entities), die einen gewissen "First Class" Status haben: sie werden im Datenmodell direkt als einzelne Entities referenziert und/oder sie können sich jederzeit ändern. Das "wenig" ist hier wirklich relativ zu sehen. Das sind immer noch Millionen an Datensätzen. Vielleicht sogar Milliarden. Aber das macht nichts. Für heutige Hardware ist das "wenig", wenn die Infrastruktur-Software nur geschickt genug ist. Mein 0815 Büro-PC etwa kann mit Jetstream eine zweistellige Millionenzahl von persistenten Entities mühelos verwalten. Eine richtige Serverhardware mit einem nennenswerten Businessbudget könnte dann auch hunderte Millionen oder Milliarden verwalten. Plus alle auslagerbaren Daten. Das muss man erst mal sinnvoll voll bekommen.

Je nach Anwendung und Hardware kann so ein Konzept tausende bis hunderttausende gleichzeitige Benutzer bedienen, mit Milliarden an Datensätzen und Terabytes an Datenmenge.

Und falls auch das mal nicht mehr reicht?
Dann kann man sich immer noch für eine spezifische Anwendung einen geschickten Ansatz überlegen, die Anwendungsdaten zu segmentieren und auf mehrere Master-Threads oder sogar -Nodes aufzuteilen, die sich dann eben untereinander anwendungsspezifisch (hoffentlich möglichst wenig) abstimmen müssen. Vielleicht finden sich auch dafür mal generische Konzepte. Vielleicht wird es an dieser Stelle anwendungsentwicklungstechnisch etwas mühsam. Wie auch immer: Das Konzept erlaubt schon in der Single-Master Grundausführung riesige Cluster und nach oben gibt es keine harte Grenze.