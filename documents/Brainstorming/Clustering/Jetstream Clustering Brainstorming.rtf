{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1031{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.17134}\viewkind4\uc1 
\pard\sl276\slmult1\f0\fs28\lang7 Jetstream Clustering Brainstorming\par
2018-10-16\par
\par
\par
"Lowlevel"\par
1 Master - liest und schreibt Daten\par
N Slaves - bekommen Kopien vom Master repliziert, nehmen ihm lese-Last ab, agieren als Failover Master\par
\par
\par
"Microservice"\par
Ist ein Teil der Anwendung (Beispiel "Umsatzserver").\par
Jede Microservice ist f\'fcr eine disjunkte Menge fachlicher Daten exklusiv zust\'e4ndig.\par
F\'fchrt Validierung von zu speichernden Daten auf fachlicher Ebene durch. Alles "darunter" kann ruhigen gewissens ungepr\'fcft "dumm" speichern\par
Kann einen "Lowlevel" Teil direkt embedded haben oder kann N Lowlevel Teile per Netzwerk ansprechen.\par
Jeder "Lowlevel" Teil wird nach OID Modulo segmentiert.\par
\par
"Masterservice"\par
Kennt alle Microservices, d.h. er "ist" in Summe "Die Anwendung".\par
\par
Problem:\par
Damit neben die LowLevel Slaves keine Leselast mehr ab.\par
Es muss eher so sein, dass ein Microservice entweder der Master ist oder es N Slave-Microservices gibt und wenn was gespeichert werden soll, muss immer an den Master-Microservice gleitet werden.\par
D.h. die Anwendung muss auch fachlich zwei Schichten unterteilt werden:\par
In...\par
- "Data Services", die liefern fachliche Daten, validieren gew\'fcnschte \'c4nderungsanfragen und schreiben g\'fcltige \'c4nderungen\par
- "Work Services", die sind selbst transient, also ohne gespeicherte fachliche Daten, halten Usersessions, gecachte Bilder und so Grusch und reden je nach Anforderung mit einem beliebigen Data-Slave (lesend) oder mit dem Data-Master (schreiben).\par
\par
Kommunikation zwischen Data und Work Nodes erfolgt \'fcber Jetstream-serialisierte Objektgraphen.\par
\par
Dar\'fcber muss es einen Load Balance geben, der Anfragen von au\'dfen auf die Work Nodes verteilt und Anfragen von Work Nodes auf die Data-Slaves verteilt.\par
\par
Die Anzahl der Work Nodes kann beliebig skaliert werden. Sie haben gar keinen persistenten State.\par
Die Anzahl der Data-Slaves kann beliebig skaliert werden. Sie bekommen komplette Kopien ihres Masters und sind read-only.\par
Die Anzahl der Data-Master ist fest und muss entsprechend der Architektur der Anwendung (Datenmodell, Logik) designt werden.\par
Ein Data-Master ist der Teil, der exklusive fachliche Validierung von zusammengeh\'f6renden Teilen ausf\'fchren muss. Soetwas kann man nicht generisch clustern.\par
\par
Man kann einen Data-Master aber oft nachtr\'e4glich auf fachlicher Ebene clustern.\par
Beispiel:\par
Umsatznode.\par
Anfangs 1 monolithischer Node mit allen Ums\'e4tzen.\par
Sp\'e4ter 1 Node f\'fcr die Ums\'e4tze des aktuellen Jahrs, 1 node f\'fcr die letzten beiden Jahre und 1 Node f\'fcr alle vorherigen.\par
Der Anwendungscode ist dann auf jedem Master identisch, nur mit anderen Segmenten der Daten, f\'fcr die er zust\'e4ndig ist.\par
\par
Um alle Nodes, also Work, Data und Master untereinander, kann man mit Lazy<> Referenzen abbilden.\par
Die h\'e4lt erst mal nur eine OID und kennt den zust\'e4ndigen Node. Bei einem .get() wird dieser Node kontaktiert und alle Daten bis zur n\'e4chsten Lazy<> Referenz \'fcbertragen.\par
\par
Knifflig: F\'fcr jede geladene Instanz muss registriert werden, wer ihr zust\'e4ndiger Data-Master Node ist. Bei einem store() Aufruf f\'fcr einen Entitygraph muss je nach Instanz nach den zust\'e4ndigen Data-Mastern unterschieden werden.\par
Der commit() Aufruf des Storers schickt die serialisierten Entities an ihre jeweiligen Data Master. Diese Validieren und melden erfolg oder miserfolg zur\'fcck. Wenn alle beteiligten Data-Master Erfolg gemeldet haben, persistieren alle Data-Master die validierten Entities. \par
\par
Problem: Was ist, wenn alle erfolg gemeldet haben, aber ein oder mehr Data-Master beim Speichern auf ein Problem gesto\'dfen sind?\par
Dann wurden manche der Entities gespeichert und manche nicht.\par
Auf Checks und Rollbacks kann man sich nicht verlassen, weil ja wiederum manche der Nodes, die rollbacken m\'fcssten, auf ein Problem sto\'dfen k\'f6nnten.\par
Muss es stattdessen eine Art Transaction-ID geben, die \'fcber den gesamten Cluster gilt und bei der Initialisierung aller Nodes f\'fcr den letzten Store auf Einheitlichkeit grepr\'fcft wird?\par
K\'f6nnte klappen.\par
\par
Alternative:\par
Ein Store darf - gepr\'fcft \'fcber die per-instanz-registry nur entities eines Data-Masters enthalten.\par
Hei\'dft: wenn man Kunden und Ums\'e4tze speichern will, muss man einen storer f\'fcr die Kunden und einen storer f\'fcr die Ums\'e4tze machen.\par
Aber was ist dann, wenn ein Umsatz seinen Kunden kennt und der Kunde seinen Umsatz?\par
Es m\'fcsste dann im Design f\'fcr jeden \'dcbergang im Datenmodell von einem Master-Bereich zum anderen ein Lazy vorgesehen sein.\par
\par
\par
\par
Ideal w\'e4re folgendes:\par
- Es gibt die Anwendung Code-m\'e4\'dfig nur einmal als Monolith.\par
- Jeder Microservice startet die komplette Anwendung, ist aber nur f\'fcr einen Teil der Daten zust\'e4ndig.\par
- Irgendwie muss sichergestellt sein, dass die Entities, die ein Microservice reingeschickt bekommt, nur die Logik triggern, f\'fcr die der Microservice mit seiner Zust\'e4ndigkeit in den Daten ausf\'fchren kann.\par
\par
Bl\'f6d daran aber:\par
Microservices sollen unter anderem ja auch die Code Komplexit\'e4t reduzieren:\par
Der Umsatzserver muss nur den Code zum Ums\'e4tze handeln haben.\par
\par
\par
Und noch ein Problem: wenn neue Instanzen zum ersten mal gespeichert werden, woher wei\'df ein Microservice, dass er wirklich daf\'fcr fachlich segment-m\'e4\'dfig zust\'e4ndig ist?\par
\par
\par
Und ein ziemlich entscheidendes Problem, geradezu ein Show-stopper:\par
Wenn sich irgendwelche Nodes von irgendwelchen anderen Nodes Daten laden und die "anderen Nodes" dann neue Versionen dieser Datens\'e4tze bekommen und abspeichern, wie bekommen die Nodes mit den geladenen Daten das dann mit?\par
Das ist genau dasselbe Probleme wie bei einer RDBMS+Java Anwendung, wo eigentlich der Java Prozess der "Chef" sein sollte, \'fcber den zwecks Businesslogik Validierung usw. alle Daten\'e4nderungen laufen sollen, aber dann \'e4ndert einer an der Seite vorbei direkt in der Datenbank rum. Der Java Prozess bekommt das nicht mit, also sind, schwups, mal eben, seine geladenen Daten inkonsistent. Also m\'fcsste er bei jeder Operationen pr\'fcfen, ob neuere Versionen da sind, oder die Caches ganz abschalten (Performance = tot) oder wegen jeder kleinen Scheiss\'e4nderung cache invalidation signale bekommen.\par
Das Kernproblem ist: Es darf nicht mehrere Wege geben, an einer Menge an Daten herumzu\'e4ndern. F\'fcr eine Menge an (fachlich sinnvoll zusammengeh\'f6render) Daten darf immer nur genau ein Prozess der sein, der \'e4ndert und durch den fliesen alle \'c4nderungen durch, damit der immer auf dem neuesten Stand ist.\par
\par
Darum vielleicht folgender, radikaler Ansatz:\par
- Es gibt nach wie vor 1 Data-Master und N Data-Slaves, die sich Kopien der Daten ziehen bzw. bekommen.\par
- Die Slaves sind aber gleichzeitig die Worker Nodes.\par
- F\'fcr \'c4nderungen leitet jeder Slave die Daten an den Data-Master, der validiert, persistiert die \'c4nderung und sendet dann die Updates wieder an jeden Data-Slave.\par
\par
Aber was ist jetzt mit Skalierung? Zumindest f\'fcrs Schreiben ist das ja dann ein monolithisches System.\par
Antwort:\par
1.)\par
Zun\'e4chst mal fliegt alles, was "triviale" Bin\'e4rdaten (Bilder, T\'f6ne, Videos, gro\'dfe Texte) sind, aus dem System ganz raus. Solche Daten kann sich ein Client per URL von irgendeinem Image Cache Server nachladen.\par
\par
2.)\par
Dann k\'f6nnen alle fachlichen Daten, die sich nur "selten" \'e4ndern ("selten" hei\'dft "nur" alle paar Minuten oder so), in mehrere andere Data-Master ausgelagert werden. JEDES Update von denen bewirkt dann ein Signal an die Worker Nodes, dass sie ihren Cache f\'fcr den betreffenden Data-Master invalidieren/leeren m\'fcssen.\par
\par
3.)\par
Was dann \'fcbrig bleibt, ist ein Kern-Node mit nur noch den sich schnell \'e4ndernden (Minuten oder weniger) Entities, die f\'fcr sich ziemliche winzige Datens\'e4tze sind (~100 Byte).\par
Wenn die mal "\'e4lter" werden und sich nicht mehr oder nur noch wenig \'e4ndern, dann kann man die in so einen "selten" Node verschieben.\par
Der Kern-Node hat also immer nur die "hei\'dfen" und "kleinen" Daten.\par
Und der muss dann nat\'fcrlich eine der Anwendung angemessene Gr\'f6\'dfe haben. 16 Core, 512 GB RAM oder noch mehr.\par
Bis man so ein Riesensystem mit relativ wenigen und kleinen Daten mal an seine Grenzen bringt, dauert das.\par
\par
Je nach Datenmodell kann man von den Kern-Nodes auch durchaus mehrere machen, aber das muss dann anwendungslogisch sauber designt sein. Z.B., keine Ahnung, Kundendaten, die sich wirklich sch\'f6b sauber disjunkt von A bis Z durchsegmentieren lassen, wodurch ein Client f\'fcr die Bearbeitung eines Kunden immer nur auf einem Kern-Node arbeiten muss. Es kann aber gut sein, dass das bei den meisten Anwendungen nicht so aufteilbar ist.\par
\par
Und man kann nat\'fcrlich immer noch ma\'dfgeschneiderte Speziall\'f6sungen machen. Wenn etwa permanent millionen Messdatens\'e4tze reinsprudeln, dann speichert man die nicht unbedingt jeden als einzelnes Entity, sondern man speichert die in Tauserbl\'f6cken direkt als Binary Blob ab.\par
\par
Dieses Konzept hat immer Clustering f\'fcr Ausfallsicherheit:\par
Die Data-Slaves bekommen permanent live die neuesten Updates und stehen jederzeit bereit, als neuer Master zu \'fcbernehmen.\par
\par
Es hat auch immer noch Clustering f\'fcr Lastenverteilung:\par
Die Data-Slaves nehmen dem Data-Master die gesamte Lese-Arbeit ab. Der Data-Master muss nur noch validieren und schreiben. Und ich wette, das geht genauso schnell, wie wenn ein chaotischer Haufen an permanent race-condition-konkurrierenden Nodes sich st\'e4ndig untereinander synchen m\'fcssen.\par
\par
Und es hat auch immer noch Clustering f\'fcr Skalierung:\par
Alles, was "gro\'df" ist, wird in andere Systeme ausgelagert. Der "Kern" Data-Master k\'fcmmert sich nur um "kleine" und "hei\'dfe" Daten.\par
\par
\par
Eine nette kleine Testfrage f\'fcr Konzepte ist immer: "K\'f6nnte man damit Facebook nachbauen?"\par
Ja, k\'f6nnte man, denn:\par
Alle Bilder, Videos, evtl. sogar die Strings der Nachrichten und die Werbungen sowieso, kommen von anderen Servern.\par
Die >1 Milliard Benutzer kann man ziemlich leicht auf mehrere Data-Master verteilen. So ein Benutzer ist eine recht abgeschlossene Sache. Profildaten, Kontakte, Nachrichtendaten. Das kann sich das Browser-Frontend je Benutzer von verschiedensten Data-Master Nodes holen, ohne dass es jemals einen Konflikt mit der Aktualit\'e4t gibt. Die k\'f6nnten dann jeweils gleich in den entsprechenden Regionen stehen. Ihre Data-Slaves stehen dann nat\'fcrlich verteilt \'fcber die ganze Welt, aber wenn f\'fcr einen bestimmten Benutzer Daten validiert&geschrieben werden, landet das immer bei dem EINEN zust\'e4ndigen Data-Master.\par
\par
\par
\par
2018-10-18\par
\par
Ich schreib mal kurz eine Idee zu einer klareren "Datensatzklassifizierung" von oben auf.\par
\par
\par
\b Daten vierter Klasse\par
\b0 Gro\'dfe ValueType Objekte, die mehr den Charakter von "Nutzlast" als von Datenmodell-Entities haben.\par
Beispiele: Bilder, Audio, Video, Dokumente, gro\'dfe Texte (> 1000 Zeichen).\par
Werden in einem ausgelagerten System gespeichert und \'fcber irgendeinen eindeutigen Identifier (UUID oder so) referenziert. Irgendein Cloud Storage Ding oder so.\par
Anwendungsserver m\'fchen sich mit solchen Daten selbst normalerweise gar nicht ab, sondern schicken Clients (Browser, App, etc.) nur den Identifier und die sollen sich das Ding bei Bedarf selbst runterladen, statt dass sie es m\'fchsam vom Anwendungsserver geliefert bekommen. Das Cloud Ding kann dann in sich geclustert, repliziert, wei\'df der Geier sein, das braucht den Anwendungsentwickler nicht k\'fcmmern.\par
Diese Objekte sind immutable, d.h. sie ver\'e4ndern sich niemals. Sollte es mal eine \'c4nderung an den Bytes geben, f\'fchrt das zu einer neuen Version mit einem neuen Identifier. Aber ein Identifier steht immer f\'fcr eine bestimmte "Kombination an Bytes". Unver\'e4nderlich f\'fcr alle Ewigkeit.\par
Wahrscheinlich muss man von Zeit zu Zeit "aufger\'e4umt" werden: Alle live referenzierten Identifier sammeln und dem Storage Ding sagen "L\'f6sch von mir alles, au\'dfer folgende". Oder umgkehrt: Eine Liste aller verf\'fcgbaren geben lassen und alle nicht mehr live referenzierten l\'f6schen. Das sind technische Details.\par
\par
Durch die Auslagerung dieser Daten nimmt man schon mal gigantisch viel Volumen aus der Datenbank raus. \'dcbrig bleiben nur noch "echte Datens\'e4tze", die typischerweise um die 50-100 Byte gro\'df sind (Collections etwas gr\'f6\'dfer).\par
\par
\b Daten dritter Klasse\par
\b0 Immutable Entities. Da die sich nie \'e4ndern, kann man sie beruhigt auf einen anderen "Master" Node auslagern und von dort bei Bedarf immer wieder laden. Es wird bei denen nie Probleme mit Schreib-Konflikten oder veralteten Caches geben.\par
Beispiele: \par
Schwer zu sagen, weil ziemlich anwendungsspezifisch. Vielleicht soetwas wie Messdaten: Die werden einmal erhoben und \'e4ndern sich dann nie wieder. Millionen von Datens\'e4tzen, die zwar schon "Entities" sind, aber die in ihrer Natur so leicht zu handeln sind, dass es Verschwendung w\'e4re, ihnen eine erstklassige Behandlung zukommen zu lassen. Nat\'fcrlich macht es keinen Sinn, jedes kleine immutable Einzelinstanz in ein andere System zu verschieben. Etwa einen Integer mit Wert 5. Sondern hier sind gro\'dfe Mengen an zusammengeh\'f6renden immutable Entities gemeint.\par
Wie das so ganz genau ist mit dem Immutable ist immer so eine Sache: Irgendwann m\'fcssen die Entities ja mal erzeugt und registriert, z.B. in eine Collection gesteckt werden. Also so "ganz immutable" kann es nicht sein.\par
Hier ist der \'dcbergang zur zweiten Klasse flie\'dfend, eventuell ist beides sogar einfacher als dasselbe abbildbar.\par
\par
Durch die Auslagerung dieser Entities l\'e4sst sich die Gesamtzahl der erster Klasse zu managenden Entities dramatisch reduzieren, ohne sich dabei architektonische Probleme (Konflikte) einzuhandeln.\par
\par
\par
\b Daten zweiter Klasse\par
\b0 Gro\'dfe Mengen an Entities, die sich nur selten \'e4ndern. "Selten" hei\'dft in der Welt von Computern: Nur alle Stunde oder noch seltener.\par
Diese werden auf einen eigenen "Master" Node ausgelagert.\par
Die Idee dahinter ist: Das Zeitintervall soll gro\'df genug sein, dass es sich rentiert, dass der zust\'e4ndige Node an die Anwendungsnodes bei jeder \'c4nderung signalisieren kann: "Bei mir hat sich was ge\'e4ndert, du musst alle deine Entities, die du von mir gecacht hast, invalidaten/clearen und bei Bedarf neu laden."\par
Wenn die Intervalle zu kurz w\'e4ren (~Sekunden), w\'fcrden die Anwendungsserver st\'e4ndig nur noch caches leeren und neu aufbauen, darum "seltene" \'c4nderungen. Als Optimierung f\'fcr kurze Intervalle nicht "alles" clearen, sondern nur ge\'e4nderte Instanzen, w\'fcrde schon wieder zu viel Verwaltungsoverhead erzeugen: Rechenaufwand, um alle ge\'e4nderten IDs zu sammeln, IDs \'fcbertragen, in den Anwendungsnodes die ID Liste durchlaufen, aufl\'f6sen, die entsprechenden Instanzen invalidaten. \par
Was ist dann mit Instanzen von und auf diese Instanzen zu eigentlich unver\'e4nderten Instanzen? Alles schwierig. Darum simpler Ansatz: ALLES, was von dem betreffenden Second-Class Masternode gecacht ist, einfach clearen.\par
Auch hier w\'e4ren Beispiele stark anwendungsabh\'e4ngig. \par
Viele Anwendungen m\'f6gen solche Art von Daten \'fcberhaupt nicht haben. Viele andere haben fast nur solche. Vielleicht soetwas wie Artikeldaten. Eine Million Artikel und all ihre Entities drum rum. Da \'e4ndert sich schon mal immer wieder was dran. Vielleicht mehrfach t\'e4glich, wenn Sachbearbeiter \'c4nderungen vornehmen. Vielleicht werden nur einmal t\'e4glich batchm\'e4\'dfig alle Artikel\'e4nderungen auf einmal im System "ver\'f6ffentlicht".\par
Oder ein Beispiel aus der pers\'f6nlichen Entwicklererfahrung: Bonusberechnungen. Es gibt Vorg\'e4nge, die werden vom Sachbearbeiter ausgef\'fchrt und die berechnen dann aus Ums\'e4tzen und Stammdaten, wie viel Kohle jedem Mitglied ausgezahlt wird. Jeder Vorgang erzeugt schon mal so 100.000 Entities oder noch mehr. Aber diese Vorg\'e4nge laufen vielleicht ... naja ... 10 mal im Monat. Selbst, wenn ein Sachbearbeiter herumprobiert, ausbessert, Daten nachtr\'e4gt, usw. und daf\'fcr immer wieder Vorgang ausf\'fchren und wieder resetten muss, dann passiert das vielleicht ab und zu 10 mal an einem Tag. Bei solchen Intervallen ist es noch problemlos m\'f6glich, in der Anwendungslogik einfach ALLES gecachte wegzuwerfen und bei Bedarf neu zu laden.\par
\par
Durch die Auslagerung dieser Entities l\'e4sst sich die Gesamtzahl der erster Klasse zu managenden Entities dramatisch reduzieren, ohne sich dabei architektonische Probleme (Konflikte) einzuhandeln.\par
\par
\b Daten erster Klasse\par
\b0\'dcbrig bleiben dann nur noch Entities "Erster Klasse". Kleine Entities, die sich potenziell sehr oft \'e4ndern und wo sich unter Umst\'e4nden sogar mal nur ein einzelnes Entity \'e4ndert.\par
Klassisches Beispiel: Kunde \'e4ndert sein Profil. Anschrift, Einstellungen, irgendsowas.\par
Nachdem alle Daten niedrigerer (einfacherer) Klasse in andere Systeme ausgelagert wurden, bleibt hier nicht mehr viel \'fcbrig.\par
Die kann ein einzelner Anwendungsserver, sozusagen der "First Class Data-Master", selbst direkt handeln.\par
Das sind nat\'fcrlich immer noch Millionen Entities. Wahrscheinlich auch Milliarden. Aber das ist mit Jetstream kein Problem. Wirklich nicht.\par
Beispiel Bonusanwendung: Besteht aus ~20 Millionen Entities und sogar unsere 0815 B\'fcro-PCs als "Serverhardware" brauchen nur etwa 15 Sekunden, um die gesamte Datenbank zu initialisieren und den Server zu starten.\par
Wenn man eine einem gesch\'e4ftlichen Unternehmen angemessene Serverhardware einsetzt (sowas wie 8 Core, 128 GB RAM oder auch 32 Core, 512 GB RAM, das ist heutzutage alles kein Drama mehr), dann dann der mit \'e4hnlicher Geschwindigkeit auch 1 Milliarde solcher "First Class" Entities managen.\par
\par
---\par
Kleine, spontane Recherche zu aktueller Serverhardware:\par
{{\field{\*\fldinst{HYPERLINK https://calculator.s3.amazonaws.com/index.html }}{\fldrslt{https://calculator.s3.amazonaws.com/index.html\ul0\cf0}}}}\f0\fs28\par
x1e.32xlarge: 128 CPUs, 4 TB (!) RAM, Windows, dedicated Server. Kostet $32 pro Stunde.\par
Das mag jetzt viel klingen, $32 pro STUNDE. Das sind $23K pro Monat. Aber man muss sich vorstellen, was so ein Monster, das sich nur noch um "Entities erster Klasse" k\'fcmmern muss, abarbeiten kann: Tausende gleichzeitige Kunden. Das hei\'dft aufs Monat gesehen hunderttausende bis Millionen durchgeschleuste Leute, die wahrscheinlich hundertausende bis Millionen an Umsatz generieren. Das sollte 23K f\'fcr den Betrieb der Gesch\'e4ftsgrundlage schon rechtfertigen, sonst liegt da irgendwo eine betriebswirtschaftliche Wahrnehmungsverzerrung vor. Ich wette, so viel zahlt ein Supermarkt schon allein f\'fcr den Strom. Mit Replikationsservern wird es nat\'fcrlich mehr. Dann nimmt man entweder mehrere kleinere oder man kann daf\'fcr ja auch das X-fache an Kunden bearbeiten.\par
...\par
Kurz mal gegoogelt: Deutschsprachiger Einzelhandel Jahresstromverbrauch: ca. 100-300 kW/h. Z.B. real hat L\'e4den von 5000-15000 m\'b2. Also sagen wir 10.000 m\'b2 mit 200 kW/h. Preis, sagen wir g\'fcnstig verhandelte 20 Cent pro kW/h.\par
Macht... 33K \'80 pro Monat. Ha! Gut gesch\'e4tzt.\par
---\par
\par
\par
Und wenn das immer noch nicht reicht, dann kann man seine Anwendung auch gezielt so designen, dass man mehrere solcher "First Class Data-Master" hat. Das sind halt dann disjunkte "Welten", die auf irgendeine g\'fcnstige Art zusammenarbeiten. Das wichtige ist dabei nur, zu verhindern, dann Daten des anderen rumzu\'e4ndern oder veraltete Versionen im Cache zu haben.\par
Ich denke da z.B. an Facebook: Selbst die dickste Einzelhardware ever kann unm\'f6glich alle >2 Milliarden Benutzer und alle ihre dranh\'e4ngenden "First-Class" Entities managen.\par
Das muss unweigerlich irgendwie segmentiert, geshardet werden. Aber das ist ziemlich simpel: man baut sich 1000 First-Class Data-Master Anwendungsnodes und jeder von denen ist "nur" f\'fcr 2 Millionen User zust\'e4ndig.\par
Oder 10.000 Nodes mit je 200.000 User. Man kann auch 1 Million kleine Nodes mit je 2000 Usern drauf machen. Spielt keine Rolle. Es wird hier nie Konflikte geben, weil die Daten in sich auf fachlicher Ebene geclustert sind. Jeder User ist f\'fcr sich eine relativ abgeschlossene Einheit. Es gibt nat\'fcrlich Kontakte und Nachrichten untereinander, aber das kann man schon irgendwie sinnvoll managen. Entweder die Nachricht liegt immer auf dem Node des Senders oder, warum nicht: Es gibt einen extra Nachrichten Node (oder 1000), der (die) nichts anderes macht, als Millionen Nachrichten zwischen den Usern zu handeln.\par
Also ich will damit nur sagen: Die Aussage "Es gibt nur einen First-Class Node" hei\'dft nicht, dass es auf Gedeih und Verderb f\'fcr egal wie gro\'dfe Systeme immer nur einen zentralen Node geben kann. Es soll nur hei\'dfen, dass ein First-Class Node die gr\'f6\'dfte Einheit ist, die automatisch verwaltet werden kann. Wenn man mehr braucht, muss man seine Anwendung entsprechend strukturieren und untereinander kommunizieren lassen, dann geht das auch.\par
\par
Das Konzept ist kurz gesagt: Daten in verschiedene Klassen einteilen und je nachdem, wie viel oder wenig unkompliziert sie zu verwalten sind, kann man die Daten mehr oder weniger weit vom zentralen Anwendungsnode weg auslagern.\par
Dazu noch so read-only Data-Slaves, die den Master entlasten und failover-absichern, dann hat selbst bei riesigen Systemen ein einzelner (durchaus leistungsstarker) Master-Node genug Luft, um Anwendungslogik mit gigantische Datenmengen auszuf\'fchren und gew\'fcnschte \'c4nderungen business-logic-m\'e4\'dfig vor dem Schreiben zu validieren. Ganz ohne Schreibkonflikte im Cluster.\par
\par
\par
\par
##################\par
\par
Mir ist dazu eine super Metapher eingefallen:\par
\par
Sagen wir, eine Serveranwendung ist wie ein Laden, in dem man Dinge kaufen kann.\par
Die Artikel sind die Datens\'e4tze. Die Mitarbeiter sind die Threads.\par
Wenn ein Kunde nur reinkommt, rumschaut, sich ggf. noch beraten l\'e4sst, ist das "nur lesend".\par
Wenn ein Kunde etwas kauft, d.h. Warenbestand und Einnahmen des Ladens ver\'e4ndert, ist das "schreibend".\par
Es gibt unterschiedliche Arten von Artikeln:\par
Riesige, aber triviale Artikel, z.B. Holzbalken, Steine, usw. zum Hausbauen.\par
So mittelgro\'dfe, mittelkomplexe Artikel, wie z.B. ... keine Ahnung ... Lampen, W\'e4schek\'f6rbe, Lebensmittel.\par
Und es gibt kleine, superkomplexe Artikel, die viel Aufwand erzeugen f\'fcr Ausstellung, Beratung, Reklamation, evtl. sogar Reparatur. Klassisches Beispiel: Elektronikkram. Fotos, Smartphones, usw.\par
\par
Java mit Jetstream macht es problemlos m\'f6glich, alle m\'f6glichen Artikel gemischt im selben Laden zu verkaufen. Alles kann angeboten werden und das auch ziemlich effizient.\par
Soweit so cool.\par
Nat\'fcrlich ist es so, dass, je mehr Waren man anbieten will, man auch immer mehr Verkaufsfl\'e4che und Mitarbeiter f\'fcr Beratung und Kassieren braucht.\par
F\'fcr Java kein Problem. F\'fcr Jetstream auch nicht. Nur die Hardware macht irgendwann nicht mehr mit. G\'e4be es einen 50 GHz 1024-Core Server mit 128 TB RAM, dann w\'e4re die ganze Clusterei \'fcberfl\'fcssig. Gibts leider nicht. Bl\'f6d.\par
In der Metapher hei\'dft das: Aus irgendeinem Grund darf ein Laden nicht mehr als 100.000 Artikel anbieten und nicht mehr als 32 Mitarbeiter haben.\par
Man kann diese Grenzen noch so clever ausnutzen und ein noch so effizient laufendes Lager haben, irgendwann platzt ein einzelner Laden aus allen N\'e4hten und die Mitarbeiter kommen mit dem Beraten und Kassieren nicht mehr nach.\par
\par
Das muss man \'fcber mehrere L\'e4den clustern. Geht nicht anders.\par
\par
Der erste, durchaus naheliegende, aber auch naive Gedanke ist: Das hei\'dft einfach X mal 100.000-Artikel-L\'e4den mit X mal 32 Mitarbeiter und die Waren dann "irgendwie" auf die L\'e4den verteilen.\par
Das Problem daran ist: Die Kunden und Mitarbeiter werden vor lauter herumtelefonieren und herumlaufen zwischen den L\'e4den nicht mehr fertig. St\'e4ndig muss rumtelefoniert werden, ob es von Artikeltyp X in einem anderen Laden noch ein Exemplar gibt, dann m\'fcssen die Kunden dorthin laufen, usw. Der Vergleich hinkt ein bisschen, verglichen damit, wie Cluster wirklich funktionieren, aber die Grundaussage ist richtig: Es gibt einen riesigen Overhead an Koordinierungsbedarf, Konflikte bei "schreibenden" Zugriffen ("kaufende Zugriffe", hehe), Austausch der Waren zwischen den M\'e4rkten, usw.\par
\par
Darum folgende Idee, das ganze etwas eleganter zu clustern:\par
Ein Laden k\'fcmmert sich nur um die riesigen, aber trivialen Artikel. Der hat dann vielleicht sogar optimierte Lagerfl\'e4chen und Ausstellungsfl\'e4chen, Kabelstapler, usw.\par
Ein Laden k\'fcmmert sich um alles zu k\'fchlende Zeug (Lebensmittel). Mit K\'fchlanlagen usw.\par
Ein Laden k\'fcmmert sich um das ganze kleine, komplexe und beratungsintensive Zeug (Smartphones usw.). Mit schicken Ausstellungsst\'e4nden, Lounges mit Musik, usw.\par
\par
Zus\'e4tzlich macht man von jedem der drei "Hauptl\'e4den" noch identisch best\'fcckte "Ausstellungsl\'e4den".\par
(Hier hinkt das Beispiel wieder ein bisschen: F\'fcr physikalische L\'e4den w\'e4re das ein riesiger Aufwand, aber in der digitalen Welt ist eine simple Kopie ziehen trivial zu machen).\par
In den Ausstellungsl\'e4den k\'f6nnen die Kunden durchlaufen, sich die Artikel ansehen, ausprobieren, sich beraten lassen, usw.\par
Aber gekauft wird nur im Hauptladen, damit an einer zentralen Stelle alles richtig verbucht wird.\par
Je mehr Kunden es gibt, umso mehr Ausstellungsl\'e4den macht man. Das ist \'fcberhaupt kein Problem. So eine Kopie ist schnell erstellt (in der Digitalen Welt), dann gehen die T\'fcren auf und 1000 weitere Kunden k\'f6nnen fr\'f6hlich reinstr\'f6men und rumschauen.\par
Die Hauptl\'e4den machen hingegen gar keine Beratung mehr. Dort geht man nur noch rein, wenn man schon ausgesucht hat, was man kaufen will. Man kriegt es ausgeh\'e4ndigt und geht zur Kasse.\par
Dadurch k\'f6nnen auch alle 32 Mitarbeiter des Hauptladens an den Kassen sitzen und kassieren.\par
Und jetzt kann man sich gut vorstellen: 32 Kassen nebeneinander, die den ganzen Tag lang ununterbrochen nur noch Kunden mit Ware in der Hand abkassieren, ohne Beratung usw., k\'f6nnen ein gigantisches Volumen an Kunden und an Warenausgabe bearbeiten.\par
\par
Es muss auch nicht unbedingt bei diesen drei L\'e4den bleiben. Man kann durchaus einen zweiten Laden f\'fcr kleine, komplexe Artikel aufmachen. Das m\'fcsste halt dann thematisch getrennt sein, damit nicht wieder Chaos untereinander ausbricht.\par
Etwa wie Mediamarkt und Obi. Der eine verkauft Elektronikschnickschnack, der andere verkauft Baustuff. Je nachdem, was man braucht, f\'e4hrt man zu dem einen oder dem anderen. Wenn \'fcberhaupt, gibt es zwischen beiden nur minimalen Koordinationsbedarf und \'fcberhaupt keine "schreibenden" Konflikte.\par
\par
Genauso ist die Idee mit den nach Datenklassen und mit Read-only Nodes geclusterten Servern.\par
\par
##################\par
\par
\par
\par
Da sind nun im Detail nat\'fcrlich noch ein paar Fragen offen: Wie erfolgt diese Zerlegung technisch genau?\par
Was die Binary ("viertklassigen") Daten angeht:\par
Etwas \'e4hnliches wie die Lazy Reference, die intern nur den Identifier h\'e4lt.\par
Falls die Serveranwendung selbst die Binary Instanz braucht, kann sie sie damit laden.\par
Ansonsten wird dem Client nur der Identifier (bzw. ein entsprechender Wrapper) mitgeschickt und der l\'e4dt sich die Binary Instanz (oder vielleicht sollte man es eher "Resource" nennen) selbst.\par
So ein "Wrapper" kann z.B. in der HTML Welt einfach ein <img> oder <video> tag sein, in dem URL+Identifier drin steht.\par
F\'fcr eine Client App w\'e4re es wohl eher eine echte Wrapper Instanz, die URL+Identifier intern h\'e4lt und bei Bedarf eine Verbindung aufbaut, l\'e4dt, Verbindung schlie\'dft, lokal cacht, zur\'fcckgibt.\par
Das sollte trivial machbar sein.\par
\par
Die "First Class" Entities sind auch simpel:\par
Das ist ein Good Old Oldschool Java Datenmodell, ohne Einschr\'e4nkungen, mit Lazy Referenzen an g\'fcnstigen Stellen. Fertig.\par
\par
Knifflig ist das Zwischending: Viele Entities, die sich selten \'e4ndern, auf einem anderen System.\par
Das m\'fcsste auch ein eigenst\'e4ndiger Teil mit Anwendungslogik sein. Vielleicht nur sehr wenig. Eher wie eine "Lagerhalle" der Anwendung. Mit eigenem Entitygraph.\par
Der Hauptteil der Anwendung muss die Entitytypen daf\'fcr aber auch kennen. Und bei Bedarf muss er sich die Entities vom Lagerhallennode reinladen k\'f6nnen.\par
An den "Hauptteil" des Entity Graphen w\'e4re dieser Subgraph wohl angeschlossen \'fcber auch wieder eine spezielle Lazy reference, die gecleart wird, sobald der "Lagerhallennode" ein Signal schickt, dass seine Daten sich ver\'e4ndert haben.\par
Das knifflige ist jetzt: Es muss sichergestellt werden, dass bei so einem "clear" tats\'e4chlich alle Instanzen, die persistent eigentlich im "Lagerhallennode" "wohnen", auch wirklich "gel\'f6scht", also vom JVM GC collectet werden.\par
Dazu ist n\'f6tig:\par
- Root-Lazy-Reference des "importierten" Subgraphen clearen.\par
- GC muss zwangsweise laufen, um die Instanzen des Subgraphen aufzur\'e4umen. System.gc() explizit aufrufen als Teil des Produktivbetriebs ist irgendwie immer doof. Und man hat keine Garantie, dass der GC dann wirklich komplett durchl\'e4uft.\par
- richtig knifflig: es darf keine einzige der "First Class" Entities so eine importierte "Second Class" Entity hart referenzieren, sonst kann diese nicht aufger\'e4umt werden. L\'f6sung: wenn Referenzieren, dann weak. Aber das muss im Anwendungsdesign peinlich genau ber\'fccksichtigt werden.\par
\par
Das hei\'dft: alle so ausgelagerten Entities m\'fcssen ein in sich abgeschlossener Subgraph sein, der importiert und wieder geclearet werden kann.\par
\par
\par
Hm. Vielleicht ist das aber zu kompliziert gedacht. Das ganze dirty-notification-Zeug mit n\'f6tigem GC-Aufruf ist seltsam.\par
\par
Vielleicht ist folgendes besser:\par
\par
Die Second-Class Entities werden vom First-Class Anwendungsserver erzeugt. Muss ja eigentlich so sein, weil nur der die ganze Anwendunglogik und die n\'f6tigen First-Class Entities f\'fcr Validierungen usw. hat.\par
Aber der Trick ist: Weil das sich eher selten \'e4ndernde, aber daf\'fcr viele Daten sind, schiebt der First-Class Master die an einen Second-Class Master ab. Genauer gesagt schiebt er einen ganzen Subgraph ab, der nur \'fcber eine Root referenziert wird und auf den es keine direkten Referenzen "von der Seite" gibt. Lazy m\'fcsste okay sein.\par
Der Second Class Master hat selbst keine Anwendungslogik und er bekommt \'c4nderungen immer nur vom First-Class Master. Damit ist gar kein Dirty-Signal n\'f6tig.\par
Nat\'fcrlich muss damit der First-Class Master die ganze Logikarbeit machen, die Entities zu erzeugen, Operationen darauf zu validieren, usw.. Aber das macht nichts. Zum einen muss/kann das eh nur er machen. Zum anderen ist das Ziel ja nicht,\par
dem First-Class Master Rechenleistung f\'fcr die Logik abzunehmen, sondern nachrangige Daten aus seiner Datenbank auszulagern, damit die m\'f6glichst klein und auf das wichtigste konzentriert bleibt.\par
\par
Das sch\'f6ne daran ist:\par
Damit werden l\'e4stige Altdaten automatisch "aus dem System raus" gebracht, aber sind trotzdem noch verf\'fcgbar.\par
Etwa Ums\'e4tze von Vorjahren. Irgendwann nach 5 Jahren wird der First-Class Server die Ums\'e4tze von vor 5 Jahren einfach gar nicht mehr brauchen. Er verschwendet damit auch gar keinen Platz auf Festplatte oder RAM. Er fordert die Subgraphen nur einfach nie wieder vom seinem Second-Class Handlanger-Node an.\par
Und was auch sch\'f6n ist:\par
Wenn eine passende Subgraph Datenstruktur (z.B. eben Ums\'e4tze) einmal konzipiert ist, dann k\'f6nnen Instanzen davon auf mehrere Second-Class-Nodes verteilt werden.\par
Die Ums\'e4tze 2021-25 auf den Second-Class-Node #1, dann schafft man einen zweiten an, der h\'e4lt dann 2026-2030, usw.\par
Irgendwann werden vielleicht mal mehrere der Subgraphen wieder auf einen Node zusammenkopiert, der als Archiv dient: Riesige Festplatte, aber wenig Rechenleistung. Wenn jemand die Uralt-Daten braucht, werden die schon geliefert, aber halt nicht mehr mit maximaler Produktivbetrieb-Hot-Performance.\par
\par
Also w\'e4r es so:\par
\par
1.) Es gibt einen First-Class Master Node.\par
- Der ist die "komplette Anwendung", sowohl was Logik als auch Datenmodell angeht.\par
- Der ist der einzige, der Anfragen f\'fcr \'c4nderungen an den Daten annimmt und neue Entities erzeugt.\par
- Der wird auf beliebig viele First-Class Slave Nodes repliziert, die ihm lesende Arbeit anehmen, damit er sich auf die Verarbeitung von \'c4nderungen konzentrieren kann.\par
- Der lagert aber alle Nicht-First-Class Entities an andere Nodes aus. Diese m\'fcssen in autarken Subgraphen organisiert sein.\par
\par
2.) Es gibt N Second-Class Master Nodes.\par
- Die bekommen Subgraphen vom First-Class Master Node zugeschickt, die sie einfach nur dumm persistieren und auf Anfrage wieder rausgeben. Ohne Anwendungslogik. Ohne Validierung oder sowas.\par
- Deren eigenes Datenmodell sieht wahrscheinlich einfach so aus, dass ihr Root eine Map ist und jeder Subgraph, der rein kommt, wird mit seinem Root dort reingeh\'e4ngt und gespeichert.\par
- Auch die werden auf beliebig viele Read-Only Nodes repliziert, um die Lese-Last zu verteilen.\par
\par
3.) Es gibt irgendeinen Drittanbieter remote binary Storage Service.\par
- In den werden "gro\'dfe" ValueType Datenobjekte reingesteckt, die unter einem Identifier wieder abgerufen werden k\'f6nnen. Idealerweise gleich nur vom Client (Browser/App) selbst und nicht vom First-Class Master Node.\par
- Wie die replizieren o.\'c4. ist deren Sache, das kann uns egal sein. Wir speichern nur URIs und l\'f6sen die bei Bedarf wieder auf.\par
\par
\par
Der einzig knifflige Punkt ist damit:\par
Wie designt man in einer spezifischen Anwendung "sch\'f6n" autarke  Subgraphen, die man problemlos an so einen Second-Class Node auslagern kann?\par
Das sollte eine zumutbare Designentscheidung sein, genauso wie die Entscheidung von g\'fcnstigen Punkten f\'fcr Lazy References.\par
\par
\par
Naja, und:\par
- Wie verhindert man, dass ein Read-Only Slave Node seinen Entity Graph \'e4ndert?\par
- Wie werden Writes an den Master Node geschickt?\par
\par
Die Slaves Nodes k\'f6nnten auf jeden Fall schon mal Validierung auf fachlicher Ebene machen und damit dem Master Arbeit abnehmen.\par
\par
Oder k\'f6nnten die Read-Only Nodes vielleicht zwar schon \'e4ndern, aber sie speichern nicht, sondern ihr "store" schickt den Chunk an den Master und der validiert und speichert.\par
Aber was soll er validieren?\par
Fachliche Logik, klar. Daf\'fcr ist der Code da.\par
Aber wie soll er entscheiden, ob der Read-Only node wirklich die aktuellen Daten hatte?\par
Dann m\'fcsste eine Versions-ID mit rumgereicht werden.\par
Ins Entity einbauen ist bl\'f6d. Aber es k\'f6nnte eine HashTable sein, deren Entries mit jedem Chunk mitgeschickt werden.\par
Ein Entry w\'e4re eine ziemlich kleine Sache: 16 Bytes. 8 f\'fcr die OID, 8 f\'fcr die versions ID.\par
Der Slave bekommt die aktuellen IDs automatisch \'fcber das replizieren.\par
Beim zur\'fcckschicken an den Master schickt er einen "Auszug" der Versionstable mit, in der f\'fcr jedes geschickte Entity die 16 Bytes dabei sind.\par
Dann kann der Master checken: Hatte der Slave \'fcberhaupt die aktuelle persistente Version je Entity?\par
Wenn ja und wenn die fachliche Logik kein Problem meldet, kann der Master den Chunk direkt produktiv verwenden, d.h. direkt auf die Platte schreiben.\par
Alle anderen Slaves bekommen die Updates dann \'fcber die Replizierung.\par
\par
Wenn die Version nicht passt, bekommt der slave eine Exception zur\'fcck, die entweder kompensiert werden kann, oder dem User als "Die Daten haben sich zwischenzeitlich ge\'e4ndert, bitte neu laden" mitgeteilt wird.\par
\par
Dann w\'e4re die Anwendung verteilt auf replizierte Nodes.\par
Jeder node k\'f6nnte die volle Anwendungslogik ausf\'fchren auf potenziell den vollen Daten, aber schreiben w\'fcrde nur ein Node.\par
\par
Problem daran k\'f6nnte sein:\par
Wenn's dumm l\'e4uft, bekommt ein Slave gerade solche Anfragen zugewiesen, die f\'fcr die Bearbeitung die halbe Datenbank im RAM erfordern. Anfrage 1 braucht die ersten 10% Daten, #2 die zweiten 10%, usw.\par
Da kann ein Slave nach nur wenigen Anfragen schon seinen ganzen RAM voll haben und abschmieren.\par
\par
Es m\'fcsste halt dann die Anwendung so gebaut sein, dass nicht zu viele ge\'e4nderte Instanzen und nicht zu viele Konflikte entstehen.\par
Beispiel:\par
Ein User kann meistens nur in "seinem Bereich" viel herum\'e4ndern (sein Profil, seine Bestellungen, seine Mails, usw.) und Interaktion mit anderen l\'e4uft meistens nur darauf hinaus, Subgraphen neu zu erzeugen (z.B. eine Message Instanz mit Timestamp und Text) und irgendwo anders synchronisiert "einzuh\'e4ngen". Die neu erzeuten Message Instanzen k\'f6nnte ein Slave Node vorbereiten, nur das "einh\'e4ngen" kann einen Konflikt erzeugen. Bei einer Versionsexception kann der Slave sich die aktuelle Version holen und das einh\'e4ngen wieder versuchen. Wobei f\'fcr soetwas eine explizite Registrierlogik sinnvoller w\'e4re. Alle slaves senden ihre einzuh\'e4ngenden Subgraphen an eine registrierungsfunktion des masters und der arbeitet die entstehende Queue sequenziell ab.\par
\par
Hei\'dft: wie implementiert man eine enqueue-Logik, wenn alle Teilnehmer eigentlich Klone desselben Systems sind und nur Daten hin und her schicken, nicht Services aufrufen?\par
Dann k\'f6nnten die Slaves aber nicht einfach generisch ein store() aufrufen, als w\'e4ren sie der Master, an den aber eigentlich nur weitergeleitet wird. Sondern es m\'fcsste so command-Instanzen verschickt werden, die der Master dann auf Methoden mappen k\'f6nnte.\par
Aber wie kriegt man das generisch in den Store rein?\par
K\'f6nnte der Master generisch anhand des Typs der Rootinstanz des gesendeten Subgraphen entscheiden? Sowas wie GuestBookEntry. Und je nachdem, ob dieser Subgraph eine neue oder schon bekannte ID hat, w\'e4re es ein "handleNew" (z.B. enqueue) oder ein "handleExisting" (z.B. edit) ?\par
K\'f6nnte sowas in den Persistence layer reingewoben werden? Callbacks im Builder und im TypeHandler?\par
Also wenn der Master von einem Slave einen neuen GuestBookEntry geschickt bekommt, dann wei\'df er, was er damit machen muss.\par
Das ist Detal aber eklig, denn der GuestBookEntry m\'fcsste dann alle n\'f6tigen Informationen enthalten (welches Ziel-Guestbook, usw.). Das w\'e4ren R\'fcckrefrenzen, die in der Datenbank aber sinnlose Redundanzen darstellen w\'fcrden.\par
Also m\'fcsste man doch wieder Logik-Level Command-Instanzen drum rum wrappen, die zum persistieren aber weggelassen werden.\par
Dann w\'e4re aber nichts mehr mit node-typ-agnostisch einfach store aufrufen.\par
\par
Oder vielleicht w\'fcrde ja folgendes gehen:\par
Anwendungsdesignm\'e4\'dfig wird JEDE Aktion, die eine Daten\'e4nderung bewirken kann, in ein Command-Objekt gepackt.\par
Das wird an eine "handleCommand" methode \'fcbergeben.\par
Beim Slave hei\'dft "handleCommand": Command-Objekt serialisieren und an den Master schicken.\par
Beim Master leitet der Netzwerk-Eventthread bekommene Command-Objekte wieder an sein "handleCommand" weiter.\par
Das macht dann: je nach Kommando entsprechende Methode aufrufen.\par
Dabei f\'e4llt das Kommando-Objekt selbst weg.\par
Wenn die Methode gr\'fcnes Licht gibt, wird ein Store aufgerufen.\par
\par
Beispiel:\par
\par
Benutzer mit einer Session auf Slave #37 will sein Passwort \'e4ndern.\par
Die User-Instanz steht fest (von der Anwendungslogik ermittelt). Das neue Passwort liegt vor.\par
Es wird ein "user.changePassword(newPassword)" aufgerufen.\par
Die Anwendungslogik darin validiert, ob das Passwort \'fcberhaupt zul\'e4ssig ist.\par
Ist es, also weiter:\par
Es wird ein "CommandChangePassword" erzeugt und an "handleCommand" \'fcbergeben.\par
Beim Slave hei\'dft das serialisieren, das Command kommt beim Master an, der Master hat daf\'fcr die methode User#changePassword gemappt und ruft die mit den n\'f6tigen Daten auf.\par
Darin gibt es wieder Validierung.\par
Am Ende wieder "CommandChangePassword", das wieder an "handleCommand" \'fcbergeben wird. Aber beim Master f\'fchrt das nun zu einem Store.\par
Fertig.\par
\par
Falls es keinen Slave gibt, sondern der Benutzer mit seiner Session direkt auf dem Master arbeitet, funktioniert das genauso, nur ohne Netzwerk Umweg:\par
- "user.changePassword(newPassword)"  Aufruf\par
- validierung\par
- CommandChangePassword erzeugung\par
- \'fcbergeben an handleCommand\par
- handleCommand speichert direkt ab.\par
\par
Vielleicht sind die Command-Dinger auf der Master Seite sogar noch gut f\'fcr Logging oder sowas.\par
\par
Interessant ist jetzt noch die Frage:\line Muss der Master \'fcberhaupt nochmal Logik ausf\'fchren?\par
Reicht es nicht, darauf zu vertrauen, dass der Slave schon die Logik ausgef\'fchrt hat?\par
Es muss ja eigentlich nur validiert werden, ob der Slave von allen Daten die aktuelle Version hatte.\par
Vielleicht w\'e4re der Master dann nichts anderes als ein dummer Persistierungsnode, der Versionsnummern abgleicht und exceptions wirft.\par
Aber Vorsicht: ne, das geht nicht. Denn dann gibt es ja keine zentrale Stelle mehr, wo Logik ausgef\'fchrt wird. Das Passwort-Beispiel ist daf\'fcr zu primitiv. Besser w\'e4re so ein GuestBookEntry Beispiel:\par
Der Slave pr\'fcft grunds\'e4tzlich, ob der User einen Guestbook Entry machen darf. Er wei\'df aber nicht, an welcher Stelle im Guestbook der Entry letztendlich eingereiht werden wird. Das wei\'df allein der Master, der den aktuellen Stand der Daten hat.\par
Wenn grunds\'e4tzlich ja, erzeugt er ein Command daf\'fcr und schickt das an den Master.\par
Der Master nudelt das wie oben besprochen durch, aber in seiner add Entry Logik kann er mit einem vor\'fcbergehenden Lock nun die genaue Position DIESES Entries im Guestbook feststellen. So reiht er das ein, erzeugt ein Command, f\'fchrt das aus, persistiert das und der asynchron reingeschickte entry ist auf synchronisiert sichere Weise enqueut worden.\par
\par
So in die Richtung muss das funktionieren:\par
- Derselbe Anwendung l\'e4uft parallel auf mehreren Nodes\par
- Ein Node ist der Master, alle anderen nur Slaves\par
- Die Anwendungslogik arbeitet mit Schnittstellen aus Command-Instanzen\par
- Die Slaves handeln die Command Instanzen einfach nur so, dass sie sie an den Master schicken.\par
- Der Master handelt die Command Instanzen so, dass die anwendungslogik zentral und synchronisiert entscheidet und ggf. speichert.\par
- Gibt es nur einen Master ohne Slaves, dann ist das halt einfach eine normale Anwendung, in der ein kleiner Umweg mit Command-Instanzen gegangen wird, die aber f\'fcr Logging o.\'c4. sogar ganz vorteilhaft sein k\'f6nnen.\par
- Irgendwo beim Empfangen auf dem Master m\'fcssen dann noch Versionsnummern von bereits existierenden Entities gecheckt werden. Neue Entities sind immer erlaubt, die validiert dann nur noch die Anwendungslogik.\par
\par
\par
Es m\'fcsste folgendes Interface geben:\par
\par
public interface CommandHandler\par
\{\par
\tab public void handleCommand(CommandCreateGuestBookEntry);\par
\par
\tab // ... usw. ganz viele solcher handling Methoden\par
\}\par
\par
Das Command Basisinterface hat folgende Methode:\par
\par
public void handleBy(CommandHandler);\par
\par
Eine Command Implementierung kann dann die passende spezifische Logik f\'fcr sich selbst raussuchen.\par
Damit vermeidet man einen teuren Handler-Lookup.\par
\par
Die allgemeine "handleCommand" Methode bei einem Slave serialisiert das Command-Objekt und alle daranh\'e4ngenden Instanzen und schickt sie zum Master.\par
Die allgemeine "handleCommand" Methode beim Master sieht so aus:\par
\par
public void handleCommand(Command command)\par
\{\par
\tab command.handleBy(this.commandHandler);\par
\}\par
\par
Der CommandHandler kann das treiben, was immer er denkt. Validierung, Logging, Persistierung.\par
Selbst, wenn der Master allein ist, ist das eine sinnvolle Architektur, weil man eine zentrale Stelle hat, in der man loggen usw. machen kann.\par
Dieses Konzept gilt nur f\'fcr Aktionen, die Daten ver\'e4ndern, d.h. die letztendlich einen store aufrufen. Am besten sollte nur der CommandHandler intern \'fcberhaupt Zugriff auf eine StorageConnection haben. D.h. man MUSS \'fcber ein Command Objekt gehen, wenn man irgendwas gespeichert haben will.\par
\par
Rein lesende Logik braucht keine Commands.\par
\par
Wahrscheinlich sollte "Command" besser umbenannt werden zu "Request". Denn ein Kommando MUSS ausgef\'fchrt werden. Worum es hier ja aber geht, sind Anfragen, etwas zu tun, die auch abgelehnt werden k\'f6nnen.\par
\par
\par
Es k\'f6nnte auch f\'fcr den RequestHandler zwei Implementierungen (bzw. eine plus Wrapper) geben: Einmal mit Validierung und einmal ohne.\par
Denn:\par
Wenn der Master allein ist und sich die Request Instanzen nur "just for fun" (bzw. for logging) selber erzeugt, dann muss es sie kurz nach dem erzeugen nicht nochmal validieren. Das w\'e4re ja d\'e4mlich.\par
Wenn der Master aber Requests von au\'dfen rein kriegt, von einem Slave, der potenziell nicht die aktuellen Daten hat und darum evtl. invalide Requests erzeugt, dann sollten die beim Handeln durchaus nochmal validiert werden.\par
Die Implementierung kann ja je nach Fall gew\'e4hlt werden. Und sie k\'f6nnte sogar live im Betrieb von nicht validierend auf validierend gewechselt werden, wenn zu dem Master dynamisch ein Slave dazugeschaltet wird.\par
\par
\par
\par
Features:\par
- Nur eine monolithische Anwendung schreiben, ohne explizite Clustering API oder sowas.\par
- Beliebig viele Nodes, die dieselbe Anwendung ausf\'fchren und "scheinbar" alle schreiben.\par
- Nodes sind problemlos dynamisch zu- und wegschaltbar.\par
- Nodes sind automatisch Failover-Absicherung.\par
- Es gibt keine Inkonsistenzen von mehreren gleichzeitigen Schreibern (weil tats\'e4chlich nur 1 Node schreibt, der alle anderen updatet)\par
\par
Probleme/Nachteile:\par
- Es kann keine Inkonsistenzen geben, aber replizierte Nodes k\'f6nnen leicht veralteten Zustand haben.\par
- Wenn einer der replizierten Nodes speichern will, kann es darum eine Exception geben, weil sich der aktuelle Zustand ge\'e4ndert hat. Muss bei jedem Speicherversuch mit einem try-catch behandelt werden. Hei\'dft: entweder nochmal probieren mit aktuellen Daten oder an Benutzer zur\'fcckmelden und den nochmal versuchen lassen, ggf. mit anderen Daten.\par
\par
Anmerkung:\par
Dieses Problem hat man grunds\'e4tzlich immer, wenn mehr als ein Prozess dieselben Daten f\'fcr den Betrieb verwenden, egal, welches Konzept.\par
Die einzige M\'f6glichkeit, das zu verhindern, w\'e4re, dass der Schreibvorgang irgendwo im Cluster kurz den gesamten Cluster anh\'e4lt und nach dem Schreiben alle Nodes aktualisiert, damit keine mit veralteten Daten arbeiten kann.\par
Immerhin hat dieses Konzept nur leicht veraltete Daten und keine inkonsistenten Daten bzw. konfliktete \'c4nderungen.\par
Letzteres w\'e4re die H\'f6lle. Ersteres kann man immerhin relativ leicht mit einem erneuten Versuch beheben.\par
\par
\par
Oder mal \'fcberlegen ...\par
\par
\par
Zwei Arten von nebenl\'e4ufigen \'c4nderung:\par
(siehe auch "Starke und Schwache Kausalit\'e4t")\par
\par
1.)\par
Eine EXAKTE \'c4nderung.\par
Beispiel: Ein Benutzer will sich f\'fcr eine Kinovorstellung Platz Nr. 68 reservieren.\par
Entweder, er hat Erfolg, oder nicht. Es gibt keine akzeptable "ungef\'e4hre" L\'f6sung.\par
\par
2.)\par
Eine UNGEF\'c4HRE \'c4nderung.\par
Beispiel: Ein Benutzer will auf einer Pinnwand mit 5 Eintr\'e4gen einen weiteren Eintrag hinzuf\'fcgen.\par
Ob sein Eintrag tats\'e4chlich der 6. Eintrag wird oder ob zwischen dem Schreiben seines Eintrags und der Registrierung im System noch andere Eintr\'e4ge registriert wurden, die letztendlich vor seinem Eintrag stehen werden, stellt f\'fcr die Anwendung keinen Fehler dar.\par
(Ob der Benutzer selbst sich dar\'fcber aufregt oder sein Eintrag inhaltlich dadurch unsinnig wird, ist kein Problem f\'fcr die Anwendung)\par
Oder im Beispiel oben: den "n\'e4chsten" freien Platz der Vorstellung reservieren. Egal, welcher das ist.\par
\par
\par
Die allermeisten Aktionen in einem nebenl\'e4ufigen System sind ungef\'e4hre \'c4nderungen.\par
Es geht fast immer darum, neues hinzuzuf\'fcgen, einzureihen, zu belegen, usw.\par
Exakte \'c4nderungen sind in einem nebenl\'e4ufigen System eher selten.\par
\par
Bei exakten \'c4nderungen kann man Konflikte vermeiden, indem man anstatt direkte \'c4nderungen vorzunehmen (die konfliktet sein k\'f6nnen), eine \'c4nderungsanfrage an eine zentrale Stelle schickt, die alle Anfragen sequenziell abarbeitet.\par
Beispiel:\par
Anstatt in Platz 68 fest den Beleger "Benutzer X" einzutragen und irgendwann sp\'e4ter festzustellen, dass dort nebenl\'e4ufig schon "Benutzer Y" eingetragen worden ist, kann eine Anfrage \'fcber die Eintragung geschickt werden.\par
Benutzer Y sendet seine Anfrage zuerst. Benutzer X kurz darauf (scheinbar gleichzeitig bzw. ohne Sichtbarkeit von Y's Anfrage).\par
Beide Anfragen landen in einer Anfrage-Queue einer "Sitzevergeber" Logik.\par
Y bekommt den Platz und eine entsprechende R\'fcckmeldung.\par
X bekommt eine Ablehnung als R\'fcckmeldung.\par
\par
\par
\par
Man kann jede Situation parallel konflikteter \'c4nderungen ganz einfach sichern:\par
Indem nicht parallel direkt \'c4nderungen vorgenommen werden und erst wenn es zu sp\'e4t ist, merkt man, dass ein Konflikt / eine Inkonsistenz vorliegt (z.B. dass dasselbe Objekt von zwei verschiedenen Stellen unterschiedlich ver\'e4ndert worden ist), sondern indem man jede gew\'fcnschte \'c4nderung als \'c4nderungsanfrage an eine zentrale Stelle stellt. Diese zentrale Stelle reiht alle Anfragen (nebenl\'e4ufigkeitsgesichert, das geht) hintereinander ein und arbeitet sie von vorne ab (FIFO). Abarbeiten bedeutet: Validieren, ob die \'c4nderungsanfrage f\'fcr den aktuellen Zutand der Daten g\'fcltig ist. Wenn ja, dann \'c4nderung vornehmen, abspeichern. Wenn nein, dann R\'fcckmeldung, dass die Anfrage abgelehnt wurde.\par
\par
Ganz konkretes Beispiel:\par
In einer bestimmten Kinovorstellung wollen sich zwei Benutzer auf unterschiedlichen Server-Nodes gleichzeitig den Platz 68 reservieren.\par
Folgendes w\'e4re die ignorante Variante, die zu einem Konsistenzfehler f\'fchrt:\par
seat68.setOwner(currentSessionUser);\par
storage.store(seat68);\par
\par
Wenn beide Servernodes (stellvertretend f\'fcr beide Benutzer) das einfach so ausf\'fchren, gibt es im "g\'fcnstigsten" Fall irgendwann mal eine Exception, dass eine Inkonsistenz festgestellt wurde. Im ung\'fcnstigten Fall bekommen beide Bentzer eine Best\'e4tigung, dass sie Platz 68 reserviert haben, beide bezahlen, usw., aber nur einer ist tats\'e4chlich als Owner gesetzt. Klingt in diesem kleinen Beispiel nach einer harmlosen kleinen Panne, aber das Prinzip kann nat\'fcrlich auch bei kritischeren Situationen auftreten. Daran k\'f6nnen Menschern sterben oder Firmen pleitegehen.\par
Die Strategie, zu sagen "Naja mei, der letzte hat halt recht" funktioniert nur, wenn es keinerlei weitere Abh\'e4ngigkeiten gibt. Aber wenn etwa, wie im Beispiel, Kunden auf einmal f\'fcr eine Leistung bezahlen m\'fcssen, die sie gar nicht bekommen, dann funktioniert das "der letzte hat halt recht" \'fcberhaupt nicht mehr.\par
Kurz gesagt: Das ist eine sehr naive, untaugliche, beschissen bl\'f6de Idee.\par
\par
Wie geht es nun richtig?\par
So:\par
\par
seatManager.handleRequest(new ReserveSeatRequest(seat68, currentSessionUser));\par
\par
Das ist ein kleiner Umweg f\'fcr die obige Logik, aber der Vorteil ist: der wirkliche "seatManager" kann ein ganz anderer Prozess sein, eine zentrale Logikstelle.\par
Beide Benutzer senden ihre Anfrage f\'fcr Platz 68. Der erste bekommt eine Best\'e4tigung. Der zweite bekommt eine Ablehnung. Idealerweise auch gleich ein Update, wie die aktuelle Sitzbelegung inzwischen aussieht. Damit wei\'df die Logik des Servernodes f\'fcr den zweiten Benutzer rechtzeitig, dass der Benutzer den Platz gar nicht bekommen hat. Anstatt ihn, wie oben, einfach ignorant trotzdem abzukassieren, kann eine Meldung "Der gew\'fcnschte Sitz wurde leider zwischenzeitlich reserviert" angezeigt werden. Und zwar BEVOR irgendwas inkonsistent wird.\par
\par
Ist das dann nicht einfach eine "Der erste hat halt recht" Strategie?\par
Ja, aber mit einem ENTSCHEIDENDEN Unterschied: Es gab dabei keine Seiteneffekte der Logik und keine Inkonsistenz in den Daten als Folge davon.\par
F\'fcr komplexere F\'e4lle muss die request-handling-Logik nat\'fcrlich entsprechend komplex sein.\par
\par
Aber wie ist das mit der Performance, wenn nun alle Nodes ihre Schreibw\'fcnsche von einem zentralen Node seriell abarbeiten lassen m\'fcssen?\par
\par
Dazu muss man Folgendes wissen: Anwendungen machen unglaublich viel Zeug. Zum Beispiel im Frontend HTML-Seiten zusammenbauen. Allein, einen String zusammenzubauen, ist im Detail eine riesige Rechenarbeit. Was wir im Code so einfach mit '"Hallo " + name' schreiben, ist in Wirklichkeit eine Orgie an Objektinstanzierungen und mehrfachem Umkopieren von Arrays (Mindestens 2 StringBuilder Instanzen, 3 String Instanzen und 5 char[] Instanzen, au\'dferdem mindestens 4 mal char[]s umkopieren. Falls der StringBuilder mehrfach dynamisch wachsen muss, wird es sogar nochmal extrem mehr).\par
Oder man denke an Session Handling, Netzwerkkommunikation, Daten laden, cachen, durchsuchen, aufbereiten (z.B. aggregieren), Validierungen machen, usw.\par
Die wirkliche "Kernlogik", um persistente Daten zu ver\'e4ndern, ist im Gesamtsystem betrachtet nur ein winziger Anteil. Vielleicht 1% des Aufwands. Vielleicht sogar noch weniger.\par
\par
Nun ist es so:\par
Wenn nur ein Node als "Master" f\'fcr das effektive \'c4ndern von Daten da ist, und X andere Nodes als "Slave" ihm zuarbeiten, k\'f6nnen diese Slaves dem Master fast den gesamten Zusatzaufwand aussen rum abnehmen. Usersessions hat der Master einfach gar nicht. Damit f\'e4llt s\'e4mtliches Frontend Ged\'f6ns schon mal weg. Mit dem Master reden nur Slaves und er nur mit ihnen. Auch das allermeiste an Validierungslogik k\'f6nnen ihm die Slaves schon abnehmen. Sie haben je nach Situation mindestens "fast" aktuelle oder oft sogar absolut aktuelle Daten und k\'f6nnen Anfragen von Benutzern damit schon mal selbst \'fcberpr\'fcfen. Etwa: Wer nicht \'fcber 18 ist, kann in einem Film ab 18 sowieso schon mal keinen Platz reservieren. Oder wenn kein Platz frei ist, dann kann auch nichts mehr reserviert werden. Das alles, inklusive dem ganzen Front-End hin-und-her mit dem Benutzer, nehmen die Slaves dem Master ab. Bis zum Master kommen \'fcberhaupt nur Anfragen, die eine "Chance" haben, angenommen zu werden.\par
Der Master bekommt also nur noch bereits vor-validierte interne Anfragen seiner zuarbeitenden Slaves rein und muss nur noch pr\'fcfen, ob die Anfrage bei dem aktuellen Stand der Daten noch valide ist, oder ob der Absender-Slave da mit seinem Wissensstand etwas hinten dran war und darum eine inzwischen unerf\'fcllbar Anfrage gestellt hat.\par
Das hei\'dft der Master muss die ~99% der \'fcblichen Arbeit einer Anwendung gar nicht machen. Daf\'fcr hat er seine Sklaven. Er muss daf\'fcr auch gar keinen RAM belegen, sondern kann praktisch alles zum Cachen der persistenten Daten verwenden. Er k\'fcmmert sich nur noch darum, die Kernfunktion, \'c4nderungsw\'fcnsche an Daten, auszuf\'fchren. Also Validieren und bei Erfolg \'c4nderung vornehmen und speichern.\par
Im Beispiel oben ist das ein Aufwand von ein paar Mikrosekunden. Instanzen aus dem Heap in den CPU-Cache laden, Felder checken und ver\'e4ndern oder R\'fcckmeldung geben.\par
F\'fcr das Drum-Rum des Masters intern hat dieser nat\'fcrlich anderen Threads. Netzwerkkommunikation, Schreiben auf die Platte, usw. Der wirklich die Logik abarbeitende Thread ist mit einer Anfrage nur ein paar Mikrosekunden besch\'e4ftigt. Danach geht es zur n\'e4chsten Anfrage. Mit dieser Geschwindigkeit k\'f6nnte selbst ein B\'fcro-PC die Sitzplatzreservierungen aller Kinos der gesamten Welt alleine stemmen. Alles andere drum rum nehmen ihm andere Maschinen ab. Vielleicht tausende.\par
\par
\par
2018-10-20\par
\par
Radikal Revolution\'e4res RAD-Konzept f\'fcr Nebenl\'e4ufigkeitshandling. In einem Prozess und auch in einem Cluster.\par
Coming soon ...\par
\par
\par
Mir ist gerade beim Computerspielen aufgefallen:\par
Das ganze Problematik, dass man von einem Slave nebenl\'e4ufig konfliktete ver\'e4nderte Entities erkennen und rollbacken m\'fcsste, gibt es gar nicht, wenn man sich an ein Regel ausnahmslos h\'e4lt:\par
\'c4nderungen an shared state (z.B. persistenter Entitygraph) werden nur \'fcber Requests gemacht, niemals direkt.\par
Denn dann ist es ja so:\par
Jeder Slave kann in Requests nur unver\'e4nderte Entities mitschicken. Die werden garantiert immer die aktuellste oder eine veraltete Version von Entities mitschicken, die passt oder problemlos verworfen werden kann.\par
Das Abarbeiten eines Requests m\'fcsste halt immer gleich den aktuelle Zustand der betroffenen Entities mitschicken, damit der Slave gleich den durch die \'c4nderung erzeugten Stand der Daten hat.\par
Hm, oder nach jedem Request m\'fcsste der Slave sich seine Daten aktualisieren. Das w\'e4re konsistenzm\'e4\'dfig die perfekte L\'f6sung, k\'f6nnte aber manchmal etwas aufwendig werden.\par
Auf jeden Fall k\'f6nnte jeder gespeicherte Chunk einen Timestamp bekommen und jede Requestantwort k\'f6nnte den Timestamp des aktuellen Chunks beinhalten, so dass der Slave extrem schnell (1 long Vergleich) checken kann, ob er sich aktuelle Daten holen muss.\par
\par
Das Konzept k\'f6nnte sogar das Nebenl\'e4ufigkeitsproblem grunds\'e4tzlich l\'f6sen. Sagen wir innerhalb eines Prozesses f\'fcr Threads:\par
\par
Logik und Datenmodell der Anwendung sind in zwei Teile unterteilt. Mit drei Projekten mit einem Schnittmengenprojekt, wie bei Client-Server:\par
- Shared\par
- Master\par
- "Work" oder so. Oder besser irgendwas mit auch 6 Buchstaben.\par
\par
Shared enth\'e4lt:\par
- F\'fcr alle Entities interfaces und Klassen mit dem State, aber mit nur lesenden Methoden. Keine Methoden, die irgendwie internen State ver\'e4ndern.\par
- Genauso interfaces f\'fcr Anwendungslogik: Nur lesendes Zeug.\par
Alles, was (in einem nebenl\'e4ufigen Raum geteilte) Daten ver\'e4ndern soll, muss \'fcber Requests gemacht werden.\par
\par
Master enth\'e4lt:\par
- Shared\par
- Ableitungen der read-only interfaces und Klassen mit auch schreibenden Methoden.\par
- Eine "Empfangsstelle" f\'fcr Requests, die diese in eine FIFO Queue einreiht.\par
- Anwendungslogik f\'fcr Validierung und Ver\'e4nderung von geteilten Daten.\par
\par
"Work" enth\'e4lt:\par
- Shared\par
- Einen "Working" Teil der Anwendungslogik. Mit Validierungen, auch zum Erzeugen neuer Entities, aber nicht zum ver\'e4ndern von bestehenden. Das k\'f6nnte je nach Algorithmus ein Konflikt sein. Manche kann man mit dem Builder Pattern aufl\'f6sen, aber nicht alle.\par
- Eine "Sendestelle" f\'fcr Requests und Logik, was gemacht wird, wenn ein Request abgelehnt wird.\par
\par
Die Anwendung funktioniert so:\par
\par
Der Master Bereich l\'e4uft auf nur einem Thread.\par
Der "Work" Bereich kann auf beliebig vielen Threads laufen.\par
Die kommen sich mit ihren \'c4nderungen am gemeinsamen Entitygraph aber nie in die Quere, weil sie alle nur Requests erstellen, die nebenl\'e4ufigkeitssicher sequenziell eingereiht werden und die tats\'e4chlichen \'c4nderungen erfolgen sequenziell im Master Thread.\par
Ist das dann nicht effektiv die nebenl\'e4ufige Anwendung auf singlethreaded runtergebremst?\par
Nein, denn:\par
- Nicht-schreib-konflikte Arbeit l\'e4uft echt parallel. Z.B. Client-Kommunikation, Suchen\par
- Neue Subgraphen, z.T. riesige Datenmengen, k\'f6nnen im "Work" Bereich erzeugt werden und brauchen dann im Master nur noch blitzschnell an ihrer Rootinstant in den gemeinsamen Graph eingeh\'e4ngt werden.\par
Wie oben mehrfach beschrieben ist der sequenziell ablaufende Teil nur winzig.\par
\par
Hm, interessant:\par
Der Ansatz funktioniert in einem Cluster, aber innerhalb eines Prozesses gibt es ein Problem:\par
Die Threads im "Work" Bereich haben im Gegensatz zu Cluster-Slaves keine Kopie des Entity Graphen mit einem vielleicht veralteten, aber in sich konsistenten Zustand, sondern die w\'fcrden die \'c4nderungen direkt live und potenziell inkonsistent mitbekommen.\par
Das Layered Entity Konzept sollte hier eine L\'f6sung bringen k\'f6nnen.\par
\par
Aber eigentlich geht es hier ja um Clustering und nicht um intra-Prozess Nebenl\'e4ufigkeit. Darum zur\'fcck zum Cluster Kontext.\par
\par
Das ganze funktioniert wie so ein pulsierendes Jing-Jang Feedback-System:\par
Slaves bekommen Updates des persistenten Entitygraphen vom Master.\par
Damit betreiben sie die Anwendungslogik, verarbeiten reinkommende Requests, validieren sie soweit m\'f6glich, lesen/suchen in den Daten, usw.\par
Der Master bekommt von den Slaves \'c4nderungsrequests, validiert die gegen den aktuellen Stand der Daten und schickt Responses zur\'fcck.\par
\par
Der entscheidende Faktor bzw. das einzige Problem an dem System ist:\par
Wie schnell bekommen die Slaves die Updates vom Master?\par
Vielleicht ist das Konzept mit "Master schreibt in die Dateien -> Slaves scannen nach gewachsenen Dateien, kopieren das Neue zu sich und lesen es dort ein" zu langsam. Da sind mehrere Festplatten-Writes beteiligt, au\'dferdme polling, usw. Alles furchtbar langsam.\par
Besser ist wohl:\par
Der im RAM des Master gebaute Chunk wird parallel zum schreiben von einem anderen Thread gleich per Netzwerk an die Slaves verschickt.\par
Jeder Slave hat einen Update-Puffer, der vom Master bei jedem Store mit Updates vollgeschrieben wird.\par
Der Slave muss halt regelm\'e4\'dfig schauen, ob der Puffer Daten enth\'e4lt und wenn ja den gleich einlesen, selber wegschreiben und alle seine laut OID passenden Instanzen noch live im Memory updaten.\par
Am besten nach jedem Request, dann ist er immer up-to-date.\par
Das ist geil. Vor allem, weil es keine zus\'e4tzliche Arbeit ist, irgendwie entities nochmal zu serialisieren, sondern das ist nur eine RAM-zu-RAM Kopie von dem, was eh schon gemacht wird.\par
Der Master br\'e4uchte je angemeldetem Slave einen eigenen Thread, der den erzeugten Chunk verschickt.\par
Hier sind Multicores gefragt:\par
Der eigentliche Master ist nur ein Thread.\par
Aber er braucht schon mal mehrere Threads f\'fcr die OGS Channels (am besten nicht nur 4, sondern 8, 16, 32)\par
Dann braucht er mehrere Threads f\'fcr Event-Listening von Requests. Oder am besten einen dedicated Request Listener Thread je Slave. Wenn der Slave nichts schickt, schl\'e4ft ja auch der Thread.\par
Und dann braucht er noch pro Slave einen Update Sender Thread.\par
Alles, damit der Master Thread nichts anderes tun muss, als permanent Requests verarbeiten, Entities \'e4ndern und storen (= chunks erstellen).\par
Geil ist auch: immutable Entities werden \'fcberhaupt nicht redundant gespeichert und verschickt, weil sich an denen ja nie was \'e4ndert und sie darum \'fcberhaupt nicht persistiert werden m\'fcssen. Es werden immer nur die ge\'e4nderten und einmalig die neu dazugekommenen Entities gespeichert und verschickt.\par
\par
Die Slaves k\'f6nnen mit diesen Requests auch durchaus Locks auf fachlicher Ebene anfragen und sichern.\par
Also "Dieser Datensatz / Subgraph / Anwendungsbereich ist jetzt mal gesperrt f\'fcr Benutzer XY".\par
Der Master ber\'fccksichtigt das dann bei der Verarbeitung/Validierung der \'c4nderungsrequests der Slaves. Bzw. die sollten die Locks auch geschickt bekommen und diese Validierung gleich selbst machen.\par
\par
Das einzig bl\'f6de ist jetzt noch, ob man nur f\'fcr die Erkennung von "manchmal leicht veralteten Entities", die Slaves in ihren Requests mitschicken m\'fcssten.\par
Da muss ich mal dr\'fcber nachdenken. Aber ich glaub, das geht nicht ohne, sonst w\'fcrden veraltete Daten aktuelle \'fcberschreiben. W\'e4r fatal.\par
\par
\par
2018-10-21\par
\par
Die Versionierung war gedanklich schnell ersetzt... Hehe.\par
\par
Man schickt einfach nicht komplette serialisierte Entities, sondern Identifiers. Was ein Identifier ist, w\'fcrde vom jeweiligen Entitytyp abh\'e4ngen. Im einfachsten Fall ist das irgendein long, sowas wie eine "UserId" oder Kundennummer oder so.\par
Oder es m\'fcsste ein zusammengesetzer Identifier sein, wie man vom Root aus zu der gew\'fcnschten Instanz kommt. Sowas wie MandantID + Jahr + KategorieId + EntityId. Das ist halt allein abh\'e4ngig davon, wie geschickt man sein Datenmodell designt. Wenn es gut gemacht ist, sollten das nie mehr als 8 bis 16 Bytes werden, die blitzschnell aufgel\'f6st werden k\'f6nnen. Wenn man d\'fcmmliche Strings rumschickt, die erst mal mit RegExps zerlegt werden m\'fcssen, usw., dann wirds halt teuer. Aber selbst dann muss das im Master Node nicht der Master thread selbst machen, sondern das machen die Kommunikationsthreads, also kein gro\'dfes Drama.\par
\par
Dann noch zu dem Problem, dass das Konzept in einem einzelnen Prozess nicht gehen w\'fcrde, weil es dort keine Kopien des Entity Graphs gibt:\par
Mit Layered Entites w\'fcrde es gehen. Die eigentliche Data Instanz wird von den Updates aktualisiert, aber es gibt auch noch eine Art thread-local Hashmap, in der f\'fcr jeden aktiven User eine lokales Data Instanz gehalten wird.\par
Das Aktualisieren eines Slaves w\'e4re dann, in einem konsistent gehaltenen (via synchronized Lock) Vorgang alle Entities auf die Master-Data-Instanz umzuschalten.\par
\par
Hm...\par
\par
Das w\'e4re sogar f\'fcr den Clustering Ansatz auch vorstellbar:\par
Damit w\'fcrde so ein Aktualisierungsvorgang nicht daraus bestehen, gepufferte binary chunks zu verarbeiten, sondern:\par
- Das Binary Chunk verarbeiten k\'f6nnte permanent und sofort erfolgen. Das wird alles in die Master-Data Instanzen reingeschrieben.\par
- Aktualisieren w\'e4re dann nur noch, blitzschnell die Referenz auf die master-data instanz in den lokalen Kontext zu kopieren.\par
Und der Vorteil w\'e4re: Das Aktualisieren w\'e4re nicht ein relativ langer Vorgang, der f\'fcr alle aktiven User gleichzeitig gemacht werden muss, sondern jeder User k\'f6nnte - nach einem Request oder so - seine Aktualisierung selbst machen, unabh\'e4ngig von den anderen Usern.\par
\par
Es m\'fcsste dann irgendeine Form von zentralem "Betriebslock" geben, das sich der Chunk Verabreiter bei jeder Chunk Verarbeitung holt und das andererseits die User-Threads brauchen, um sich updaten zu k\'f6nnen.\par
Genauer gesagt m\'fcsste das ein 1-write-n-read Lock sein: Wenn der Verarbeiter schreiben will, kann keiner Lesen. Aber die Leser k\'f6nnen gleichzeitig darauf zugreifen.\par
\par
Evtl. sollte es dann f\'fcr jeden User einen dedicated Thread geben. Den Overhead, den so eine Thread Instanz erzeugt, ist nicht der Rede wert in einem System, in dem \'fcber mehrere Nodes verteilt wird. Daf\'fcr hat man eine 1-zu-1 Beziehung zwischen User und Thread.\par
\par
Das Konzept w\'fcrde nat\'fcrlich hei\'dfen, dass der Laufzeit-Speicherbedarf f\'fcr einen Entity Graphen ein vielfaches w\'e4re. Jede Data Instanz belegt den mehrfachen Platz des Original Datensatzes plus es muss noch eine Hashing Metastruktur in jedem Entity geben.\par
Momentan m\'fcssen f\'fcr jeden Aktiven User zwangsweise Updates auf den aktuellen Master gemacht werden, damit der Master unsynchronisiert jederzeit ersetzt werden k\'f6nnte. Auch dann, wenn sie die Instanz gar nicht lesen oder gar nicht ver\'e4ndern wollen. Das ist nat\'fcrlich doof.\par
\par
Vielleicht w\'e4re doch folgendes besser:\par
- Chunks permanent einlesen, aber nur die binary Daten abspeichern, nicht runtime instanzen updaten.\par
- Jeder User hat eine eigene SwizzleRegistry, d.h. seine eigene Kopie des Entity Graphen, aber nat\'fcrlich nur die Instanzen, die der braucht.\par
- Aktualisieren w\'e4re dann, alle ausstehenden Updates in die live Instanzen zu schreiben.\par
H\'e4tte zwar auch sehr viele Redundanzen, vor allem immutable Instanzen w\'fcrden mehrfach instanziert werden, was leicht d\'e4mlich ist, aber immer noch weniger Redunanzen, als von einem zentralen Entity Graphen jede Instanz vorsichtshalber f\'fcr jeden aktiven User kopieren zu m\'fcssen (Kreuzprodukt, autsch).\par
\par
Cool w\'e4re, immutable instanzen zu erkennen und in einer gesharten SwizzleRegistry zu halten.\par
Wobei das dann nur solche Instanzen sein d\'fcrften, die nicht wieder irgendwo auf mutable Instanzen zeigen, d.h. vom shared Graph auf einen local graph.\par
D.h. ein gesamter Subgraph m\'fcsste eine immutable "sackgasse" sein. Der k\'f6nnte shared sein. Wobei der Code f\'fcr das sharen dann komplexer werden w\'fcrde. Lookups in zwei Registries, usw.\par
Bzw. es k\'f6nnten dieselben Instanzen in mehreren SwizzleRegistries registriert sein. Das m\'fcsste halt nur richtig erkannt werden.\par
\par
\par
2018-10-24\par
\par
\par
\par
Das mit dem Slaves updaten ist gar nicht so einfach:\par
Existierende Instanzen m\'fcssen entsprechend den neuen Daten geupdatet werden.\par
Das macht erst mal keinen Konflikt, weil die Anwendungslogik im Slave keine persistenten Entities ver\'e4ndern darf. Wenn sie es doch tut, werden die \'c4nderungen vom Update einfach \'fcberschrieben und der Verlust ist ein Bug in der Anwendungslogik, nicht im Framework. Kann man nat\'fcrlich vermeiden durch das Interface-Trennung-Konzept, aber wie ich geh\'f6rt habe gibt es sogar bei uns Leute, die interfaces grunds\'e4tzlich ablehnen, weil sie abstrakter als ein primitives Einsteigerbeispiel sind.\par
\par
Ein Konflikt ist aber:\par
Wenn die Anwendungslogik gerade l\'e4uft und die Entities ausliest, kann es zu inkonsistenzen in Algorithmen, in gecachten Daten der Anwendungslogik, usw., kommen.\par
Simples Beispiel: Wenn eine Collection leer ist, dann dies tun, ansonsten das. Erst wird gepr\'fcft, der Branch gew\'e4hlt, gleichzeitig wird die collection geupdatet und ist damit nicht mehr leer. Der nun falsche branch im Algorithmus l\'e4uft durch und erzeugt eine Inkonsistenz in den Daten.\par
Das hei\'dft, es darf nicht einfach "irgendwann" (so schnell wie m\'f6glich) geupdatet werden, sondern es muss zu definierten Zeitpunkten erfolgen, indem es keine Inkonsistenzen geben kann. Diese k\'f6nnen nur sein: zwischen der Abarbeitung von Requests von au\'dfen. Oder allgemeiner formuliert (falls die Serveranwendung auch request-unabh\'e4ngige Aktivit\'e4ten hat, Chron-Jobs oder \'e4hnliches): Zwischen zwei Aktionen, wobei eine "Aktion" eine f\'fcr sich abgeschlossene Logik ist.\par
Das hei\'dft jeder Thread in der Anwendung, der mit dem persistenten Entity Graph arbeitet, muss in einer schleife "Aktionen" ausf\'fchren und am Ende der Schleife immer checken, ob es etwas zu updaten gibt.\par
\par
Ein weiteres Problem:\par
Der Entity Graph ist eigentlich read-only, also k\'f6nnten in einem Slave eigentlich alle Threads (Sessions / user / Aktivit\'e4ten, wie auch immer) auf demselben arbeiten.\par
Das w\'fcrde aber hei\'dfen, dass ein Update der Instanzen erst gemacht werden kann, wenn ALLE Threads an so einem Update Punkt sind. D.h. die Threads m\'fcssten bei vorhandenem Update aufeinander warten. Falls manche der Threads sehr lange Arbeiten haben, m\'fcssten alle threads warten, bis die sehr lange Arbeit abgeschlossen ist. Das Ergebnis w\'e4re ein nach au\'dfen hin sehr tr\'e4ges System, das intern aber fast nichts arbeitet. Das ist scheisse.\par
Besser w\'e4re:\par
Jede Aktivit\'e4t hat ihre exklusive Kopie des Entity Graphen. Das ist nicht X mal die ganze Datenbank, sondern es wird ja \'fcber Lazy References immer nur das geladen, was gerade ben\'f6tigt wird. Immutable Instanzen k\'f6nnten \'fcber alle gesharet werden, weil sich an denen ja nie was \'e4ndern kann. Dann bliebe als Redundanz in den Daten nur noch, wenn mehrere Aktivit\'e4ten \'fcberschneidende Daten laden m\'fcssen. Je nach Anwendung k\'f6nnte man bestimmte Subgraphen in eine Session-unabh\'e4ngige Aktivit\'e4t/Thread packen, mit der die Session-Aktivit\'e4ten dann kommunizieren, anstatt selbst alles laden zu m\'fcssen. Beispiel: Aus einer gro\'dfen Collection bestimmte Entities raussuchen. Die Session-Aktivit\'e4t will nur das Ergebnis, daf\'fcr muss nicht jede von denen die riesige Collection selbst laden, sondern das kann zentral ein Ding machen. Das ist nat\'fcrlich Anwendungsarchitekturdesign. Die Grundaussage ist lediglich: Die Redundanzen lassen sich auf ein vertr\'e4gliches Ma\'df reduzieren.\par
\par
Und es gibt noch ein Problem:\par
Die lokale Storage-Ebene eines Slaves (Dateien und ggf. in der Storageebene gecachte Daten) muss asynchron zu den Aktivit\'e4ten aktualisiert werden, weil es die ja in jedem Fall nur einmalig zentral gibt und die Aktualisierung nicht auf jede Aktivit\'e4t warten bzw. alle anderen bremsen kann, bis die letzte an einem Update-Punkt ist.\par
Das hei\'dft aber: Sobald eine laufende Aktion aus einer Lazy Reference nachladen will, l\'e4dt sie race-condition-m\'e4\'dfig entweder aus dem Zustand, zu dem begonnen wurde, oder schon aus dem aktuell geupdateten Zustand. Damit liegt wieder die Inkonsistenzsituation von oben vor.\par
Es bringt auch nichts, im Rahmen des Ladens aus der Lazy Reference zuvor erst mal die Instanzen zu updaten, weil man ja eben nicht wei\'df, was der Algorithmus davor gemacht hat und welche Inkonsistenzen durch eine \'c4nderung mittendrin entstehen k\'f6nnen.\par
\par
Dieses Clustering ist im Detail echt knifflig.\par
Wie synchronisiert man asynchrone Aktivit\'e4ten, ohne sie zu sehr auszubremsen?\par
\par
Gibt es wirklich keine anderen M\'f6glichkeit, als gewisse, wenn auch auf ein Minimum reduzierte Inkonsistenzen hinzunehmen und alles mit try-catch in der Anwendungslogik so lang zu versuchen, bis es klappt?\par
Unbefriedigend :(\par
\par
Eine L\'f6sung w\'e4re:\par
- Gemeinsam genutzte read-only Kopie des persistenten Entity-Graphen in den Slave Nodes.\par
- Alle Aktivit\'e4ten in einem Slave m\'fcssen f\'fcr einen Update an einem gemeinsamen Synchronisationspunkt zwischen Aktionen warten.\par
- Eine Aktion darf in der Aktivit\'e4t keinen lokalen State zur\'fccklassen, der durch eine \'c4nderung am Entity-Graph inkonsistent werden w\'fcrde (Anwendungsverantwortung).\par
- Eine Aktion darf nicht "lange" dauern, damit der Bremseffekt auf alle wartenden Aktivit\'e4ten nicht zu gro\'df wird (Anwendungsverantwortung).\par
\par
Vielleicht w\'e4re ein "k\'fcnstliche" Update-Frequenz eine Hilfe?\par
Alle X Millisekunden, z.B. 1000, wird ein Update gemacht. Solang noch Zeit ist, k\'f6nnen Aktivit\'e4ten die n\'e4chste Aktion beginnnen, die dann hoffentlich nicht zu lang dauert.\par
Wenn die Zeit um ist, ist es Zeit zum updaten. Dann warten alle Aktivit\'e4ten aufeinander, dann werden einmalig zentrall alle zwischenzeitlich ge\'e4nderten Instanzen geupdatet, dann gehts mit dem n\'e4chsten Zeitbuget weiter.\par
\par
Das Dumme daran ist, dass Aktivit\'e4ten gebremst werden f\'fcr updates an instanzen, die sie nie brauchen.\par
\par
Vielleicht w\'e4re mit so einer Update-Frequenz eine Trennung in lokale Graphen doch wieder besser, mal sehen:\par
\par
Jede Aktivit\'e4t hat ihre eigene, lokale Entity Graph Kopie.\par
- Jede Aktivit\'e4t arbeitet dahin, so lange noch zeitbudget da ist.\par
- Wenn die Zeit um ist, werden die zwischenzeitlich empfandenen Update chunks nur auf die Platte geschrieben und ggf. Storage-gecachte Entity-Daten durch die aktuelle Version ersetzt.\par
- Danach updatet jede Aktivit\'e4t die f\'fcr sie n\'f6tigen Instanzen aus den aktualisierten Daten (geht schnell: anhand OID Set, update chunks sind schom im RAM, alle Aktivit\'e4ten k\'f6nnen auf die unver\'e4nderlichen Update chunks gleichzeitig zugreifen). Das sollte in den allermeisten F\'e4llen nichts oder sehr wenig sein und sehr schnell gehen.\par
- Dann geht es wieder von vorne los.\par
\par
Vielleicht m\'fcsste man noch ein bisschen eventhandling daf\'fcr dazubauen:\par
Logik aufrufen, wenn cycle zuende ist.\par
Logik aufrufen, wenn cycle neu beginnt, mit einem Set der ge\'e4nderten Instanzen. Erst nach dieser Logik kommt die n\'e4chste Aktion dran.\par
\par
Vielleicht m\'fcsste man sogar User-Session-Aktivit\'e4t und User-Session-Netzwerkkommunikation in zwei Threads aufteilen:\par
Nur die Aktivit\'e4t muss sich dem Cycle unterwerfen.\par
Der Netzwerk Thread kann schon mal alle reinkommenden Requests einlesen, Instanzen daraus bauen (z.B. riesige Graphen an neuen Instanzen), usw.. Dann muss das die Aktivit\'e4t nicht machen und spart sich Zeit, die responsiveness bleibt besser.\par
Vielleicht sogar drei Threads:\par
1.) Eingehende Daten und alle Logik, die keinen Zugriff auf den Entity Graph braucht\par
2.) Anwendungslogik Aktivit\'e4t mit Zugriff auf Entity Graph.\par
3.) Ausgehende Daten: Netzwerkkommunikation und ggf. Serialisierung, die nicht vom Entity Graph abh\'e4ngt.\par
\par
\par
\par
Das w\'fcrde alles sehr "smooth" laufen und gut skalieren, weil die Aktivit\'e4ten/Threads sich gegenseitig kaum behindern.\par
Das einzige Problem ist: was ist, wenn eine Aktion zu lang dauert? Anwendungsverantwortung?\par
Andererseits:\par
Es ist ja auch schon Anwendungsverantwortung, keine Endlosschleifen, Memoryleaks oder schlicht RAM-sprengend riesige Arrays zu allokieren.\par
Was Anwendungslogik angeht, muss man dem Anwendungsentwickler schon die Verantwortung \'fcbertragen k\'f6nnen, sonst sind wir im Kindergarten.\par
Aus Sicht der Cluster-Infrastruktur ist es einfach so: wenn manche Aktionen in der Anwendungslogik zu lang brauchen, dann wird halt das System lahm.\par
\par
Der einzige Haken auf ebene der Cluster-Infrastruktur ist aber halt, dass eine lahme Aktion bei einer Aktivit\'e4t ALLE Aktivit\'e4ten bremst.\par
Vielleicht k\'f6nnte das mit einem Dubble-Buffering Prinzip gelindert werden:\par
\par
Datenbankdateien gibt es nur einmal, aber die Information, bis wie weit die gelten und die gecachten Entitydaten liegen zweimal vor: Einmal aktueller Cycle, einmal neuer Cycle.\par
Der neue Cycle wird asynchron zu den Aktivit\'e4ten aktualisiert.\par
Am Cycle-Ende schalten dann einfach alle Aktivit\'e4ten auf die Daten des neuen Cycle um und updaten ihre Instanzen. Damit ist der der aktuelle und der bisher aktuelle ist der alte.\par
Okay, dann br\'e4uchte man ein Triple-Buffer-Konzept: alt (wird noch verwendet von nachz\'fcglern), aktuell (wird von allen anderen verwendet), neu (wird gerade aktualisiert, bis der aktuelle cycle aus ist).\par
Das ist ziemlich viel Komplexit\'e4t und Speicherplatzbedarf und es l\'f6st das Problem nicht vollst\'e4ndig, sondern lindert es nur um einen cycle:\par
Was ist, wenn eine Aktivit\'e4t zwei Cycles lang immer noch nicht fertig ist? Man kann nicht f\'fcr jeden \'fcberzogenen cycle eine neue Bufferebene aufmachen.\par
Ich wei\'df nicht, ob der Aufwand das wert ist.\par
\par
Aber moment mal, eine Vereinfachung gibt es noch:\par
Die Aktivit\'e4ten k\'f6nnen sich anhand der vorliegenden Update-Chunks ihre Instanzen aktualisieren, auch ohne dass bzw. bevor die zentralen Storage-Daten aktualisiert worden sind. Die Daten sind ja schon da. Und die Storage-Daten w\'fcrden f\'fcr noch laufende Aktivit\'e4ten weiterhin den alten Stand haben, bis die letzte Aktion aus dem alten cycle beendet ist.\par
W\'e4hrend dem Storage-cache updaten und schreiben auf die Platte m\'fcssten trotzdem wieder alle warten.\par
Ne Moment: Auf die Platte geschrieben k\'f6nnte schon werden, nur die Storage-Level-Metainformationen \'fcber den Fortschritt und die Storage-Cachedaten d\'fcrften dann erst zwischen den Cycles aktualisiert werden.\par
\par
Damit w\'e4re es dann zumindest so, dass die rechzeitigen Aktivit\'e4ten nur noch auf die letzte Aktivit\'e4t warten m\'fcssten und kurz darauf gleich wieder mit ihrem Cycle beginnen k\'f6nnten, ohne dann erst noch ihre instanzen aktualisieren zu m\'fcssen. Hm...\par
\par
Das hei\'dft nat\'fcrlich, dass OGS f\'fcr Clustering-Funktionalit\'e4t erweitert werden m\'fcsste. Aber das kriegt man bestimmt in einer ordentlichen Weise hin.\par
\par
Es bleibt aber immer noch, dass eine besonders lange Aktion alle anderen ausbremst.\par
Naja gut, immerhin nicht ALLE anderen, sondern nur die auf demselben Slave. Wenn es davon viele gibt, werden immer nur die User eines Slaves gebremst.\par
\par
\par
Hm. Eine wirkliche L\'f6sung w\'e4re folgendes:\par
- Die Datenbankdateien gibt es nur einmalig zentral und die werden immer asynchron geupdatet.\par
- Der Fortschritt der Datenbankdateien ist aber je Aktivit\'e4t.\par
- Der Storage-Cache bzw. die Storage-Metadaten der Entities (Fileposition) sind je Aktivit\'e4t.\par
- Und, wie gehabt, der Entity-Graph ist je Aktivit\'e4t. "Deep Immutable" Instanzen k\'f6nnen zwischen allen Aktivit\'e4ten geteilt werden.\par
- Aktivit\'e4ten updaten sich ihren Datei-Fortschritt, ihren Storage-Cache und ihre Instanzen asynchron.\par
\par
Bzw. Update des Storage Caches w\'fcrde hei\'dfen, den einfach auf null zu setzen, denn:\par
- Die Instanzen sind die prim\'e4re "in Memory" Datenbank, nicht die Caching Daten.\par
- Die Instanzen k\'f6nnen \'fcber den eh schon im Speicher liegenden Update Chunk aktualisiert werden, die gecachten Daten braucht man daf\'fcr nicht.\par
\par
Das macht aber auch das OGS Housekeeping komplexer:\par
Es darf immer nur der \'e4lteste, noch aktive Stand betrachtet werden.\par
\par
\par
---\par
Definition "Deep Immutable":\par
Eine Instanz ist "Deep Immutable", wenn sie selbst immutable ist und auch alle von ihr aus rekursiv erreichbaren Instanzen immutable sind.\par
D.h. ein ganzer Subgraph an Instanzen muss unver\'e4nderlich sein, ohne irgendeine Referenz zu irgendeiner mutable Instanz. Etwa eine immutable collection mit immutable Ums\'e4tzen oder sowas.\par
Messdatens\'e4tze sind in aller Regel deep immutable. X tausend Datens\'e4tze, die einmal erzeugt und dann nie wieder ver\'e4ndert werden. Auch etwaige dazugeh\'f6rende Kopfdaten sind immutable. So ein ganzer Subgraph einer Messung ist normalerweise deep immutable.\par
Oder Statistiken. Oder eben simple Umsatz Datenmengen.\par
\par
Deep Immutable Subgraphen k\'f6nnen zwischen mehreren Aktivit\'e4ten, die eigentlich redundante Kopien von Instanzen halten, gemeinsam verwendet werden.\par
---\par
\par
\par
\par
Wobei die storage-level Trennung nach Aktivit\'e4ten einfacher gesagt als getan ist. Die FilePosition und Datencache eines Storage-Entities sind zentral gehaltene Werte. Wie soll man die je high-level Aktivit\'e4t unterscheiden? Das geht schon, machts aber unheimlich komplex, speicherhungriger und auch etwas langsamer.\par
Da w\'e4re die Variante mit den im worst-case mal etwas l\'e4nger geblockten Slave-Mitbewohnern vorzuziehen.\par
\par
Vielleicht k\'f6nnte man das mit einer Art Good-Will-Konzept etwas entsch\'e4rfen:\par
Die Anwendungslogik kann irgendwo sehr einfach und schnell nachfragen, ob sie den cycle \'fcberzieht. Einfach als eine Methode.\par
Sowas wie "if(something.isCycleOverdue())".\par
Dann kann sie von sich aus abbrechen, updaten und danach nochmal von vorne beginnen. Je nach Fall und Daten evtl. sogar weitermachen, wo sie ausgeh\'f6rt hat.\par
\par
Man kann auch einen Threshold f\'fcr neue Aktionen einf\'fchren:\par
Wenn ein Cycle, mal angenommen, 1000 ms lang ist, dann werden ab 990 ms keine neuen Aktionen mehr gestartet, weil die Gefahr zu gro\'df ist, dass die Aktion \'fcberzieht.\par
10 ms sind zwar eine gigantisch lange Zeit f\'fcr CPUs, aber wenn was lazy nachgeladen werden muss oder so, ist die Zeit evtl. schnell um.\par
Oder generisch gesprochen bei 1% Restzeit oder noch generischer gesprochen bei einem konfigurierbaren Faktor oder evtl. einer formel (maximum aus faktor und konstanter zeit).\par
\par
Eine L\'f6sung w\'e4re nat\'fcrlich auch: wenn man in der Anwendung nunmal lang laufende Aktionen hat, dann setzt man die Cycle Zeit entsprechend hoch. Nicht 1 Sekunde, sondern 10 Sekunden. Der Nachteil ist, dass es auch nur alle 10 Sekunden updates gibt, aber das kann je nach Anwendung ganz okay sein.\par
\par
\par
Vielleicht w\'e4re auch ein anderes Modell besser:\par
Jede Aktivit\'e4t hat ihren eigenen Entity Graph, Deep Immutable Instanzen werden untereinander geteilt.\par
Es gibt keinen Cycle.\par
Alle Aktivit\'e4ten aktualisieren ihre Instanzen asynchron zwischen Aktionen.\par
Es wird mitgeloggt, was \'fcber alle aktivit\'e4ten hinweg der sp\'e4teste verarbeitete Stand der updates ist.\par
Datenbankdateien werden sofort und asynchron geschrieben.\par
Der Fortschritt des Datenbankzustands (Dateifortschritt und Entity Fileposition) werden aber immer nur auf den Stand des sp\'e4testen Stands der updates gesetzt.\par
Wenn ein Lazy Nachladen aus der Datenbank n\'f6tig wird, wird gepr\'fcft, ob die Datenbank den stand der aktivit\'e4t hat.\par
Wenn nicht, muss die Aktivit\'e4t im Laden warten, bis dies der Fall ist. D.h. die Aktivit\'e4t muss dann auf nachz\'fcgler-aktivit\'e4ten warten.\par
\par
Das Problem dabei ist aber: Aktivit\'e4t und Datenbank m\'fcssen EXAKT den gleichen Stand haben. Wenn einer der beiden voraus oder hinterher ist, kann es Inkonsistenzen in der Anwendung geben.\par
Das k\'f6nnte aber evtl. gar kein Problem sein:\par
Eine Aktivit\'e4t kann nicht "hinterher" sein, wenn die Datenbankaktualisierung immer auf die sp\'e4teste Aktivit\'e4t wartet.\par
Wenn eine Aktivit\'e4t voraus ist, aber kein lazy nachladen braucht, gibt es ja kein Problem.\par
Wenn eine Aktivit\'e4t voraus ist und lazy nachladen muss, muss sie warten. Irgendwann ist die Datenbank dann exakt auf demselben Stand, dann gehts weiter.\par
Die Datenbank kann dann aber auch nicht voraus sein, weil zumindest diese eine Aktivit\'e4t ja auf diesem Stand arbeitet und noch nicht fertig ist.\par
Falls es schon updates gibt und die Aktivit\'e4t ihre aktuelle Aktion abschlie\'dft, wird sie ihre Instanzen updaten. Damit kann entweder auch die Datenbank weiter geupdatet werden, oder die aktivit\'e4t muss beim n\'e4chsten lazy laden halt wieder warten.\par
\par
Der gro\'dfe Vorteil gegen\'fcber dem Cycle Konzept ist:\par
- Wenn kein lazy laden gebraucht wird, st\'f6ren sich aktivit\'e4ten untereinander nicht und k\'f6nnen fr\'f6hlich asynchron dahinarbeiten, auch wenns mal l\'e4nger dauert.\par
- Es ist einfacher zu implementieren. Es k\'f6nnte schon reichen, im SwizzleObjectSupplier einen entsprechenden Layer einzubauen.\par
\par
Man k\'f6nnte das sogar noch intelligenter machen:\par
Ein Set aller OIDs sammeln, f\'fcr die Updates ausstehen. Erst, wenn ein Ladevorgang auf so eine trifft (Storage-Ebene), muss er warten.\par
In dem Fall m\'fcsste das Laden auf der Storage-Ebene dann wahrscheinlich nochmal komplett von vorne gemacht werden, aber macht ja nichts. Beim ersten Konflikt sofort abbrechen und warten, belegter Speicher wird wieder freigegeben.\par
Das w\'e4re die ultimative L\'f6sung, bei der Aktivit\'e4ten eines Slaves fast st\'e4ndig asynchron zueinander arbeiten und sich updaten k\'f6nnten und nur, wenn es wirklich konfliktete updates auf Entity-Ebene gibt, m\'fcsste gewartet werden.\par
Allerdings w\'fcrde das OGS Ladeprozess etwas verkomplizieren. Gut, wenn keine Updates ausstehen, ist die Instanz f\'fcr den OID Lookup einfach null, das kann schnell gecheckt werden.\par
\par
Zusammenfassung:\par
- Ein Slave (= die Anwendung) bedient mehrere User (Sessions) bzw. allgemein Verbindungen von au\'dfen. \par
- Ein Slave/Anwendung kann auch interne aktive Vorg\'e4nge haben, etwa Chronjobs o.\'c4.\par
- Beides zusammen wird abstrakt "Aktivit\'e4t" bezeichnet: Irgendein Ding, das aktive Aktionen ausl\'f6st. Kann, muss aber nicht immer derselbe Thread sein.\par
- Jede Aktivit\'e4t hat eine exklusive Kopie des Entity Graphen bzw. SwizzleRegistry, wobei nur die aktuell n\'f6tigen Instanzen geladen werden.\par
- Deep-immutable Subgraphen k\'f6nnen von allen Aktivit\'e4ten gemeinsam verwendet (Eintr\'e4ge in allen SwizzleRegistries auf dieselben Instanzen)\par
- Ein Slave bekommt mit jedem Store im Master, mit etwas Verz\'f6gerung, Update Chunks \'fcbertragen. Diese werden von einem extra System-Thread empfangen und eingereiht.\par
- Jeder Update Chunk wird (von einem weiteren System-Thread) sofort auf die Platte geschrieben, aber dann noch aufgehoben.\par
- Der Fortschritt der Storage Metadaten wird aber erst in Abstimmung mit allen Aktivit\'e4ten vorgenommen.\par
- Jede Aktivit\'e4t f\'fchrt ihre Aktionen in einer Schleife aus, wobei sie nach jeder Aktion auf ausstehende Update chunks pr\'fcft.\par
- Eine Aktion ist z.B. die Verarbeitung einer Anfrage von au\'dfen. Oder jede beliebige Logik, die als Runnable ausgef\'fchrt wird.\par
- Von den Update Chunks aus aktualisiert sich eine Aktivit\'e4t direkt ihre existierenden Instanzen mit passender OID. Alle anderen Chunk-Daten ignoriert die Aktivit\'e4t.\par
- Der Storage Metadaten Fortschritt wird immer nur so weit gesetzt, wie die am weitesten zur\'fcckh\'e4ngende Aktivit\'e4t ist.\par
- Muss eine Aktivit\'e4t Daten nachladen (Lazy), wird gepr\'fcft, ob ihr Update-Stand und der der Storage-Ebene \'fcbereinstimmt. Wenn nicht, muss sie warten.\par
- Um Laufzeitaufwand f\'fcr Aktivit\'e4ten zu minimieren, k\'f6nnen alle nicht-Entity-Graph-bezogenen Arbeiten in Hilfsthreads ausgelagert werden, z.B. Netzwerkkommunikation.\par
- Das Konzept k\'f6nnte zuk\'fcnftig bis auf OID-Ebene verfeinert werden, wodurch Aktivit\'e4ten fast nicht mehr aufeinander warten m\'fcssten.\par
\par
\par
Damit bleibt als einziges "Eckchen" einer geclusterten Anwendung, dass eine Aktivit\'e4t auf einem Slave einen Request an den Master schickt und dieser beim Validieren aufgrund von zwischenzeitlich ge\'e4nderten Daten eine Exception wirft, mit der die Aktivit\'e4t umgehen muss. I.d.R. R\'fcckmeldung an den Benutzer und nochmal versuchen.\par
Das liegt in einem nebenl\'e4ufigen System in der Natur und kann nicht gel\'f6st werden, bzw. nur durch ein pessimistisches Lock auf Anwendungsebene.\par
\par
Ach, und ein Problem ist noch, wie \'c4nderungen an "Second Class" Entities, also ausgelagerten Massendaten-Entities, konsistent kommunziert und aktualisiert werden k\'f6nnen.\par
Trivial einfach w\'e4ren hier Deep Immutable Subgraphen: Bei Bedarf von dem anderen node laden, nie wieder irgendwas updaten m\'fcssen. Fertig.\par
Bei Massendaten-Entity Subgraphen, die sich schon mal \'e4ndern k\'f6nnen, und sei es nur gelegentlich, ist das schwieriger.\par
Die Kommunikation, dass jetzt eine neue Version vorliegt, usw., kriegt man schon hin. Aber wie kriegt man es hin, alle m\'f6glichen Referenzen auf solche Instanzen in der Anwendung zu kappen? Da hilft auch Instanzen live updaten nicht, weil das nicht die wegfallenden Instanzen abdeckt.\par
Vielleicht sollten solche Second Class Entities nur Deep Immutable sein d\'fcrfen. Quantitativ m\'fcsste das auch v\'f6llig ausreichen.\par
Was wirklich gro\'dfes volumen erzeugt sind ja nicht 1-2 oder 100-200 "Kopfdaten"-artige Instanzen, sondern irgendwelche Collections mit 1000 und mehr Instanzen.\par
Es w\'fcrde v\'f6llig reichen, nur die Collections und die gehaltenen Instanzen immutable zu machen und auszulagern. Falls sie deep immutable ist, kann da schon auch mal die eine oder andere Kopfdaten-artige Instanz dabei sein, aber im Wesentlichen geht es darum, dicke deep immutable collection instanzen auszulagern.\par
Man kann wachsende Datenmengen auch intelligent segmentieren, um die Masse der Daten deep immutable zu halten:\par
Anstatt einer giga-Collection mit einer million Eintr\'e4gen, die immer wieder mal erweitert wird, macht man eine Table, die Segmente h\'e4lt und jedes Segment ist deep immutable mit so um die 1000 Instanzen. Die kann man f\'fcr \'c4nderunden dann sogar gegen ver\'e4nderte austauschen. Dann haben sich Daten auf einer relativ kleinen Ebene ge\'e4ndert, aber trotzdem sind 99% der Daten Deep immutable. Nur die Segment-Kopf-Table w\'e4re mutable. Die liegt halt dann im normalen Entity Graph. St\'f6rt ja keinen.\par
Falls die Instanzen in den Collection selbst mutable sind, dann m\'fcssen sie teil des normalen Entity Graphen sein und k\'f6nnen nicht ausgelagert werden.\par
\par
Das ist eine super L\'f6sung. Komplexere F\'e4lle, mutable Zeug auszulagern, f\'fchren nur in Teufels K\'fcche.\par
\par
D.h. es gibt "Hilfsmaster" Nodes, auch wieder mit Replikationsslaves, denen man unter einer ID einen Deep Immutable subgraphen hinschmeissen kann und den die bei Bedarf wieder zur\'fcckgeben.\par
Hm, interessant ist: die br\'e4uchten gar keine Klassen. Es w\'fcrde reichen, den erhaltenen binary chunk einfach wegzuschreiben. Garbage Collector usw. ist darauf ja nicht n\'f6tig, weil sich nie was \'e4ndern kann, darum auch keine Instanzen unerreichbar werden k\'f6nnen, usw.\par
Es muss halt nur einen trivialen mechanismus geben: Wenn f\'fcr eine ID ein neuer Byte block geschickt wird, dann den bisherigen f\'fcr diese ID l\'f6schen. Das sollte hinzukirgen sein. Evtl. kann man die dann einfach so abspeichern, wie ein file oder ein video: Als Binary Block in einem Cloud dingsbums store Ding.\par
Die Typdefinitionen sollten evtl. noch mitgeschickt werden, damit man den Batzen sp\'e4ter validieren und ggf. Legacy-Type-Mappen kann.\par
Bzw. eigentlich sollten die Typdefinitionen im TypeDictionary der Anwendung stehen und sollten dort auch nicht einfach rausgel\'f6scht werden, dann kann die Anwendung das beim Start gleich mappen.\par
Hm, wobei, schwierig: bisher macht sie das, indem alle vorkommenden TypeIds bei der Initialisierung gesammelt werden. Wenn erst zur Laufzeit solche Deep Immutable Graphen geladen werden, fehlt evtl. noch das Mapping. Dann m\'fcsste man es so machen, dass f\'fcr alle Typdefinitionen im Dictionary ein Handler sichergestellt wird, um f\'fcr alle eventualit\'e4ten gewappnet zu sein. Oder irgendwas findet sich da schon. Live TypeIds definieren oder so.\par
\par
Hm, das w\'e4re das ziemlich geil: Massendaten-Entities auslagern, ohne sie \'fcberhaupt als Entities zu verwalten. Das w\'fcrde einen kompletten Zweig an Node Typ und Replikation daf\'fcr entfallen lassen.\par
\par
\par
Hm, das Konsistenzproblem mit den Referenzen gibts aber trotzdem noch. Aber nicht anders, als wenn man eine ganz simple singlethreaded Anwendung hat:\par
Subgraph wird geladen.\par
Anwendungslogik referenziert bestimmte Instanzen in diesem Subgraph.\par
Subgraph wird neu geladen, die referenzierten Instanzen sind nun nicht mehr darin enthalten.\par
Damit sind die Referenzen aus der Anwendungslogik inkonsistent.\par
Aber ist aber eben ein Fall, der bei jeglichen Referenzen in jeder Anwendung vorkommen kann.\par
Man muss halt in Anwendungsverantwortung darauf achten, durch "Referenzen von der Seite" nicht veraltete Instanzen inkonsistent am Leben zu halten.\par
}
 