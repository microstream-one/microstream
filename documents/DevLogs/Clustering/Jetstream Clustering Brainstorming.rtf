{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1031{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\colortbl ;\red0\green0\blue255;}
{\*\generator Riched20 10.0.17134}\viewkind4\uc1 
\pard\sl276\slmult1\f0\fs28\lang7 Jetstream Clustering Brainstorming\par
2018-10-16\par
\par
\par
"Lowlevel"\par
1 Master - liest und schreibt Daten\par
N Slaves - bekommen Kopien vom Master repliziert, nehmen ihm lese-Last ab, agieren als Failover Master\par
\par
\par
"Microservice"\par
Ist ein Teil der Anwendung (Beispiel "Umsatzserver").\par
Jede Microservice ist f\'fcr eine disjunkte Menge fachlicher Daten exklusiv zust\'e4ndig.\par
F\'fchrt Validierung von zu speichernden Daten auf fachlicher Ebene durch. Alles "darunter" kann ruhigen gewissens ungepr\'fcft "dumm" speichern\par
Kann einen "Lowlevel" Teil direkt embedded haben oder kann N Lowlevel Teile per Netzwerk ansprechen.\par
Jeder "Lowlevel" Teil wird nach OID Modulo segmentiert.\par
\par
"Masterservice"\par
Kennt alle Microservices, d.h. er "ist" in Summe "Die Anwendung".\par
\par
Problem:\par
Damit neben die LowLevel Slaves keine Leselast mehr ab.\par
Es muss eher so sein, dass ein Microservice entweder der Master ist oder es N Slave-Microservices gibt und wenn was gespeichert werden soll, muss immer an den Master-Microservice gleitet werden.\par
D.h. die Anwendung muss auch fachlich zwei Schichten unterteilt werden:\par
In...\par
- "Data Services", die liefern fachliche Daten, validieren gew\'fcnschte \'c4nderungsanfragen und schreiben g\'fcltige \'c4nderungen\par
- "Work Services", die sind selbst transient, also ohne gespeicherte fachliche Daten, halten Usersessions, gecachte Bilder und so Grusch und reden je nach Anforderung mit einem beliebigen Data-Slave (lesend) oder mit dem Data-Master (schreiben).\par
\par
Kommunikation zwischen Data und Work Nodes erfolgt \'fcber Jetstream-serialisierte Objektgraphen.\par
\par
Dar\'fcber muss es einen Load Balance geben, der Anfragen von au\'dfen auf die Work Nodes verteilt und Anfragen von Work Nodes auf die Data-Slaves verteilt.\par
\par
Die Anzahl der Work Nodes kann beliebig skaliert werden. Sie haben gar keinen persistenten State.\par
Die Anzahl der Data-Slaves kann beliebig skaliert werden. Sie bekommen komplette Kopien ihres Masters und sind read-only.\par
Die Anzahl der Data-Master ist fest und muss entsprechend der Architektur der Anwendung (Datenmodell, Logik) designt werden.\par
Ein Data-Master ist der Teil, der exklusive fachliche Validierung von zusammengeh\'f6renden Teilen ausf\'fchren muss. Soetwas kann man nicht generisch clustern.\par
\par
Man kann einen Data-Master aber oft nachtr\'e4glich auf fachlicher Ebene clustern.\par
Beispiel:\par
Umsatznode.\par
Anfangs 1 monolithischer Node mit allen Ums\'e4tzen.\par
Sp\'e4ter 1 Node f\'fcr die Ums\'e4tze des aktuellen Jahrs, 1 node f\'fcr die letzten beiden Jahre und 1 Node f\'fcr alle vorherigen.\par
Der Anwendungscode ist dann auf jedem Master identisch, nur mit anderen Segmenten der Daten, f\'fcr die er zust\'e4ndig ist.\par
\par
Um alle Nodes, also Work, Data und Master untereinander, kann man mit Lazy<> Referenzen abbilden.\par
Die h\'e4lt erst mal nur eine OID und kennt den zust\'e4ndigen Node. Bei einem .get() wird dieser Node kontaktiert und alle Daten bis zur n\'e4chsten Lazy<> Referenz \'fcbertragen.\par
\par
Knifflig: F\'fcr jede geladene Instanz muss registriert werden, wer ihr zust\'e4ndiger Data-Master Node ist. Bei einem store() Aufruf f\'fcr einen Entitygraph muss je nach Instanz nach den zust\'e4ndigen Data-Mastern unterschieden werden.\par
Der commit() Aufruf des Storers schickt die serialisierten Entities an ihre jeweiligen Data Master. Diese Validieren und melden erfolg oder miserfolg zur\'fcck. Wenn alle beteiligten Data-Master Erfolg gemeldet haben, persistieren alle Data-Master die validierten Entities. \par
\par
Problem: Was ist, wenn alle erfolg gemeldet haben, aber ein oder mehr Data-Master beim Speichern auf ein Problem gesto\'dfen sind?\par
Dann wurden manche der Entities gespeichert und manche nicht.\par
Auf Checks und Rollbacks kann man sich nicht verlassen, weil ja wiederum manche der Nodes, die rollbacken m\'fcssten, auf ein Problem sto\'dfen k\'f6nnten.\par
Muss es stattdessen eine Art Transaction-ID geben, die \'fcber den gesamten Cluster gilt und bei der Initialisierung aller Nodes f\'fcr den letzten Store auf Einheitlichkeit grepr\'fcft wird?\par
K\'f6nnte klappen.\par
\par
Alternative:\par
Ein Store darf - gepr\'fcft \'fcber die per-instanz-registry nur entities eines Data-Masters enthalten.\par
Hei\'dft: wenn man Kunden und Ums\'e4tze speichern will, muss man einen storer f\'fcr die Kunden und einen storer f\'fcr die Ums\'e4tze machen.\par
Aber was ist dann, wenn ein Umsatz seinen Kunden kennt und der Kunde seinen Umsatz?\par
Es m\'fcsste dann im Design f\'fcr jeden \'dcbergang im Datenmodell von einem Master-Bereich zum anderen ein Lazy vorgesehen sein.\par
\par
\par
\par
Ideal w\'e4re folgendes:\par
- Es gibt die Anwendung Code-m\'e4\'dfig nur einmal als Monolith.\par
- Jeder Microservice startet die komplette Anwendung, ist aber nur f\'fcr einen Teil der Daten zust\'e4ndig.\par
- Irgendwie muss sichergestellt sein, dass die Entities, die ein Microservice reingeschickt bekommt, nur die Logik triggern, f\'fcr die der Microservice mit seiner Zust\'e4ndigkeit in den Daten ausf\'fchren kann.\par
\par
Bl\'f6d daran aber:\par
Microservices sollen unter anderem ja auch die Code Komplexit\'e4t reduzieren:\par
Der Umsatzserver muss nur den Code zum Ums\'e4tze handeln haben.\par
\par
\par
Und noch ein Problem: wenn neue Instanzen zum ersten mal gespeichert werden, woher wei\'df ein Microservice, dass er wirklich daf\'fcr fachlich segment-m\'e4\'dfig zust\'e4ndig ist?\par
\par
\par
Und ein ziemlich entscheidendes Problem, geradezu ein Show-stopper:\par
Wenn sich irgendwelche Nodes von irgendwelchen anderen Nodes Daten laden und die "anderen Nodes" dann neue Versionen dieser Datens\'e4tze bekommen und abspeichern, wie bekommen die Nodes mit den geladenen Daten das dann mit?\par
Das ist genau dasselbe Probleme wie bei einer RDBMS+Java Anwendung, wo eigentlich der Java Prozess der "Chef" sein sollte, \'fcber den zwecks Businesslogik Validierung usw. alle Daten\'e4nderungen laufen sollen, aber dann \'e4ndert einer an der Seite vorbei direkt in der Datenbank rum. Der Java Prozess bekommt das nicht mit, also sind, schwups, mal eben, seine geladenen Daten inkonsistent. Also m\'fcsste er bei jeder Operationen pr\'fcfen, ob neuere Versionen da sind, oder die Caches ganz abschalten (Performance = tot) oder wegen jeder kleinen Scheiss\'e4nderung cache invalidation signale bekommen.\par
Das Kernproblem ist: Es darf nicht mehrere Wege geben, an einer Menge an Daten herumzu\'e4ndern. F\'fcr eine Menge an (fachlich sinnvoll zusammengeh\'f6render) Daten darf immer nur genau ein Prozess der sein, der \'e4ndert und durch den fliesen alle \'c4nderungen durch, damit der immer auf dem neuesten Stand ist.\par
\par
Darum vielleicht folgender, radikaler Ansatz:\par
- Es gibt nach wie vor 1 Data-Master und N Data-Slaves, die sich Kopien der Daten ziehen bzw. bekommen.\par
- Die Slaves sind aber gleichzeitig die Worker Nodes.\par
- F\'fcr \'c4nderungen leitet jeder Slave die Daten an den Data-Master, der validiert, persistiert die \'c4nderung und sendet dann die Updates wieder an jeden Data-Slave.\par
\par
Aber was ist jetzt mit Skalierung? Zumindest f\'fcrs Schreiben ist das ja dann ein monolithisches System.\par
Antwort:\par
1.)\par
Zun\'e4chst mal fliegt alles, was "triviale" Bin\'e4rdaten (Bilder, T\'f6ne, Videos, gro\'dfe Texte) sind, aus dem System ganz raus. Solche Daten kann sich ein Client per URL von irgendeinem Image Cache Server nachladen.\par
\par
2.)\par
Dann k\'f6nnen alle fachlichen Daten, die sich nur "selten" \'e4ndern ("selten" hei\'dft "nur" alle paar Minuten oder so), in mehrere andere Data-Master ausgelagert werden. JEDES Update von denen bewirkt dann ein Signal an die Worker Nodes, dass sie ihren Cache f\'fcr den betreffenden Data-Master invalidieren/leeren m\'fcssen.\par
\par
3.)\par
Was dann \'fcbrig bleibt, ist ein Kern-Node mit nur noch den sich schnell \'e4ndernden (Minuten oder weniger) Entities, die f\'fcr sich ziemliche winzige Datens\'e4tze sind (~100 Byte).\par
Wenn die mal "\'e4lter" werden und sich nicht mehr oder nur noch wenig \'e4ndern, dann kann man die in so einen "selten" Node verschieben.\par
Der Kern-Node hat also immer nur die "hei\'dfen" und "kleinen" Daten.\par
Und der muss dann nat\'fcrlich eine der Anwendung angemessene Gr\'f6\'dfe haben. 16 Core, 512 GB RAM oder noch mehr.\par
Bis man so ein Riesensystem mit relativ wenigen und kleinen Daten mal an seine Grenzen bringt, dauert das.\par
\par
Je nach Datenmodell kann man von den Kern-Nodes auch durchaus mehrere machen, aber das muss dann anwendungslogisch sauber designt sein. Z.B., keine Ahnung, Kundendaten, die sich wirklich sch\'f6b sauber disjunkt von A bis Z durchsegmentieren lassen, wodurch ein Client f\'fcr die Bearbeitung eines Kunden immer nur auf einem Kern-Node arbeiten muss. Es kann aber gut sein, dass das bei den meisten Anwendungen nicht so aufteilbar ist.\par
\par
Und man kann nat\'fcrlich immer noch ma\'dfgeschneiderte Speziall\'f6sungen machen. Wenn etwa permanent millionen Messdatens\'e4tze reinsprudeln, dann speichert man die nicht unbedingt jeden als einzelnes Entity, sondern man speichert die in Tauserbl\'f6cken direkt als Binary Blob ab.\par
\par
Dieses Konzept hat immer Clustering f\'fcr Ausfallsicherheit:\par
Die Data-Slaves bekommen permanent live die neuesten Updates und stehen jederzeit bereit, als neuer Master zu \'fcbernehmen.\par
\par
Es hat auch immer noch Clustering f\'fcr Lastenverteilung:\par
Die Data-Slaves nehmen dem Data-Master die gesamte Lese-Arbeit ab. Der Data-Master muss nur noch validieren und schreiben. Und ich wette, das geht genauso schnell, wie wenn ein chaotischer Haufen an permanent race-condition-konkurrierenden Nodes sich st\'e4ndig untereinander synchen m\'fcssen.\par
\par
Und es hat auch immer noch Clustering f\'fcr Skalierung:\par
Alles, was "gro\'df" ist, wird in andere Systeme ausgelagert. Der "Kern" Data-Master k\'fcmmert sich nur um "kleine" und "hei\'dfe" Daten.\par
\par
\par
Eine nette kleine Testfrage f\'fcr Konzepte ist immer: "K\'f6nnte man damit Facebook nachbauen?"\par
Ja, k\'f6nnte man, denn:\par
Alle Bilder, Videos, evtl. sogar die Strings der Nachrichten und die Werbungen sowieso, kommen von anderen Servern.\par
Die >1 Milliard Benutzer kann man ziemlich leicht auf mehrere Data-Master verteilen. So ein Benutzer ist eine recht abgeschlossene Sache. Profildaten, Kontakte, Nachrichtendaten. Das kann sich das Browser-Frontend je Benutzer von verschiedensten Data-Master Nodes holen, ohne dass es jemals einen Konflikt mit der Aktualit\'e4t gibt. Die k\'f6nnten dann jeweils gleich in den entsprechenden Regionen stehen. Ihre Data-Slaves stehen dann nat\'fcrlich verteilt \'fcber die ganze Welt, aber wenn f\'fcr einen bestimmten Benutzer Daten validiert&geschrieben werden, landet das immer bei dem EINEN zust\'e4ndigen Data-Master.\par
\par
\par
\par
2018-10-18\par
\par
Ich schreib mal kurz eine Idee zu einer klareren "Datensatzklassifizierung" von oben auf.\par
\par
\par
\b Daten vierter Klasse\par
\b0 Gro\'dfe ValueType Objekte, die mehr den Charakter von "Nutzlast" als von Datenmodell-Entities haben.\par
Beispiele: Bilder, Audio, Video, Dokumente, gro\'dfe Texte (> 1000 Zeichen).\par
Werden in einem ausgelagerten System gespeichert und \'fcber irgendeinen eindeutigen Identifier (UUID oder so) referenziert. Irgendein Cloud Storage Ding oder so.\par
Anwendungsserver m\'fchen sich mit solchen Daten selbst normalerweise gar nicht ab, sondern schicken Clients (Browser, App, etc.) nur den Identifier und die sollen sich das Ding bei Bedarf selbst runterladen, statt dass sie es m\'fchsam vom Anwendungsserver geliefert bekommen. Das Cloud Ding kann dann in sich geclustert, repliziert, wei\'df der Geier sein, das braucht den Anwendungsentwickler nicht k\'fcmmern.\par
Diese Objekte sind immutable, d.h. sie ver\'e4ndern sich niemals. Sollte es mal eine \'c4nderung an den Bytes geben, f\'fchrt das zu einer neuen Version mit einem neuen Identifier. Aber ein Identifier steht immer f\'fcr eine bestimmte "Kombination an Bytes". Unver\'e4nderlich f\'fcr alle Ewigkeit.\par
Wahrscheinlich muss man von Zeit zu Zeit "aufger\'e4umt" werden: Alle live referenzierten Identifier sammeln und dem Storage Ding sagen "L\'f6sch von mir alles, au\'dfer folgende". Oder umgkehrt: Eine Liste aller verf\'fcgbaren geben lassen und alle nicht mehr live referenzierten l\'f6schen. Das sind technische Details.\par
\par
Durch die Auslagerung dieser Daten nimmt man schon mal gigantisch viel Volumen aus der Datenbank raus. \'dcbrig bleiben nur noch "echte Datens\'e4tze", die typischerweise um die 50-100 Byte gro\'df sind (Collections etwas gr\'f6\'dfer).\par
\par
\b Daten dritter Klasse\par
\b0 Immutable Entities. Da die sich nie \'e4ndern, kann man sie beruhigt auf einen anderen "Master" Node auslagern und von dort bei Bedarf immer wieder laden. Es wird bei denen nie Probleme mit Schreib-Konflikten oder veralteten Caches geben.\par
Beispiele: \par
Schwer zu sagen, weil ziemlich anwendungsspezifisch. Vielleicht soetwas wie Messdaten: Die werden einmal erhoben und \'e4ndern sich dann nie wieder. Millionen von Datens\'e4tzen, die zwar schon "Entities" sind, aber die in ihrer Natur so leicht zu handeln sind, dass es Verschwendung w\'e4re, ihnen eine erstklassige Behandlung zukommen zu lassen. Nat\'fcrlich macht es keinen Sinn, jedes kleine immutable Einzelinstanz in ein andere System zu verschieben. Etwa einen Integer mit Wert 5. Sondern hier sind gro\'dfe Mengen an zusammengeh\'f6renden immutable Entities gemeint.\par
Wie das so ganz genau ist mit dem Immutable ist immer so eine Sache: Irgendwann m\'fcssen die Entities ja mal erzeugt und registriert, z.B. in eine Collection gesteckt werden. Also so "ganz immutable" kann es nicht sein.\par
Hier ist der \'dcbergang zur zweiten Klasse flie\'dfend, eventuell ist beides sogar einfacher als dasselbe abbildbar.\par
\par
Durch die Auslagerung dieser Entities l\'e4sst sich die Gesamtzahl der erster Klasse zu managenden Entities dramatisch reduzieren, ohne sich dabei architektonische Probleme (Konflikte) einzuhandeln.\par
\par
\par
\b Daten zweiter Klasse\par
\b0 Gro\'dfe Mengen an Entities, die sich nur selten \'e4ndern. "Selten" hei\'dft in der Welt von Computern: Nur alle Stunde oder noch seltener.\par
Diese werden auf einen eigenen "Master" Node ausgelagert.\par
Die Idee dahinter ist: Das Zeitintervall soll gro\'df genug sein, dass es sich rentiert, dass der zust\'e4ndige Node an die Anwendungsnodes bei jeder \'c4nderung signalisieren kann: "Bei mir hat sich was ge\'e4ndert, du musst alle deine Entities, die du von mir gecacht hast, invalidaten/clearen und bei Bedarf neu laden."\par
Wenn die Intervalle zu kurz w\'e4ren (~Sekunden), w\'fcrden die Anwendungsserver st\'e4ndig nur noch caches leeren und neu aufbauen, darum "seltene" \'c4nderungen. Als Optimierung f\'fcr kurze Intervalle nicht "alles" clearen, sondern nur ge\'e4nderte Instanzen, w\'fcrde schon wieder zu viel Verwaltungsoverhead erzeugen: Rechenaufwand, um alle ge\'e4nderten IDs zu sammeln, IDs \'fcbertragen, in den Anwendungsnodes die ID Liste durchlaufen, aufl\'f6sen, die entsprechenden Instanzen invalidaten. \par
Was ist dann mit Instanzen von und auf diese Instanzen zu eigentlich unver\'e4nderten Instanzen? Alles schwierig. Darum simpler Ansatz: ALLES, was von dem betreffenden Second-Class Masternode gecacht ist, einfach clearen.\par
Auch hier w\'e4ren Beispiele stark anwendungsabh\'e4ngig. \par
Viele Anwendungen m\'f6gen solche Art von Daten \'fcberhaupt nicht haben. Viele andere haben fast nur solche. Vielleicht soetwas wie Artikeldaten. Eine Million Artikel und all ihre Entities drum rum. Da \'e4ndert sich schon mal immer wieder was dran. Vielleicht mehrfach t\'e4glich, wenn Sachbearbeiter \'c4nderungen vornehmen. Vielleicht werden nur einmal t\'e4glich batchm\'e4\'dfig alle Artikel\'e4nderungen auf einmal im System "ver\'f6ffentlicht".\par
Oder ein Beispiel aus der pers\'f6nlichen Entwicklererfahrung: Bonusberechnungen. Es gibt Vorg\'e4nge, die werden vom Sachbearbeiter ausgef\'fchrt und die berechnen dann aus Ums\'e4tzen und Stammdaten, wie viel Kohle jedem Mitglied ausgezahlt wird. Jeder Vorgang erzeugt schon mal so 100.000 Entities oder noch mehr. Aber diese Vorg\'e4nge laufen vielleicht ... naja ... 10 mal im Monat. Selbst, wenn ein Sachbearbeiter herumprobiert, ausbessert, Daten nachtr\'e4gt, usw. und daf\'fcr immer wieder Vorgang ausf\'fchren und wieder resetten muss, dann passiert das vielleicht ab und zu 10 mal an einem Tag. Bei solchen Intervallen ist es noch problemlos m\'f6glich, in der Anwendungslogik einfach ALLES gecachte wegzuwerfen und bei Bedarf neu zu laden.\par
\par
Durch die Auslagerung dieser Entities l\'e4sst sich die Gesamtzahl der erster Klasse zu managenden Entities dramatisch reduzieren, ohne sich dabei architektonische Probleme (Konflikte) einzuhandeln.\par
\par
\b Daten erster Klasse\par
\b0\'dcbrig bleiben dann nur noch Entities "Erster Klasse". Kleine Entities, die sich potenziell sehr oft \'e4ndern und wo sich unter Umst\'e4nden sogar mal nur ein einzelnes Entity \'e4ndert.\par
Klassisches Beispiel: Kunde \'e4ndert sein Profil. Anschrift, Einstellungen, irgendsowas.\par
Nachdem alle Daten niedrigerer (einfacherer) Klasse in andere Systeme ausgelagert wurden, bleibt hier nicht mehr viel \'fcbrig.\par
Die kann ein einzelner Anwendungsserver, sozusagen der "First Class Data-Master", selbst direkt handeln.\par
Das sind nat\'fcrlich immer noch Millionen Entities. Wahrscheinlich auch Milliarden. Aber das ist mit Jetstream kein Problem. Wirklich nicht.\par
Beispiel Bonusanwendung: Besteht aus ~20 Millionen Entities und sogar unsere 0815 B\'fcro-PCs als "Serverhardware" brauchen nur etwa 15 Sekunden, um die gesamte Datenbank zu initialisieren und den Server zu starten.\par
Wenn man eine einem gesch\'e4ftlichen Unternehmen angemessene Serverhardware einsetzt (sowas wie 8 Core, 128 GB RAM oder auch 32 Core, 512 GB RAM, das ist heutzutage alles kein Drama mehr), dann dann der mit \'e4hnlicher Geschwindigkeit auch 1 Milliarde solcher "First Class" Entities managen.\par
\par
---\par
Kleine, spontane Recherche zu aktueller Serverhardware:\par
{{\field{\*\fldinst{HYPERLINK https://calculator.s3.amazonaws.com/index.html }}{\fldrslt{https://calculator.s3.amazonaws.com/index.html\ul0\cf0}}}}\f0\fs28\par
x1e.32xlarge: 128 CPUs, 4 TB (!) RAM, Windows, dedicated Server. Kostet $32 pro Stunde.\par
Das mag jetzt viel klingen, $32 pro STUNDE. Das sind $23K pro Monat. Aber man muss sich vorstellen, was so ein Monster, das sich nur noch um "Entities erster Klasse" k\'fcmmern muss, abarbeiten kann: Tausende gleichzeitige Kunden. Das hei\'dft aufs Monat gesehen hunderttausende bis Millionen durchgeschleuste Leute, die wahrscheinlich hundertausende bis Millionen an Umsatz generieren. Das sollte 23K f\'fcr den Betrieb der Gesch\'e4ftsgrundlage schon rechtfertigen, sonst liegt da irgendwo eine betriebswirtschaftliche Wahrnehmungsverzerrung vor. Ich wette, so viel zahlt ein Supermarkt schon allein f\'fcr den Strom. Mit Replikationsservern wird es nat\'fcrlich mehr. Dann nimmt man entweder mehrere kleinere oder man kann daf\'fcr ja auch das X-fache an Kunden bearbeiten.\par
...\par
Kurz mal gegoogelt: Deutschsprachiger Einzelhandel Jahresstromverbrauch: ca. 100-300 kW/h. Z.B. real hat L\'e4den von 5000-15000 m\'b2. Also sagen wir 10.000 m\'b2 mit 200 kW/h. Preis, sagen wir g\'fcnstig verhandelte 20 Cent pro kW/h.\par
Macht... 33K \'80 pro Monat. Ha! Gut gesch\'e4tzt.\par
---\par
\par
\par
Und wenn das immer noch nicht reicht, dann kann man seine Anwendung auch gezielt so designen, dass man mehrere solcher "First Class Data-Master" hat. Das sind halt dann disjunkte "Welten", die auf irgendeine g\'fcnstige Art zusammenarbeiten. Das wichtige ist dabei nur, zu verhindern, dann Daten des anderen rumzu\'e4ndern oder veraltete Versionen im Cache zu haben.\par
Ich denke da z.B. an Facebook: Selbst die dickste Einzelhardware ever kann unm\'f6glich alle >2 Milliarden Benutzer und alle ihre dranh\'e4ngenden "First-Class" Entities managen.\par
Das muss unweigerlich irgendwie segmentiert, geshardet werden. Aber das ist ziemlich simpel: man baut sich 1000 First-Class Data-Master Anwendungsnodes und jeder von denen ist "nur" f\'fcr 2 Millionen User zust\'e4ndig.\par
Oder 10.000 Nodes mit je 200.000 User. Man kann auch 1 Million kleine Nodes mit je 2000 Usern drauf machen. Spielt keine Rolle. Es wird hier nie Konflikte geben, weil die Daten in sich auf fachlicher Ebene geclustert sind. Jeder User ist f\'fcr sich eine relativ abgeschlossene Einheit. Es gibt nat\'fcrlich Kontakte und Nachrichten untereinander, aber das kann man schon irgendwie sinnvoll managen. Entweder die Nachricht liegt immer auf dem Node des Senders oder, warum nicht: Es gibt einen extra Nachrichten Node (oder 1000), der (die) nichts anderes macht, als Millionen Nachrichten zwischen den Usern zu handeln.\par
Also ich will damit nur sagen: Die Aussage "Es gibt nur einen First-Class Node" hei\'dft nicht, dass es auf Gedeih und Verderb f\'fcr egal wie gro\'dfe Systeme immer nur einen zentralen Node geben kann. Es soll nur hei\'dfen, dass ein First-Class Node die gr\'f6\'dfte Einheit ist, die automatisch verwaltet werden kann. Wenn man mehr braucht, muss man seine Anwendung entsprechend strukturieren und untereinander kommunizieren lassen, dann geht das auch.\par
\par
Das Konzept ist kurz gesagt: Daten in verschiedene Klassen einteilen und je nachdem, wie viel oder wenig unkompliziert sie zu verwalten sind, kann man die Daten mehr oder weniger weit vom zentralen Anwendungsnode weg auslagern.\par
Dazu noch so read-only Data-Slaves, die den Master entlasten und failover-absichern, dann hat selbst bei riesigen Systemen ein einzelner (durchaus leistungsstarker) Master-Node genug Luft, um Anwendungslogik mit gigantische Datenmengen auszuf\'fchren und gew\'fcnschte \'c4nderungen business-logic-m\'e4\'dfig vor dem Schreiben zu validieren. Ganz ohne Schreibkonflikte im Cluster.\par
\par
\par
\par
##################\par
\par
Mir ist dazu eine super Metapher eingefallen:\par
\par
Sagen wir, eine Serveranwendung ist wie ein Laden, in dem man Dinge kaufen kann.\par
Die Artikel sind die Datens\'e4tze. Die Mitarbeiter sind die Threads.\par
Wenn ein Kunde nur reinkommt, rumschaut, sich ggf. noch beraten l\'e4sst, ist das "nur lesend".\par
Wenn ein Kunde etwas kauft, d.h. Warenbestand und Einnahmen des Ladens ver\'e4ndert, ist das "schreibend".\par
Es gibt unterschiedliche Arten von Artikeln:\par
Riesige, aber triviale Artikel, z.B. Holzbalken, Steine, usw. zum Hausbauen.\par
So mittelgro\'dfe, mittelkomplexe Artikel, wie z.B. ... keine Ahnung ... Lampen, W\'e4schek\'f6rbe, Lebensmittel.\par
Und es gibt kleine, superkomplexe Artikel, die viel Aufwand erzeugen f\'fcr Ausstellung, Beratung, Reklamation, evtl. sogar Reparatur. Klassisches Beispiel: Elektronikkram. Fotos, Smartphones, usw.\par
\par
Java mit Jetstream macht es problemlos m\'f6glich, alle m\'f6glichen Artikel gemischt im selben Laden zu verkaufen. Alles kann angeboten werden und das auch ziemlich effizient.\par
Soweit so cool.\par
Nat\'fcrlich ist es so, dass, je mehr Waren man anbieten will, man auch immer mehr Verkaufsfl\'e4che und Mitarbeiter f\'fcr Beratung und Kassieren braucht.\par
F\'fcr Java kein Problem. F\'fcr Jetstream auch nicht. Nur die Hardware macht irgendwann nicht mehr mit. G\'e4be es einen 50 GHz 1024-Core Server mit 128 TB RAM, dann w\'e4re die ganze Clusterei \'fcberfl\'fcssig. Gibts leider nicht. Bl\'f6d.\par
In der Metapher hei\'dft das: Aus irgendeinem Grund darf ein Laden nicht mehr als 100.000 Artikel anbieten und nicht mehr als 32 Mitarbeiter haben.\par
Man kann diese Grenzen noch so clever ausnutzen und ein noch so effizient laufendes Lager haben, irgendwann platzt ein einzelner Laden aus allen N\'e4hten und die Mitarbeiter kommen mit dem Beraten und Kassieren nicht mehr nach.\par
\par
Das muss man \'fcber mehrere L\'e4den clustern. Geht nicht anders.\par
\par
Der erste, durchaus naheliegende, aber auch naive Gedanke ist: Das hei\'dft einfach X mal 100.000-Artikel-L\'e4den mit X mal 32 Mitarbeiter und die Waren dann "irgendwie" auf die L\'e4den verteilen.\par
Das Problem daran ist: Die Kunden und Mitarbeiter werden vor lauter herumtelefonieren und herumlaufen zwischen den L\'e4den nicht mehr fertig. St\'e4ndig muss rumtelefoniert werden, ob es von Artikeltyp X in einem anderen Laden noch ein Exemplar gibt, dann m\'fcssen die Kunden dorthin laufen, usw. Der Vergleich hinkt ein bisschen, verglichen damit, wie Cluster wirklich funktionieren, aber die Grundaussage ist richtig: Es gibt einen riesigen Overhead an Koordinierungsbedarf, Konflikte bei "schreibenden" Zugriffen ("kaufende Zugriffe", hehe), Austausch der Waren zwischen den M\'e4rkten, usw.\par
\par
Darum folgende Idee, das ganze etwas eleganter zu clustern:\par
Ein Laden k\'fcmmert sich nur um die riesigen, aber trivialen Artikel. Der hat dann vielleicht sogar optimierte Lagerfl\'e4chen und Ausstellungsfl\'e4chen, Kabelstapler, usw.\par
Ein Laden k\'fcmmert sich um alles zu k\'fchlende Zeug (Lebensmittel). Mit K\'fchlanlagen usw.\par
Ein Laden k\'fcmmert sich um das ganze kleine, komplexe und beratungsintensive Zeug (Smartphones usw.). Mit schicken Ausstellungsst\'e4nden, Lounges mit Musik, usw.\par
\par
Zus\'e4tzlich macht man von jedem der drei "Hauptl\'e4den" noch identisch best\'fcckte "Ausstellungsl\'e4den".\par
(Hier hinkt das Beispiel wieder ein bisschen: F\'fcr physikalische L\'e4den w\'e4re das ein riesiger Aufwand, aber in der digitalen Welt ist eine simple Kopie ziehen trivial zu machen).\par
In den Ausstellungsl\'e4den k\'f6nnen die Kunden durchlaufen, sich die Artikel ansehen, ausprobieren, sich beraten lassen, usw.\par
Aber gekauft wird nur im Hauptladen, damit an einer zentralen Stelle alles richtig verbucht wird.\par
Je mehr Kunden es gibt, umso mehr Ausstellungsl\'e4den macht man. Das ist \'fcberhaupt kein Problem. So eine Kopie ist schnell erstellt (in der Digitalen Welt), dann gehen die T\'fcren auf und 1000 weitere Kunden k\'f6nnen fr\'f6hlich reinstr\'f6men und rumschauen.\par
Die Hauptl\'e4den machen hingegen gar keine Beratung mehr. Dort geht man nur noch rein, wenn man schon ausgesucht hat, was man kaufen will. Man kriegt es ausgeh\'e4ndigt und geht zur Kasse.\par
Dadurch k\'f6nnen auch alle 32 Mitarbeiter des Hauptladens an den Kassen sitzen und kassieren.\par
Und jetzt kann man sich gut vorstellen: 32 Kassen nebeneinander, die den ganzen Tag lang ununterbrochen nur noch Kunden mit Ware in der Hand abkassieren, ohne Beratung usw., k\'f6nnen ein gigantisches Volumen an Kunden und an Warenausgabe bearbeiten.\par
\par
Es muss auch nicht unbedingt bei diesen drei L\'e4den bleiben. Man kann durchaus einen zweiten Laden f\'fcr kleine, komplexe Artikel aufmachen. Das m\'fcsste halt dann thematisch getrennt sein, damit nicht wieder Chaos untereinander ausbricht.\par
Etwa wie Mediamarkt und Obi. Der eine verkauft Elektronikschnickschnack, der andere verkauft Baustuff. Je nachdem, was man braucht, f\'e4hrt man zu dem einen oder dem anderen. Wenn \'fcberhaupt, gibt es zwischen beiden nur minimalen Koordinationsbedarf und \'fcberhaupt keine "schreibenden" Konflikte.\par
\par
Genauso ist die Idee mit den nach Datenklassen und mit Read-only Nodes geclusterten Servern.\par
\par
##################\par
\par
\par
\par
Da sind nun im Detail nat\'fcrlich noch ein paar Fragen offen: Wie erfolgt diese Zerlegung technisch genau?\par
Was die Binary ("viertklassigen") Daten angeht:\par
Etwas \'e4hnliches wie die Lazy Reference, die intern nur den Identifier h\'e4lt.\par
Falls die Serveranwendung selbst die Binary Instanz braucht, kann sie sie damit laden.\par
Ansonsten wird dem Client nur der Identifier (bzw. ein entsprechender Wrapper) mitgeschickt und der l\'e4dt sich die Binary Instanz (oder vielleicht sollte man es eher "Resource" nennen) selbst.\par
So ein "Wrapper" kann z.B. in der HTML Welt einfach ein <img> oder <video> tag sein, in dem URL+Identifier drin steht.\par
F\'fcr eine Client App w\'e4re es wohl eher eine echte Wrapper Instanz, die URL+Identifier intern h\'e4lt und bei Bedarf eine Verbindung aufbaut, l\'e4dt, Verbindung schlie\'dft, lokal cacht, zur\'fcckgibt.\par
Das sollte trivial machbar sein.\par
\par
Die "First Class" Entities sind auch simpel:\par
Das ist ein Good Old Oldschool Java Datenmodell, ohne Einschr\'e4nkungen, mit Lazy Referenzen an g\'fcnstigen Stellen. Fertig.\par
\par
Knifflig ist das Zwischending: Viele Entities, die sich selten \'e4ndern, auf einem anderen System.\par
Das m\'fcsste auch ein eigenst\'e4ndiger Teil mit Anwendungslogik sein. Vielleicht nur sehr wenig. Eher wie eine "Lagerhalle" der Anwendung. Mit eigenem Entitygraph.\par
Der Hauptteil der Anwendung muss die Entitytypen daf\'fcr aber auch kennen. Und bei Bedarf muss er sich die Entities vom Lagerhallennode reinladen k\'f6nnen.\par
An den "Hauptteil" des Entity Graphen w\'e4re dieser Subgraph wohl angeschlossen \'fcber auch wieder eine spezielle Lazy reference, die gecleart wird, sobald der "Lagerhallennode" ein Signal schickt, dass seine Daten sich ver\'e4ndert haben.\par
Das knifflige ist jetzt: Es muss sichergestellt werden, dass bei so einem "clear" tats\'e4chlich alle Instanzen, die persistent eigentlich im "Lagerhallennode" "wohnen", auch wirklich "gel\'f6scht", also vom JVM GC collectet werden.\par
Dazu ist n\'f6tig:\par
- Root-Lazy-Reference des "importierten" Subgraphen clearen.\par
- GC muss zwangsweise laufen, um die Instanzen des Subgraphen aufzur\'e4umen. System.gc() explizit aufrufen als Teil des Produktivbetriebs ist irgendwie immer doof. Und man hat keine Garantie, dass der GC dann wirklich komplett durchl\'e4uft.\par
- richtig knifflig: es darf keine einzige der "First Class" Entities so eine importierte "Second Class" Entity hart referenzieren, sonst kann diese nicht aufger\'e4umt werden. L\'f6sung: wenn Referenzieren, dann weak. Aber das muss im Anwendungsdesign peinlich genau ber\'fccksichtigt werden.\par
\par
Das hei\'dft: alle so ausgelagerten Entities m\'fcssen ein in sich abgeschlossener Subgraph sein, der importiert und wieder geclearet werden kann.\par
\par
\par
Hm. Vielleicht ist das aber zu kompliziert gedacht. Das ganze dirty-notification-Zeug mit n\'f6tigem GC-Aufruf ist seltsam.\par
\par
Vielleicht ist folgendes besser:\par
\par
Die Second-Class Entities werden vom First-Class Anwendungsserver erzeugt. Muss ja eigentlich so sein, weil nur der die ganze Anwendunglogik und die n\'f6tigen First-Class Entities f\'fcr Validierungen usw. hat.\par
Aber der Trick ist: Weil das sich eher selten \'e4ndernde, aber daf\'fcr viele Daten sind, schiebt der First-Class Master die an einen Second-Class Master ab. Genauer gesagt schiebt er einen ganzen Subgraph ab, der nur \'fcber eine Root referenziert wird und auf den es keine direkten Referenzen "von der Seite" gibt. Lazy m\'fcsste okay sein.\par
Der Second Class Master hat selbst keine Anwendungslogik und er bekommt \'c4nderungen immer nur vom First-Class Master. Damit ist gar kein Dirty-Signal n\'f6tig.\par
Nat\'fcrlich muss damit der First-Class Master die ganze Logikarbeit machen, die Entities zu erzeugen, Operationen darauf zu validieren, usw.. Aber das macht nichts. Zum einen muss/kann das eh nur er machen. Zum anderen ist das Ziel ja nicht,\par
dem First-Class Master Rechenleistung f\'fcr die Logik abzunehmen, sondern nachrangige Daten aus seiner Datenbank auszulagern, damit die m\'f6glichst klein und auf das wichtigste konzentriert bleibt.\par
\par
Das sch\'f6ne daran ist:\par
Damit werden l\'e4stige Altdaten automatisch "aus dem System raus" gebracht, aber sind trotzdem noch verf\'fcgbar.\par
Etwa Ums\'e4tze von Vorjahren. Irgendwann nach 5 Jahren wird der First-Class Server die Ums\'e4tze von vor 5 Jahren einfach gar nicht mehr brauchen. Er verschwendet damit auch gar keinen Platz auf Festplatte oder RAM. Er fordert die Subgraphen nur einfach nie wieder vom seinem Second-Class Handlanger-Node an.\par
Und was auch sch\'f6n ist:\par
Wenn eine passende Subgraph Datenstruktur (z.B. eben Ums\'e4tze) einmal konzipiert ist, dann k\'f6nnen Instanzen davon auf mehrere Second-Class-Nodes verteilt werden.\par
Die Ums\'e4tze 2021-25 auf den Second-Class-Node #1, dann schafft man einen zweiten an, der h\'e4lt dann 2026-2030, usw.\par
Irgendwann werden vielleicht mal mehrere der Subgraphen wieder auf einen Node zusammenkopiert, der als Archiv dient: Riesige Festplatte, aber wenig Rechenleistung. Wenn jemand die Uralt-Daten braucht, werden die schon geliefert, aber halt nicht mehr mit maximaler Produktivbetrieb-Hot-Performance.\par
\par
Also w\'e4r es so:\par
\par
1.) Es gibt einen First-Class Master Node.\par
- Der ist die "komplette Anwendung", sowohl was Logik als auch Datenmodell angeht.\par
- Der ist der einzige, der Anfragen f\'fcr \'c4nderungen an den Daten annimmt und neue Entities erzeugt.\par
- Der wird auf beliebig viele First-Class Slave Nodes repliziert, die ihm lesende Arbeit anehmen, damit er sich auf die Verarbeitung von \'c4nderungen konzentrieren kann.\par
- Der lagert aber alle Nicht-First-Class Entities an andere Nodes aus. Diese m\'fcssen in autarken Subgraphen organisiert sein.\par
\par
2.) Es gibt N Second-Class Master Nodes.\par
- Die bekommen Subgraphen vom First-Class Master Node zugeschickt, die sie einfach nur dumm persistieren und auf Anfrage wieder rausgeben. Ohne Anwendungslogik. Ohne Validierung oder sowas.\par
- Deren eigenes Datenmodell sieht wahrscheinlich einfach so aus, dass ihr Root eine Map ist und jeder Subgraph, der rein kommt, wird mit seinem Root dort reingeh\'e4ngt und gespeichert.\par
- Auch die werden auf beliebig viele Read-Only Nodes repliziert, um die Lese-Last zu verteilen.\par
\par
3.) Es gibt irgendeinen Drittanbieter remote binary Storage Service.\par
- In den werden "gro\'dfe" ValueType Datenobjekte reingesteckt, die unter einem Identifier wieder abgerufen werden k\'f6nnen. Idealerweise gleich nur vom Client (Browser/App) selbst und nicht vom First-Class Master Node.\par
- Wie die replizieren o.\'c4. ist deren Sache, das kann uns egal sein. Wir speichern nur URIs und l\'f6sen die bei Bedarf wieder auf.\par
\par
\par
Der einzig knifflige Punkt ist damit:\par
Wie designt man in einer spezifischen Anwendung "sch\'f6n" autarke  Subgraphen, die man problemlos an so einen Second-Class Node auslagern kann?\par
Das sollte eine zumutbare Designentscheidung sein, genauso wie die Entscheidung von g\'fcnstigen Punkten f\'fcr Lazy References.\par
\par
\par
Naja, und:\par
- Wie verhindert man, dass ein Read-Only Slave Node seinen Entity Graph \'e4ndert?\par
- Wie werden Writes an den Master Node geschickt?\par
\par
Die Slaves Nodes k\'f6nnten auf jeden Fall schon mal Validierung auf fachlicher Ebene machen und damit dem Master Arbeit abnehmen.\par
\par
Oder k\'f6nnten die Read-Only Nodes vielleicht zwar schon \'e4ndern, aber sie speichern nicht, sondern ihr "store" schickt den Chunk an den Master und der validiert und speichert.\par
Aber was soll er validieren?\par
Fachliche Logik, klar. Daf\'fcr ist der Code da.\par
Aber wie soll er entscheiden, ob der Read-Only node wirklich die aktuellen Daten hatte?\par
Dann m\'fcsste eine Versions-ID mit rumgereicht werden.\par
Ins Entity einbauen ist bl\'f6d. Aber es k\'f6nnte eine HashTable sein, deren Entries mit jedem Chunk mitgeschickt werden.\par
Ein Entry w\'e4re eine ziemlich kleine Sache: 16 Bytes. 8 f\'fcr die OID, 8 f\'fcr die versions ID.\par
Der Slave bekommt die aktuellen IDs automatisch \'fcber das replizieren.\par
Beim zur\'fcckschicken an den Master schickt er einen "Auszug" der Versionstable mit, in der f\'fcr jedes geschickte Entity die 16 Bytes dabei sind.\par
Dann kann der Master checken: Hatte der Slave \'fcberhaupt die aktuelle persistente Version je Entity?\par
Wenn ja und wenn die fachliche Logik kein Problem meldet, kann der Master den Chunk direkt produktiv verwenden, d.h. direkt auf die Platte schreiben.\par
Alle anderen Slaves bekommen die Updates dann \'fcber die Replizierung.\par
\par
Wenn die Version nicht passt, bekommt der slave eine Exception zur\'fcck, die entweder kompensiert werden kann, oder dem User als "Die Daten haben sich zwischenzeitlich ge\'e4ndert, bitte neu laden" mitgeteilt wird.\par
\par
Dann w\'e4re die Anwendung verteilt auf replizierte Nodes.\par
Jeder node k\'f6nnte die volle Anwendungslogik ausf\'fchren auf potenziell den vollen Daten, aber schreiben w\'fcrde nur ein Node.\par
\par
Problem daran k\'f6nnte sein:\par
Wenn's dumm l\'e4uft, bekommt ein Slave gerade solche Anfragen zugewiesen, die f\'fcr die Bearbeitung die halbe Datenbank im RAM erfordern. Anfrage 1 braucht die ersten 10% Daten, #2 die zweiten 10%, usw.\par
Da kann ein Slave nach nur wenigen Anfragen schon seinen ganzen RAM voll haben und abschmieren.\par
\par
Es m\'fcsste halt dann die Anwendung so gebaut sein, dass nicht zu viele ge\'e4nderte Instanzen und nicht zu viele Konflikte entstehen.\par
Beispiel:\par
Ein User kann meistens nur in "seinem Bereich" viel herum\'e4ndern (sein Profil, seine Bestellungen, seine Mails, usw.) und Interaktion mit anderen l\'e4uft meistens nur darauf hinaus, Subgraphen neu zu erzeugen (z.B. eine Message Instanz mit Timestamp und Text) und irgendwo anders synchronisiert "einzuh\'e4ngen". Die neu erzeuten Message Instanzen k\'f6nnte ein Slave Node vorbereiten, nur das "einh\'e4ngen" kann einen Konflikt erzeugen. Bei einer Versionsexception kann der Slave sich die aktuelle Version holen und das einh\'e4ngen wieder versuchen. Wobei f\'fcr soetwas eine explizite Registrierlogik sinnvoller w\'e4re. Alle slaves senden ihre einzuh\'e4ngenden Subgraphen an eine registrierungsfunktion des masters und der arbeitet die entstehende Queue sequenziell ab.\par
\par
Hei\'dft: wie implementiert man eine enqueue-Logik, wenn alle Teilnehmer eigentlich Klone desselben Systems sind und nur Daten hin und her schicken, nicht Services aufrufen?\par
Dann k\'f6nnten die Slaves aber nicht einfach generisch ein store() aufrufen, als w\'e4ren sie der Master, an den aber eigentlich nur weitergeleitet wird. Sondern es m\'fcsste so command-Instanzen verschickt werden, die der Master dann auf Methoden mappen k\'f6nnte.\par
Aber wie kriegt man das generisch in den Store rein?\par
K\'f6nnte der Master generisch anhand des Typs der Rootinstanz des gesendeten Subgraphen entscheiden? Sowas wie GuestBookEntry. Und je nachdem, ob dieser Subgraph eine neue oder schon bekannte ID hat, w\'e4re es ein "handleNew" (z.B. enqueue) oder ein "handleExisting" (z.B. edit) ?\par
K\'f6nnte sowas in den Persistence layer reingewoben werden? Callbacks im Builder und im TypeHandler?\par
Also wenn der Master von einem Slave einen neuen GuestBookEntry geschickt bekommt, dann wei\'df er, was er damit machen muss.\par
Das ist Detal aber eklig, denn der GuestBookEntry m\'fcsste dann alle n\'f6tigen Informationen enthalten (welches Ziel-Guestbook, usw.). Das w\'e4ren R\'fcckrefrenzen, die in der Datenbank aber sinnlose Redundanzen darstellen w\'fcrden.\par
Also m\'fcsste man doch wieder Logik-Level Command-Instanzen drum rum wrappen, die zum persistieren aber weggelassen werden.\par
Dann w\'e4re aber nichts mehr mit node-typ-agnostisch einfach store aufrufen.\par
\par
Oder vielleicht w\'fcrde ja folgendes gehen:\par
Anwendungsdesignm\'e4\'dfig wird JEDE Aktion, die eine Daten\'e4nderung bewirken kann, in ein Command-Objekt gepackt.\par
Das wird an eine "handleCommand" methode \'fcbergeben.\par
Beim Slave hei\'dft "handleCommand": Command-Objekt serialisieren und an den Master schicken.\par
Beim Master leitet der Netzwerk-Eventthread bekommene Command-Objekte wieder an sein "handleCommand" weiter.\par
Das macht dann: je nach Kommando entsprechende Methode aufrufen.\par
Dabei f\'e4llt das Kommando-Objekt selbst weg.\par
Wenn die Methode gr\'fcnes Licht gibt, wird ein Store aufgerufen.\par
\par
Beispiel:\par
\par
Benutzer mit einer Session auf Slave #37 will sein Passwort \'e4ndern.\par
Die User-Instanz steht fest (von der Anwendungslogik ermittelt). Das neue Passwort liegt vor.\par
Es wird ein "user.changePassword(newPassword)" aufgerufen.\par
Die Anwendungslogik darin validiert, ob das Passwort \'fcberhaupt zul\'e4ssig ist.\par
Ist es, also weiter:\par
Es wird ein "CommandChangePassword" erzeugt und an "handleCommand" \'fcbergeben.\par
Beim Slave hei\'dft das serialisieren, das Command kommt beim Master an, der Master hat daf\'fcr die methode User#changePassword gemappt und ruft die mit den n\'f6tigen Daten auf.\par
Darin gibt es wieder Validierung.\par
Am Ende wieder "CommandChangePassword", das wieder an "handleCommand" \'fcbergeben wird. Aber beim Master f\'fchrt das nun zu einem Store.\par
Fertig.\par
\par
Falls es keinen Slave gibt, sondern der Benutzer mit seiner Session direkt auf dem Master arbeitet, funktioniert das genauso, nur ohne Netzwerk Umweg:\par
- "user.changePassword(newPassword)"  Aufruf\par
- validierung\par
- CommandChangePassword erzeugung\par
- \'fcbergeben an handleCommand\par
- handleCommand speichert direkt ab.\par
\par
Vielleicht sind die Command-Dinger auf der Master Seite sogar noch gut f\'fcr Logging oder sowas.\par
\par
Interessant ist jetzt noch die Frage:\line Muss der Master \'fcberhaupt nochmal Logik ausf\'fchren?\par
Reicht es nicht, darauf zu vertrauen, dass der Slave schon die Logik ausgef\'fchrt hat?\par
Es muss ja eigentlich nur validiert werden, ob der Slave von allen Daten die aktuelle Version hatte.\par
Vielleicht w\'e4re der Master dann nichts anderes als ein dummer Persistierungsnode, der Versionsnummern abgleicht und exceptions wirft.\par
Aber Vorsicht: ne, das geht nicht. Denn dann gibt es ja keine zentrale Stelle mehr, wo Logik ausgef\'fchrt wird. Das Passwort-Beispiel ist daf\'fcr zu primitiv. Besser w\'e4re so ein GuestBookEntry Beispiel:\par
Der Slave pr\'fcft grunds\'e4tzlich, ob der User einen Guestbook Entry machen darf. Er wei\'df aber nicht, an welcher Stelle im Guestbook der Entry letztendlich eingereiht werden wird. Das wei\'df allein der Master, der den aktuellen Stand der Daten hat.\par
Wenn grunds\'e4tzlich ja, erzeugt er ein Command daf\'fcr und schickt das an den Master.\par
Der Master nudelt das wie oben besprochen durch, aber in seiner add Entry Logik kann er mit einem vor\'fcbergehenden Lock nun die genaue Position DIESES Entries im Guestbook feststellen. So reiht er das ein, erzeugt ein Command, f\'fchrt das aus, persistiert das und der asynchron reingeschickte entry ist auf synchronisiert sichere Weise enqueut worden.\par
\par
So in die Richtung muss das funktionieren:\par
- Derselbe Anwendung l\'e4uft parallel auf mehreren Nodes\par
- Ein Node ist der Master, alle anderen nur Slaves\par
- Die Anwendungslogik arbeitet mit Schnittstellen aus Command-Instanzen\par
- Die Slaves handeln die Command Instanzen einfach nur so, dass sie sie an den Master schicken.\par
- Der Master handelt die Command Instanzen so, dass die anwendungslogik zentral und synchronisiert entscheidet und ggf. speichert.\par
- Gibt es nur einen Master ohne Slaves, dann ist das halt einfach eine normale Anwendung, in der ein kleiner Umweg mit Command-Instanzen gegangen wird, die aber f\'fcr Logging o.\'c4. sogar ganz vorteilhaft sein k\'f6nnen.\par
- Irgendwo beim Empfangen auf dem Master m\'fcssen dann noch Versionsnummern von bereits existierenden Entities gecheckt werden. Neue Entities sind immer erlaubt, die validiert dann nur noch die Anwendungslogik.\par
\par
\par
Es m\'fcsste folgendes Interface geben:\par
\par
public interface CommandHandler\par
\{\par
\tab public void handleCommand(CommandCreateGuestBookEntry);\par
\par
\tab // ... usw. ganz viele solcher handling Methoden\par
\}\par
\par
Das Command Basisinterface hat folgende Methode:\par
\par
public void handleBy(CommandHandler);\par
\par
Eine Command Implementierung kann dann die passende spezifische Logik f\'fcr sich selbst raussuchen.\par
Damit vermeidet man einen teuren Handler-Lookup.\par
\par
Die allgemeine "handleCommand" Methode bei einem Slave serialisiert das Command-Objekt und alle daranh\'e4ngenden Instanzen und schickt sie zum Master.\par
Die allgemeine "handleCommand" Methode beim Master sieht so aus:\par
\par
public void handleCommand(Command command)\par
\{\par
\tab command.handleBy(this.commandHandler);\par
\}\par
\par
Der CommandHandler kann das treiben, was immer er denkt. Validierung, Logging, Persistierung.\par
Selbst, wenn der Master allein ist, ist das eine sinnvolle Architektur, weil man eine zentrale Stelle hat, in der man loggen usw. machen kann.\par
Dieses Konzept gilt nur f\'fcr Aktionen, die Daten ver\'e4ndern, d.h. die letztendlich einen store aufrufen. Am besten sollte nur der CommandHandler intern \'fcberhaupt Zugriff auf eine StorageConnection haben. D.h. man MUSS \'fcber ein Command Objekt gehen, wenn man irgendwas gespeichert haben will.\par
\par
Rein lesende Logik braucht keine Commands.\par
\par
Wahrscheinlich sollte "Command" besser umbenannt werden zu "Request". Denn ein Kommando MUSS ausgef\'fchrt werden. Worum es hier ja aber geht, sind Anfragen, etwas zu tun, die auch abgelehnt werden k\'f6nnen.\par
}
 