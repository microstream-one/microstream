{\rtf1\ansi\ansicpg1252\deff0\nouicompat\deflang1031{\fonttbl{\f0\fnil\fcharset0 Calibri;}}
{\*\generator Riched20 10.0.17134}\viewkind4\uc1 
\pard\sl276\slmult1\f0\fs28\lang7 Jetstream Clustering Brainstorming\par
2018-10-16\par
\par
\par
"Lowlevel"\par
1 Master - liest und schreibt Daten\par
N Slaves - bekommen Kopien vom Master repliziert, nehmen ihm lese-Last ab, agieren als Failover Master\par
\par
\par
"Microservice"\par
Ist ein Teil der Anwendung (Beispiel "Umsatzserver").\par
Jede Microservice ist f\'fcr eine disjunkte Menge fachlicher Daten exklusiv zust\'e4ndig.\par
F\'fchrt Validierung von zu speichernden Daten auf fachlicher Ebene durch. Alles "darunter" kann ruhigen gewissens ungepr\'fcft "dumm" speichern\par
Kann einen "Lowlevel" Teil direkt embedded haben oder kann N Lowlevel Teile per Netzwerk ansprechen.\par
Jeder "Lowlevel" Teil wird nach OID Modulo segmentiert.\par
\par
"Masterservice"\par
Kennt alle Microservices, d.h. er "ist" in Summe "Die Anwendung".\par
\par
Problem:\par
Damit neben die LowLevel Slaves keine Leselast mehr ab.\par
Es muss eher so sein, dass ein Microservice entweder der Master ist oder es N Slave-Microservices gibt und wenn was gespeichert werden soll, muss immer an den Master-Microservice gleitet werden.\par
D.h. die Anwendung muss auch fachlich zwei Schichten unterteilt werden:\par
In...\par
- "Data Services", die liefern fachliche Daten, validieren gew\'fcnschte \'c4nderungsanfragen und schreiben g\'fcltige \'c4nderungen\par
- "Work Services", die sind selbst transient, also ohne gespeicherte fachliche Daten, halten Usersessions, gecachte Bilder und so Grusch und reden je nach Anforderung mit einem beliebigen Data-Slave (lesend) oder mit dem Data-Master (schreiben).\par
\par
Kommunikation zwischen Data und Work Nodes erfolgt \'fcber Jetstream-serialisierte Objektgraphen.\par
\par
Dar\'fcber muss es einen Load Balance geben, der Anfragen von au\'dfen auf die Work Nodes verteilt und Anfragen von Work Nodes auf die Data-Slaves verteilt.\par
\par
Die Anzahl der Work Nodes kann beliebig skaliert werden. Sie haben gar keinen persistenten State.\par
Die Anzahl der Data-Slaves kann beliebig skaliert werden. Sie bekommen komplette Kopien ihres Masters und sind read-only.\par
Die Anzahl der Data-Master ist fest und muss entsprechend der Architektur der Anwendung (Datenmodell, Logik) designt werden.\par
Ein Data-Master ist der Teil, der exklusive fachliche Validierung von zusammengeh\'f6renden Teilen ausf\'fchren muss. Soetwas kann man nicht generisch clustern.\par
\par
Man kann einen Data-Master aber oft nachtr\'e4glich auf fachlicher Ebene clustern.\par
Beispiel:\par
Umsatznode.\par
Anfangs 1 monolithischer Node mit allen Ums\'e4tzen.\par
Sp\'e4ter 1 Node f\'fcr die Ums\'e4tze des aktuellen Jahrs, 1 node f\'fcr die letzten beiden Jahre und 1 Node f\'fcr alle vorherigen.\par
Der Anwendungscode ist dann auf jedem Master identisch, nur mit anderen Segmenten der Daten, f\'fcr die er zust\'e4ndig ist.\par
\par
Um alle Nodes, also Work, Data und Master untereinander, kann man mit Lazy<> Referenzen abbilden.\par
Die h\'e4lt erst mal nur eine OID und kennt den zust\'e4ndigen Node. Bei einem .get() wird dieser Node kontaktiert und alle Daten bis zur n\'e4chsten Lazy<> Referenz \'fcbertragen.\par
\par
Knifflig: F\'fcr jede geladene Instanz muss registriert werden, wer ihr zust\'e4ndiger Data-Master Node ist. Bei einem store() Aufruf f\'fcr einen Entitygraph muss je nach Instanz nach den zust\'e4ndigen Data-Mastern unterschieden werden.\par
Der commit() Aufruf des Storers schickt die serialisierten Entities an ihre jeweiligen Data Master. Diese Validieren und melden erfolg oder miserfolg zur\'fcck. Wenn alle beteiligten Data-Master Erfolg gemeldet haben, persistieren alle Data-Master die validierten Entities. \par
\par
Problem: Was ist, wenn alle erfolg gemeldet haben, aber ein oder mehr Data-Master beim Speichern auf ein Problem gesto\'dfen sind?\par
Dann wurden manche der Entities gespeichert und manche nicht.\par
Auf Checks und Rollbacks kann man sich nicht verlassen, weil ja wiederum manche der Nodes, die rollbacken m\'fcssten, auf ein Problem sto\'dfen k\'f6nnten.\par
Muss es stattdessen eine Art Transaction-ID geben, die \'fcber den gesamten Cluster gilt und bei der Initialisierung aller Nodes f\'fcr den letzten Store auf Einheitlichkeit grepr\'fcft wird?\par
K\'f6nnte klappen.\par
\par
Alternative:\par
Ein Store darf - gepr\'fcft \'fcber die per-instanz-registry nur entities eines Data-Masters enthalten.\par
Hei\'dft: wenn man Kunden und Ums\'e4tze speichern will, muss man einen storer f\'fcr die Kunden und einen storer f\'fcr die Ums\'e4tze machen.\par
Aber was ist dann, wenn ein Umsatz seinen Kunden kennt und der Kunde seinen Umsatz?\par
Es m\'fcsste dann im Design f\'fcr jeden \'dcbergang im Datenmodell von einem Master-Bereich zum anderen ein Lazy vorgesehen sein.\par
\par
\par
\par
Ideal w\'e4re folgendes:\par
- Es gibt die Anwendung Code-m\'e4\'dfig nur einmal als Monolith.\par
- Jeder Microservice startet die komplette Anwendung, ist aber nur f\'fcr einen Teil der Daten zust\'e4ndig.\par
- Irgendwie muss sichergestellt sein, dass die Entities, die ein Microservice reingeschickt bekommt, nur die Logik triggern, f\'fcr die der Microservice mit seiner Zust\'e4ndigkeit in den Daten ausf\'fchren kann.\par
\par
Bl\'f6d daran aber:\par
Microservices sollen unter anderem ja auch die Code Komplexit\'e4t reduzieren:\par
Der Umsatzserver muss nur den Code zum Ums\'e4tze handeln haben.\par
\par
\par
Und noch ein Problem: wenn neue Instanzen zum ersten mal gespeichert werden, woher wei\'df ein Microservice, dass er wirklich daf\'fcr fachlich segment-m\'e4\'dfig zust\'e4ndig ist?\par
\par
\par
Und ein ziemlich entscheidendes Problem, geradezu ein Show-stopper:\par
Wenn sich irgendwelche Nodes von irgendwelchen anderen Nodes Daten laden und die "anderen Nodes" dann neue Versionen dieser Datens\'e4tze bekommen und abspeichern, wie bekommen die Nodes mit den geladenen Daten das dann mit?\par
Das ist genau dasselbe Probleme wie bei einer RDBMS+Java Anwendung, wo eigentlich der Java Prozess der "Chef" sein sollte, \'fcber den zwecks Businesslogik Validierung usw. alle Daten\'e4nderungen laufen sollen, aber dann \'e4ndert einer an der Seite vorbei direkt in der Datenbank rum. Der Java Prozess bekommt das nicht mit, also sind, schwups, mal eben, seine geladenen Daten inkonsistent. Also m\'fcsste er bei jeder Operationen pr\'fcfen, ob neuere Versionen da sind, oder die Caches ganz abschalten (Performance = tot) oder wegen jeder kleinen Scheiss\'e4nderung cache invalidation signale bekommen.\par
Das Kernproblem ist: Es darf nicht mehrere Wege geben, an einer Menge an Daten herumzu\'e4ndern. F\'fcr eine Menge an (fachlich sinnvoll zusammengeh\'f6render) Daten darf immer nur genau ein Prozess der sein, der \'e4ndert und durch den fliesen alle \'c4nderungen durch, damit der immer auf dem neuesten Stand ist.\par
\par
Darum vielleicht folgender, radikaler Ansatz:\par
- Es gibt nach wie vor 1 Data-Master und N Data-Slaves, die sich Kopien der Daten ziehen bzw. bekommen.\par
- Die Slaves sind aber gleichzeitig die Worker Nodes.\par
- F\'fcr \'c4nderungen leitet jeder Slave die Daten an den Data-Master, der validiert, persistiert die \'c4nderung und sendet dann die Updates wieder an jeden Data-Slave.\par
\par
Aber was ist jetzt mit Skalierung? Zumindest f\'fcrs Schreiben ist das ja dann ein monolithisches System.\par
Antwort:\par
1.)\par
Zun\'e4chst mal fliegt alles, was "triviale" Bin\'e4rdaten (Bilder, T\'f6ne, Videos, gro\'dfe Texte) sind, aus dem System ganz raus. Solche Daten kann sich ein Client per URL von irgendeinem Image Cache Server nachladen.\par
\par
2.)\par
Dann k\'f6nnen alle fachlichen Daten, die sich nur "selten" \'e4ndern ("selten" hei\'dft "nur" alle paar Minuten oder so), in mehrere andere Data-Master ausgelagert werden. JEDES Update von denen bewirkt dann ein Signal an die Worker Nodes, dass sie ihren Cache f\'fcr den betreffenden Data-Master invalidieren/leeren m\'fcssen.\par
\par
3.)\par
Was dann \'fcbrig bleibt, ist ein Kern-Node mit nur noch den sich schnell \'e4ndernden (Minuten oder weniger) Entities, die f\'fcr sich ziemliche winzige Datens\'e4tze sind (~100 Byte).\par
Wenn die mal "\'e4lter" werden und sich nicht mehr oder nur noch wenig \'e4ndern, dann kann man die in so einen "selten" Node verschieben.\par
Der Kern-Node hat also immer nur die "hei\'dfen" und "kleinen" Daten.\par
Und der muss dann nat\'fcrlich eine der Anwendung angemessene Gr\'f6\'dfe haben. 16 Core, 512 GB RAM oder noch mehr.\par
Bis man so ein Riesensystem mit relativ wenigen und kleinen Daten mal an seine Grenzen bringt, dauert das.\par
\par
Je nach Datenmodell kann man von den Kern-Nodes auch durchaus mehrere machen, aber das muss dann anwendungslogisch sauber designt sein. Z.B., keine Ahnung, Kundendaten, die sich wirklich sch\'f6b sauber disjunkt von A bis Z durchsegmentieren lassen, wodurch ein Client f\'fcr die Bearbeitung eines Kunden immer nur auf einem Kern-Node arbeiten muss. Es kann aber gut sein, dass das bei den meisten Anwendungen nicht so aufteilbar ist.\par
\par
Und man kann nat\'fcrlich immer noch ma\'dfgeschneiderte Speziall\'f6sungen machen. Wenn etwa permanent millionen Messdatens\'e4tze reinsprudeln, dann speichert man die nicht unbedingt jeden als einzelnes Entity, sondern man speichert die in Tauserbl\'f6cken direkt als Binary Blob ab.\par
\par
Dieses Konzept hat immer Clustering f\'fcr Ausfallsicherheit:\par
Die Data-Slaves bekommen permanent live die neuesten Updates und stehen jederzeit bereit, als neuer Master zu \'fcbernehmen.\par
\par
Es hat auch immer noch Clustering f\'fcr Lastenverteilung:\par
Die Data-Slaves nehmen dem Data-Master die gesamte Lese-Arbeit ab. Der Data-Master muss nur noch validieren und schreiben. Und ich wette, das geht genauso schnell, wie wenn ein chaotischer Haufen an permanent race-condition-konkurrierenden Nodes sich st\'e4ndig untereinander synchen m\'fcssen.\par
\par
Und es hat auch immer noch Clustering f\'fcr Skalierung:\par
Alles, was "gro\'df" ist, wird in andere Systeme ausgelagert. Der "Kern" Data-Master k\'fcmmert sich nur um "kleine" und "hei\'dfe" Daten.\par
\par
\par
Eine nette kleine Testfrage f\'fcr Konzepte ist immer: "K\'f6nnte man damit Facebook nachbauen?"\par
Ja, k\'f6nnte man, denn:\par
Alle Bilder, Videos, evtl. sogar die Strings der Nachrichten und die Werbungen sowieso, kommen von anderen Servern.\par
Die >1 Milliard Benutzer kann man ziemlich leicht auf mehrere Data-Master verteilen. So ein Benutzer ist eine recht abgeschlossene Sache. Profildaten, Kontakte, Nachrichtendaten. Das kann sich das Browser-Frontend je Benutzer von verschiedensten Data-Master Nodes holen, ohne dass es jemals einen Konflikt mit der Aktualit\'e4t gibt. Die k\'f6nnten dann jeweils gleich in den entsprechenden Regionen stehen. Ihre Data-Slaves stehen dann nat\'fcrlich verteilt \'fcber die ganze Welt, aber wenn f\'fcr einen bestimmten Benutzer Daten validiert&geschrieben werden, landet das immer bei dem EINEN zust\'e4ndigen Data-Master.\par
\par
\par
\par
2018-10-18\par
\par
Ich schreib mal kurz eine Idee zu einer klareren "Datensatzklassifizierung" von oben auf.\par
\par
\par
\b Daten vierter Klasse\par
\b0 Gro\'dfe ValueType Objekte, die mehr den Charakter von "Nutzlast" als von Datenmodell-Entities haben.\par
Beispiele: Bilder, Audio, Video, Dokumente, gro\'dfe Texte (> 1000 Zeichen).\par
Werden in einem ausgelagerten System gespeichert und \'fcber irgendeinen eindeutigen Identifier (UUID oder so) referenziert. Irgendein Cloud Storage Ding oder so.\par
Anwendungsserver m\'fchen sich mit solchen Daten selbst normalerweise gar nicht ab, sondern schicken Clients (Browser, App, etc.) nur den Identifier und die sollen sich das Ding bei Bedarf selbst runterladen, statt dass sie es m\'fchsam vom Anwendungsserver geliefert bekommen. Das Cloud Ding kann dann in sich geclustert, repliziert, wei\'df der Geier sein, das braucht den Anwendungsentwickler nicht k\'fcmmern.\par
Diese Objekte sind immutable, d.h. sie ver\'e4ndern sich niemals. Sollte es mal eine \'c4nderung an den Bytes geben, f\'fchrt das zu einer neuen Version mit einem neuen Identifier. Aber ein Identifier steht immer f\'fcr eine bestimmte "Kombination an Bytes". Unver\'e4nderlich f\'fcr alle Ewigkeit.\par
Wahrscheinlich muss man von Zeit zu Zeit "aufger\'e4umt" werden: Alle live referenzierten Identifier sammeln und dem Storage Ding sagen "L\'f6sch von mir alles, au\'dfer folgende". Oder umgkehrt: Eine Liste aller verf\'fcgbaren geben lassen und alle nicht mehr live referenzierten l\'f6schen. Das sind technische Details.\par
\par
Durch die Auslagerung dieser Daten nimmt man schon mal gigantisch viel Volumen aus der Datenbank raus. \'dcbrig bleiben nur noch "echte Datens\'e4tze", die typischerweise um die 50-100 Byte gro\'df sind (Collections etwas gr\'f6\'dfer).\par
\par
\b Daten dritter Klasse\par
\b0 Immutable Entities. Da die sich nie \'e4ndern, kann man sie beruhigt auf einen anderen "Master" Node auslagern und von dort bei Bedarf immer wieder laden. Es wird bei denen nie Probleme mit Schreib-Konflikten oder veralteten Caches geben.\par
Beispiele: \par
Schwer zu sagen, weil ziemlich anwendungsspezifisch. Vielleicht soetwas wie Messdaten: Die werden einmal erhoben und \'e4ndern sich dann nie wieder. Millionen von Datens\'e4tzen, die zwar schon "Entities" sind, aber die in ihrer Natur so leicht zu handeln sind, dass es Verschwendung w\'e4re, ihnen eine erstklassige Behandlung zukommen zu lassen. Nat\'fcrlich macht es keinen Sinn, jedes kleine immutable Einzelinstanz in ein andere System zu verschieben. Etwa einen Integer mit Wert 5. Sondern hier sind gro\'dfe Mengen an zusammengeh\'f6renden immutable Entities gemeint.\par
Wie das so ganz genau ist mit dem Immutable ist immer so eine Sache: Irgendwann m\'fcssen die Entities ja mal erzeugt und registriert, z.B. in eine Collection gesteckt werden. Also so "ganz immutable" kann es nicht sein.\par
Hier ist der \'dcbergang zur zweiten Klasse flie\'dfend, eventuell ist beides sogar einfacher als dasselbe abbildbar.\par
\par
Durch die Auslagerung dieser Entities l\'e4sst sich die Gesamtzahl der erster Klasse zu managenden Entities dramatisch reduzieren, ohne sich dabei architektonische Probleme (Konflikte) einzuhandeln.\par
\par
\par
\b Daten zweiter Klasse\par
\b0 Gro\'dfe Mengen an Entities, die sich nur selten \'e4ndern. "Selten" hei\'dft in der Welt von Computern: Nur alle Stunde oder noch seltener.\par
Diese werden auf einen eigenen "Master" Node ausgelagert.\par
Die Idee dahinter ist: Das Zeitintervall soll gro\'df genug sein, dass es sich rentiert, dass der zust\'e4ndige Node an die Anwendungsnodes bei jeder \'c4nderung signalisieren kann: "Bei mir hat sich was ge\'e4ndert, du musst alle deine Entities, die du von mir gecacht hast, invalidaten/clearen und bei Bedarf neu laden."\par
Wenn die Intervalle zu kurz w\'e4ren (~Sekunden), w\'fcrden die Anwendungsserver st\'e4ndig nur noch caches leeren und neu aufbauen, darum "seltene" \'c4nderungen. Als Optimierung f\'fcr kurze Intervalle nicht "alles" clearen, sondern nur ge\'e4nderte Instanzen, w\'fcrde schon wieder zu viel Verwaltungsoverhead erzeugen: Rechenaufwand, um alle ge\'e4nderten IDs zu sammeln, IDs \'fcbertragen, in den Anwendungsnodes die ID Liste durchlaufen, aufl\'f6sen, die entsprechenden Instanzen invalidaten. \par
Was ist dann mit Instanzen von und auf diese Instanzen zu eigentlich unver\'e4nderten Instanzen? Alles schwierig. Darum simpler Ansatz: ALLES, was von dem betreffenden Second-Class Masternode gecacht ist, einfach clearen.\par
Auch hier w\'e4ren Beispiele stark anwendungsabh\'e4ngig. \par
Viele Anwendungen m\'f6gen solche Art von Daten \'fcberhaupt nicht haben. Viele andere haben fast nur solche. Vielleicht soetwas wie Artikeldaten. Eine Million Artikel und all ihre Entities drum rum. Da \'e4ndert sich schon mal immer wieder was dran. Vielleicht mehrfach t\'e4glich, wenn Sachbearbeiter \'c4nderungen vornehmen. Vielleicht werden nur einmal t\'e4glich batchm\'e4\'dfig alle Artikel\'e4nderungen auf einmal im System "ver\'f6ffentlicht".\par
Oder ein Beispiel aus der pers\'f6nlichen Entwicklererfahrung: Bonusberechnungen. Es gibt Vorg\'e4nge, die werden vom Sachbearbeiter ausgef\'fchrt und die berechnen dann aus Ums\'e4tzen und Stammdaten, wie viel Kohle jedem Mitglied ausgezahlt wird. Jeder Vorgang erzeugt schon mal so 100.000 Entities oder noch mehr. Aber diese Vorg\'e4nge laufen vielleicht ... naja ... 10 mal im Monat. Selbst, wenn ein Sachbearbeiter herumprobiert, ausbessert, Daten nachtr\'e4gt, usw. und daf\'fcr immer wieder Vorgang ausf\'fchren und wieder resetten muss, dann passiert das vielleicht ab und zu 10 mal an einem Tag. Bei solchen Intervallen ist es noch problemlos m\'f6glich, in der Anwendungslogik einfach ALLES gecachte wegzuwerfen und bei Bedarf neu zu laden.\par
\par
Durch die Auslagerung dieser Entities l\'e4sst sich die Gesamtzahl der erster Klasse zu managenden Entities dramatisch reduzieren, ohne sich dabei architektonische Probleme (Konflikte) einzuhandeln.\par
\par
\b Daten erster Klasse\par
\b0\'dcbrig bleiben dann nur noch Entities "Erster Klasse". Kleine Entities, die sich potenziell sehr oft \'e4ndern und wo sich unter Umst\'e4nden sogar mal nur ein einzelnes Entity \'e4ndert.\par
Klassisches Beispiel: Kunde \'e4ndert sein Profil. Anschrift, Einstellungen, irgendsowas.\par
Nachdem alle Daten niedrigerer (einfacherer) Klasse in andere Systeme ausgelagert wurden, bleibt hier nicht mehr viel \'fcbrig.\par
Die kann ein einzelner Anwendungsserver, sozusagen der "First Class Data-Master", selbst direkt handeln.\par
Das sind nat\'fcrlich immer noch Millionen Entities. Wahrscheinlich auch Milliarden. Aber das ist mit Jetstream kein Problem. Wirklich nicht.\par
Beispiel Bonusanwendung: Besteht aus ~20 Millionen Entities und sogar unsere 0815 B\'fcro-PCs als "Serverhardware" brauchen nur etwa 15 Sekunden, um die gesamte Datenbank zu initialisieren und den Server zu starten.\par
Wenn man eine einem gesch\'e4ftlichen Unternehmen angemessene Serverhardware einsetzt (sowas wie 8 Core, 128 GB RAM oder auch 32 Core, 512 GB RAM, das ist heutzutage alles kein Drama mehr), dann dann der mit \'e4hnlicher Geschwindigkeit auch 1 Milliarde solcher "First Class" Entities managen.\par
\par
Und wenn das immer noch nicht reicht, dann kann man seine Anwendung auch gezielt so designen, dass man mehrere solcher "First Class Data-Master" hat. Das sind halt dann disjunkte "Welten", die auf irgendeine g\'fcnstige Art zusammenarbeiten. Das wichtige ist dabei nur, zu verhindern, dann Daten des anderen rumzu\'e4ndern oder veraltete Versionen im Cache zu haben.\par
Ich denke da z.B. an Facebook: Selbst die dickste Einzelhardware ever kann unm\'f6glich alle >2 Milliarden Benutzer und alle ihre dranh\'e4ngenden "First-Class" Entities managen.\par
Das muss unweigerlich irgendwie segmentiert, geshardet werden. Aber das ist ziemlich simpel: man baut sich 1000 First-Class Data-Master Anwendungsnodes und jeder von denen ist "nur" f\'fcr 2 Millionen User zust\'e4ndig.\par
Oder 10.000 Nodes mit je 200.000 User. Man kann auch 1 Million kleine Nodes mit je 2000 Usern drauf machen. Spielt keine Rolle. Es wird hier nie Konflikte geben, weil die Daten in sich auf fachlicher Ebene geclustert sind. Jeder User ist f\'fcr sich eine relativ abgeschlossene Einheit. Es gibt nat\'fcrlich Kontakte und Nachrichten untereinander, aber das kann man schon irgendwie sinnvoll managen. Entweder die Nachricht liegt immer auf dem Node des Senders oder, warum nicht: Es gibt einen extra Nachrichten Node (oder 1000), der (die) nichts anderes macht, als Millionen Nachrichten zwischen den Usern zu handeln.\par
Also ich will damit nur sagen: Die Aussage "Es gibt nur einen First-Class Node" hei\'dft nicht, dass es auf Gedeih und Verderb f\'fcr egal wie gro\'dfe Systeme immer nur einen zentralen Node geben kann. Es soll nur hei\'dfen, dass ein First-Class Node die gr\'f6\'dfte Einheit ist, die automatisch verwaltet werden kann. Wenn man mehr braucht, muss man seine Anwendung entsprechend strukturieren und untereinander kommunizieren lassen, dann geht das auch.\par
\par
Das Konzept ist kurz gesagt: Daten in verschiedene Klassen einteilen und je nachdem, wie viel oder wenig unkompliziert sie zu verwalten sind, kann man die Daten mehr oder weniger weit vom zentralen Anwendungsnode weg auslagern.\par
Dazu noch so read-only Data-Slaves, die den Master entlasten und failover-absichern, dann hat selbst bei riesigen Systemen ein einzelner (durchaus leistungsstarker) Master-Node genug Luft, um Anwendungslogik mit gigantische Datenmengen auszuf\'fchren und gew\'fcnschte \'c4nderungen business-logic-m\'e4\'dfig vor dem Schreiben zu validieren. Ganz ohne Schreibkonflikte im Cluster.\par
\par
\par
\par
Mir ist dazu eine super Metapher eingefallen:\par
\par
Sagen wir, eine Serveranwendung ist wie ein Laden, in dem man Dinge kaufen kann.\par
Die Artikel sind die Datens\'e4tze. Die Mitarbeiter sind die Threads.\par
Wenn ein Kunde nur reinkommt, rumschaut, sich ggf. noch beraten l\'e4sst, ist das "nur lesend".\par
Wenn ein Kunde etwas kauft, d.h. Warenbestand und Einnahmen des Ladens ver\'e4ndert, ist das "schreibend".\par
Es gibt unterschiedliche Arten von Artikeln:\par
Riesige, aber triviale Artikel, z.B. Holzbalken, Steine, usw. zum Hausbauen.\par
So mittelgro\'dfe, mittelkomplexe Artikel, wie z.B. ... keine Ahnung ... Lampen, W\'e4schek\'f6rbe, Lebensmittel.\par
Und es gibt kleine, superkomplexe Artikel, die viel Aufwand erzeugen f\'fcr Ausstellung, Beratung, Reklamation, evtl. sogar Reparatur. Klassisches Beispiel: Elektronikkram. Fotos, Smartphones, usw.\par
\par
Java mit Jetstream macht es problemlos m\'f6glich, alle m\'f6glichen Artikel gemischt im selben Laden zu verkaufen. Alles kann angeboten werden und das auch ziemlich effizient.\par
Soweit so cool.\par
Nat\'fcrlich ist es so, dass, je mehr Waren man anbieten will, man auch immer mehr Verkaufsfl\'e4che und Mitarbeiter f\'fcr Beratung und Kassieren braucht.\par
F\'fcr Java kein Problem. F\'fcr Jetstream auch nicht. Nur die Hardware macht irgendwann nicht mehr mit. G\'e4be es einen 50 GHz 1024-Core Server mit 128 TB RAM, dann w\'e4re die ganze Clusterei \'fcberfl\'fcssig. Gibts leider nicht. Bl\'f6d.\par
In der Metapher hei\'dft das: Aus irgendeinem Grund darf ein Laden nicht mehr als 100.000 Artikel anbieten und nicht mehr als 32 Mitarbeiter haben.\par
Man kann diese Grenzen noch so clever ausnutzen und ein noch so effizient laufendes Lager haben, irgendwann platzt ein einzelner Laden aus allen N\'e4hten und die Mitarbeiter kommen mit dem Beraten und Kassieren nicht mehr nach.\par
\par
Das muss man \'fcber mehrere L\'e4den clustern. Geht nicht anders.\par
\par
Der erste, durchaus naheliegende, aber auch naive Gedanke ist: Das hei\'dft einfach X mal 100.000-Artikel-L\'e4den mit X mal 32 Mitarbeiter und die Waren dann "irgendwie" auf die L\'e4den verteilen.\par
Das Problem daran ist: Die Kunden und Mitarbeiter werden vor lauter herumtelefonieren und herumlaufen zwischen den L\'e4den nicht mehr fertig. St\'e4ndig muss rumtelefoniert werden, ob es von Artikeltyp X in einem anderen Laden noch ein Exemplar gibt, dann m\'fcssen die Kunden dorthin laufen, usw. Der Vergleich hinkt ein bisschen, verglichen damit, wie Cluster wirklich funktionieren, aber die Grundaussage ist richtig: Es gibt einen riesigen Overhead an Koordinierungsbedarf, Konflikte bei "schreibenden" Zugriffen ("kaufende Zugriffe", hehe), Austausch der Waren zwischen den M\'e4rkten, usw.\par
\par
Darum folgende Idee, das ganze etwas eleganter zu clustern:\par
Ein Laden k\'fcmmert sich nur um die riesigen, aber trivialen Artikel. Der hat dann vielleicht sogar optimierte Lagerfl\'e4chen und Ausstellungsfl\'e4chen, Kabelstapler, usw.\par
Ein Laden k\'fcmmert sich um alles zu k\'fchlende Zeug (Lebensmittel). Mit K\'fchlanlagen usw.\par
Ein Laden k\'fcmmert sich um das ganze kleine, komplexe und beratungsintensive Zeug (Smartphones usw.). Mit schicken Ausstellungsst\'e4nden, Lounges mit Musik, usw.\par
\par
Zus\'e4tzlich macht man von jedem der drei "Hauptl\'e4den" noch identisch best\'fcckte "Ausstellungsl\'e4den".\par
(Hier hinkt das Beispiel wieder ein bisschen: F\'fcr physikalische L\'e4den w\'e4re das ein riesiger Aufwand, aber in der digitalen Welt ist eine simple Kopie ziehen trivial zu machen).\par
In den Ausstellungsl\'e4den k\'f6nnen die Kunden durchlaufen, sich die Artikel ansehen, ausprobieren, sich beraten lassen, usw.\par
Aber gekauft wird nur im Hauptladen, damit an einer zentralen Stelle alles richtig verbucht wird.\par
Je mehr Kunden es gibt, umso mehr Ausstellungsl\'e4den macht man. Das ist \'fcberhaupt kein Problem. So eine Kopie ist schnell erstellt (in der Digitalen Welt), dann gehen die T\'fcren auf und 1000 weitere Kunden k\'f6nnen fr\'f6hlich reinstr\'f6men und rumschauen.\par
Die Hauptl\'e4den machen hingegen gar keine Beratung mehr. Dort geht man nur noch rein, wenn man schon ausgesucht hat, was man kaufen will. Man kriegt es ausgeh\'e4ndigt und geht zur Kasse.\par
Dadurch k\'f6nnen auch alle 32 Mitarbeiter des Hauptladens an den Kassen sitzen und kassieren.\par
Und jetzt kann man sich gut vorstellen: 32 Kassen nebeneinander, die den ganzen Tag lang ununterbrochen nur noch Kunden mit Ware in der Hand abkassieren, ohne Beratung usw., k\'f6nnen ein gigantisches Volumen an Kunden und an Warenausgabe bearbeiten.\par
\par
Es muss auch nicht unbedingt bei diesen drei L\'e4den bleiben. Man kann durchaus einen zweiten Laden f\'fcr kleine, komplexe Artikel aufmachen. Das m\'fcsste halt dann thematisch getrennt sein, damit nicht wieder Chaos untereinander ausbricht.\par
Etwa wie Mediamarkt und Obi. Der eine verkauft Elektronikschnickschnack, der andere verkauft Baustuff. Je nachdem, was man braucht, f\'e4hrt man zu dem einen oder dem anderen. Wenn \'fcberhaupt, gibt es zwischen beiden nur minimalen Koordinationsbedarf und \'fcberhaupt keine "schreibenden" Konflikte.\par
\par
Genauso ist die Idee mit den nach Datenklassen und mit Read-only Nodes geclusterten Servern.\par
}
 